{
    "input_path": "test_data/tokenized_text.paths",
    "output_tokenized_dir": "tokenized_text1",
    "checkpoint_dir": "checkpoint",
    "chunk_size": 8000,
    "size": 100,
    "min_count": 1,
    "epochs": 10,
    "resume": true,
    "verbose": true
}
