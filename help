python src/embedding/get_files_multi.py -i /home/lca80/Desktop/data/emtell/PMC/txt -o txt_exist.paths 

python src/embedding/w2v.py -m models -i txt_exist.paths --save_dir preprocessed_sentences

python src/embedding/helper.py -i preprocessed_sentences -o sentences -j mv_1_100  #op_start_step

python src/embedding/get_files_multi.py -i sentences -o txt_new.paths

mv models models1

python src/embedding/w2v.py -m models -i txt_new.paths 

python src/embedding/helper.py -m models/1000d100_model/d100_model.model -t "he questions about the subjects' self-reported oral health status,"
python src/embedding/helper.py -m models/1000d100_model/d100_model.model -t "self-reorted"

python src/embedding/helper.py -m models1/1000d100_model/d100_model.model -t "he questions about the subjects' self-reported oral health status,"
python src/embedding/helper.py -m models1/1000d100_model/d100_model.model -t "self-reorted"




#create sentences and vocabs.txt
mkdir tokenized_sentences
python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/clinical/dc_summaries -o tokenized_sentences
mkdir ds_sentences     ds cr  ct  mg  mr  nm  us
python word2vec/src/word2vec/helper.py -i tokenized_sentences -o ds_sentences -j mv_0_10000             #cp_0_100 -l 500

python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/radiology/cr -o tokenized_sentences
mkdir cr_sentences
python word2vec/src/word2vec/helper.py -i tokenized_sentences -o cr_sentences -j mv_6_10000

python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/radiology/ct -o tokenized_sentences
mkdir ct_sentences
python word2vec/src/word2vec/helper.py -i tokenized_sentences -o ct_sentences -j mv_36_10000

python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/radiology/mg -o tokenized_sentences
mkdir mg_sentences
python word2vec/src/word2vec/helper.py -i tokenized_sentences -o mg_sentences -j mv_46_10000

python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/radiology/mr -o tokenized_sentences
mkdir mr_sentences
python word2vec/src/word2vec/helper.py -i tokenized_sentences -o mr_sentences -j mv_47_10000

python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/radiology/nm -o tokenized_sentences
mkdir nm_sentences
python word2vec/src/word2vec/helper.py -i tokenized_sentences -o nm_sentences -j mv_50_10000

python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/radiology/us -o tokenized_sentences
mkdir us_sentences
python word2vec/src/word2vec/helper.py -i tokenized_sentences -o us_sentences -j mv_51_10000

mkdir ctakes_tokenized
mv -v ds_sentences/* ctakes_tokenised/
mv -v cr_sentences/* ctakes_tokenised/
mv -v ct_sentences/* ctakes_tokenised/
mv -v mg_sentences/* ctakes_tokenised/
mv -v mr_sentences/* ctakes_tokenised/
mv -v nm_sentences/* ctakes_tokenised/
mv -v us_sentences/* ctakes_tokenised/

rm -r *_sentences

echo "" > ctakes_wc.txt
python word2vec/src/word2vec/helper.py -i ctakes_tokenized -o ctakes_wc.txt


#create ctakes_vocabs.txt
with open("ctakes_wc.txt", 'r') as fd:	lines = fd.read().split('\n')

words = [ tuple(line.split()) for line in lines]; print (len(words), words[-10:])
words = [ w for (w, c) in words if w != ""]   #and int(c) > 1] ; print (len(words), words[-10:])
words = ["<stop>", "<start>", "<unk>", "<UNK>"] + [ w for w in words if w != ""] 

with open("ctakes_vocabs.txt", 'w') as fd:	fd.write("\n".join(words))

python word2vec/src/word2vec/helper.py -m /shared/data/PMC/w2v_models/1300000d300_model/d300_model.model -i ctakes_vocabs.txt -o ctakes_d300.txt
python word2vec/src/word2vec/helper.py -m /shared/data/PMC/w2v_models/1250000d200_model/d200_model.model -i ctakes_vocabs.txt -o ctakes_d200.txt
python word2vec/src/word2vec/helper.py -m /shared/data/PMC/w2v_models/1250000d100_model/d100_model.model -i ctakes_vocabs.txt -o ctakes_d100.txt

