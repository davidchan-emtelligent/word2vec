python src/word2vec/get_files_multi.py -i /home/lca80/Desktop/data/emtell/PMC/txt -o txt_exist.paths 

python src/word2vec/w2v.py -m models -i txt_exist.paths --save_dir tokenized_text

python src/word2vec/helper.py -i tokenized_text -o ds_sentences -j mv_0_10000

python src/word2vec/get_files_multi.py -i ds_sentences -o tokenized_text_fs.paths

python src/word2vec/w2v.py -m other_models -i tokenized_text_fs.paths -p tokenized


#---w2v.py

python src/word2vec/w2v.py -m models -i txt_new.paths
 
python src/word2vec/helper.py -m models/1000d100_model/d100_model.model -t "he questions about the subjects' self-reported oral health status,"
python src/word2vec/helper.py -m models/1000d100_model/d100_model.model -t "self-reorted"

python src/word2vec/helper.py -m models1/1000d100_model/d100_model.model -t "he questions about the subjects' self-reported oral health status,"
python src/word2vec/helper.py -m models1/1000d100_model/d100_model.model -t "self-reorted"


#---helper.py

# extract word count from tokenized text
	python src/word2vec/helper.py -i <input_dir> -o <output_path>
	python src/word2vec/helper.py -i sentences -o word_count.txt

# split files of a dir into sub_dirs
	python src/word2vec/helper.py -i <input_dir> -o <output_dir> -j cp_3_100	#cp input_dir to sentences/sentences_3, 4, .. with 100 each
	python src/word2vec/helper.py -i tokenized_text -o ds_sentences -j mv_0_10000

# extract vec from model
	python src/word2vec/helper.py -m <input_model> -i <input_vocab_path> -o <save_vec_file> 
	python src/word2vec/helper.py -m <input_model> -o <save_vec_file> 	#vocab from model
        python word2vec/src/word2vec/helper.py -m models5/500000d200_model/d200_model.model -i ctakes_vocabs.txt -o ctakes_d200.txt

# test w2v models:
	python src/word2vec/helper.py -m <input_model> -t "he questions about the subjects' self-reported oral health status,"
	python src/word2vec/helper.py -m <input_model> -t "self-reorted"



#--- create tokenized_text and vocabs.txt

mkdir tokenized_text
python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/clinical/dc_summaries -o tokenized_text
mkdir ds_sentences     ds cr  ct  mg  mr  nm  us
python word2vec/src/word2vec/helper.py -i tokenized_text -o ds_sentences -j mv_0_10000             #cp_0_100 -l 500

python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/radiology/cr -o tokenized_text
mkdir cr_sentences
python word2vec/src/word2vec/helper.py -i tokenized_text -o cr_sentences -j mv_6_10000

python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/radiology/ct -o tokenized_text
mkdir ct_sentences
python word2vec/src/word2vec/helper.py -i tokenized_text -o ct_sentences -j mv_36_10000

python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/radiology/mg -o tokenized_text
mkdir mg_sentences
python word2vec/src/word2vec/helper.py -i tokenized_text -o mg_sentences -j mv_46_10000

python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/radiology/mr -o tokenized_text
mkdir mr_sentences
python word2vec/src/word2vec/helper.py -i tokenized_text -o mr_sentences -j mv_47_10000

python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/radiology/nm -o tokenized_text
mkdir nm_sentences
python word2vec/src/word2vec/helper.py -i tokenized_text -o nm_sentences -j mv_50_10000

python word2vec/src/word2vec/conll2tokens.py -i /shared/dropbox/ctakes_conll/output/radiology/us -o tokenized_text
mkdir us_sentences
python word2vec/src/word2vec/helper.py -i tokenized_text -o us_sentences -j mv_51_10000

mkdir ctakes_tokenized
mv -v ds_sentences/* ctakes_tokenized/
mv -v cr_sentences/* ctakes_tokenized/
mv -v ct_sentences/* ctakes_tokenized/
mv -v mg_sentences/* ctakes_tokenized/
mv -v mr_sentences/* ctakes_tokenized/
mv -v nm_sentences/* ctakes_tokenized/
mv -v us_sentences/* ctakes_tokenized/

rm -r *_sentences

echo "" > ctakes_wc.txt
python word2vec/src/word2vec/helper.py -i ctakes_tokenized -o ctakes_wc.txt


#create ctakes_vocabs.txt
with open("ctakes_wc.txt", 'r') as fd:	lines = fd.read().split('\n')

words = [ tuple(line.split()) for line in lines]; print (len(words), words[-10:])
words = [ w for (w, c) in words if w != ""]   #and int(c) > 1] ; print (len(words), words[-10:])
words = ["<stop>", "<start>", "<unk>", "<UNK>"] + [ w for w in words if w != ""] 

with open("ctakes_vocabs.txt", 'w') as fd:	fd.write("\n".join(words))

python word2vec/src/word2vec/helper.py -m /shared/data/PMC/w2v_models/1300000d300_model/d300_model.model -i ctakes_vocabs.txt -o ctakes_d300.txt
python word2vec/src/word2vec/helper.py -m /shared/data/PMC/w2v_models/1250000d200_model/d200_model.model -i ctakes_vocabs.txt -o ctakes_d200.txt
python word2vec/src/word2vec/helper.py -m /shared/data/PMC/w2v_models/1250000d100_model/d100_model.model -i ctakes_vocabs.txt -o ctakes_d100.txt


#train ctakes
python word2vec/src/word2vec/get_files_multi.py -i /shared/dropbox/ctakes_conll/tokenized_text -o ctakes_tokenized.paths

python word2vec/src/word2vec/w2v.py -m models -i ctakes_tokenized.paths --tokenizer simple -c config.json
