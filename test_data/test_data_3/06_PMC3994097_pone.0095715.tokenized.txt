plos one plos one plos plosone plos one 1932 - 6203 public library of science san francisco , usa 24752287 3994097 pone - d - 13 - 49333 10.1371 / journal.pone.0095715 research articlecomputer and information sciencescomputer applicationscomputing methodsmathematical computingphysical sciencesmathematicsapplied mathematicsalgorithms one - hot vector hybrid associative classifier for medical data classification hybrid associative classifier chat - ohm uriarte - arcia abril valeria 1 * lopez - yanez itzama 2 yanez - marquez cornelio 1 1 neural networks and unconventional computating lab / alpha - beta group , centro de investigacion en computacion , instituto politecnico nacional , ciudad de mexico , distrito federal , mexico 2 intelligent computing lab / alpha - beta group , centro de innovacion y desarrollo tecnologico en computo , instituto politecnico nacional , ciudad de mexico , distrito federal , mexico dalby andrew r. editor university of westminster , united kingdom * e - mail : auriarteb10 @ sagitario.cic.ipn.mx competing interests : the authors have declared that no competing interests exist .
conceived and designed the experiments : avua cym ily .
performed the experiments : avua .
analyzed the data : avua cym .
contributed reagents / materials / analysis tools : avua cym ily .
wrote the paper : avua .
2014 21 4 2014 9 4 e95715 22 11 2013 30 3 2014 ( c ) 2014 uriarte - arcia et al 2014 uriarte - arcia et althis is an open - access article distributed under the terms of the creative commons attribution license , which permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are properly credited .
pattern recognition and classification are two of the key topics in computer science .
in this paper a novel method for the task of pattern classification is presented .
the proposed method combines a hybrid associative classifier ( clasificador hibrido asociativo con traslacion , chat , in spanish ) , a coding technique for output patterns called one - hot vector and majority voting during the classification step .
the method is termed as chat one - hot majority ( chat - ohm ) .
the performance of the method is validated by comparing the accuracy of chat - ohm with other well - known classification algorithms .
during the experimental phase , the classifier was applied to four datasets related to the medical field .
the results also show that the proposed method outperforms the original chat classification accuracy .
centro de investigacion en computacion ( http :// www.cic.ipn.mx /) .
the funders had no role in study design , data collection and analysis , decision to publish , or preparation of the manuscript .
introduction recognizing objects is an automatic routine task for humans and there is a myriad of problems involving pattern recognition .
simulating the human capacity for objects recognition has been a very important topic for computer sciences .
for several decade , various approaches have been developed , which can be implemented on computers , to simulate the human ability to recognize objects .
one of such approaches is the associative approach , whose main purpose is to correctly retrieve complete patterns from input patterns .
the first known model of associative memories is the lernmatrix , developed in 1961 by karl steinbuch [ 1 ] .
some years later , an optical device capable of behaving as an associative memory was created by buneman and longuet - higgins .
[ 2 ] .
in 1972 , the work of anderson [ 3 ] , kohonen [ 4 ] , and to some extent nakano [ 5 ] , led to the model that is now known by the generic name of linear associator .
in this same year shun - ichi amari , published a theoretical work about self - organizing nets of threshold elements [ 6 ] .
the work of amari represents an essential background to one of the most important associative models : the hopfield memory [ 7 ] .
in the late 1980 's , kosko [ 8 ] developed a bidirectional associative memory from two hopfield memories .
the morphological associative memories were introduced by ritter et al. in 1998 [ 9 ] , which represented a qualitative leap for associative models .
these models incorporated concepts from mathematical morphology , which give them several advantages over the known models .
associative models have been widely and successfully used in different applications such as : pollutant concentration prediction [ 10 ] , pattern classification [ 11 ] , images processing [ 12 ] , [ 13 ] , among others .
in this paper , a method that combines a hybrid associative classifier , a coding technique for output patterns and majority voting , is presented .
the rest of this paper is organized as follows .
section 2 describes all the materials and methods needed to develop our proposal .
section 3 describes how the experimental phase was conducted and discusses the results .
some conclusions are presented in section 4 and finally the acknowledge and references are included .
materials and methods associative memories an associative memory m is a system that relates input patterns and output patterns as follows [ 14 ] : with x and y being the input and output patterns vectors .
each input vector form an association with its corresponding output vector .
an associative memory is represented by a matrix whose - th component is . for each k integer and positive , the corresponding association will be denoted as : . the matrix m is generated from a finite set of previously known associations , called the fundamental set .
if mu is an index , the fundamental set is represented as : , where is the cardinality of the fundamental set .
the patterns that form the fundamental set are called fundamental patterns .
if it holds that , m is autoassociative , otherwise it is heteroassociative .
if we consider the fundamental set of patterns where and are the dimensions of the input patterns and output patterns , respectively , it is said that and . then the j - th component of an input pattern is . analogously , the j - th component of an output pattern is represented as . therefore , the fundamental input and output patterns are represented as follows : a distorted version of a pattern to be recalled will be denoted as . an unknown input pattern to be recalled will be denoted as . if when an unknown input pattern is fed to an associative memory m , and it happens that the output corresponds exactly to the associated pattern , it is said that recalling is correct .
lernmatrix the lernmatrix is a heteroassociative memory that can function as a binary pattern classifier if the output patterns are properly selected [ 14 ] .
it is an input and output system that accepts a binary input pattern and produce as an output the class . for a class , where m is the number of classes in the fundamental set , the class is coded according to the following expression : and for . the lernmatrix is represented by a matrix m. at the beginning of the learning phase , each component of m is set to zero and then it is updated according to rule , where : where is any positive constant that was previously chosen .
the recovery phase consists of finding the class vector for a given vector . finding the class means to obtain the coordinates of the vector that corresponds to the pattern . the i - th component of the class vector is obtained according to the following expression : linear associator let be the fundamental set with [ 15 ] : the learning phase consists of two steps : for each of the associations find the matrix of dimensions sum the matrices to obtain the memory where the ij - th component of m can be expressed as follow : . the recovering phase consists of presenting an input pattern to the memory m and performing the following operation : chat the chat is a hybrid associative classifier developed by santiago - montero [ 16 ] , which is based on two associative memories : the lernmatrix and the linear associator .
this classifier overcomes some limitations that these two memories presented , by ingeniously combining the learning and recovery phases of both models .
the first proposed model was called cha , which combined the learning phase of the linear associator and the recovering phase of the lernmatrix , but sometimes this model fails to perform a correct classification .
to overcome this limitation , a new version was proposed , by adding a new step to the model : translation of coordinates axes .
this new version was named chat. with this axes translation , the new origin is located at the centroid of the input vectors patterns .
definition 3.1 : let be a set of fundamental input patterns , and let be the mean vector of them , where : definition 3.2 : let be a set of fundamental input patterns and a new set of translated patterns generated using the following expression : chat algorithm let be a set of n - dimensional fundamental input patterns with real values in its components , which are grouped into m classes .
to each of the fundamental input patterns belonging to class k , an output vector of size m is assigned .
this vector consists of zeros , except for the k - th component , whose value is set to 1 .
calculate the mean vector of the set of input patterns according to definition 3.1 .
the mean vector is taken as the new origin of the coordinate axes .
translate the patterns of the input set according to definition 3.2 .
apply the learning phase , which is the same as the learning phase of the linear associator , to the translated set obtained in the previous step .
translate the patterns that have to be classified using the definition 3.2 .
apply the recovering phase , which is the same as the recovering phase of the lernmatrix , to the translated set obtained in the previous step .
chat - ohm in this section the description of the proposed method is presented .
this proposal is part of the results achieved by several members of the neural networks and unconventional computing laboratory of the centro de investigacion en computacion , instituto politecnico nacional , in an attempt to improve the performance of the chat model [ 16 ] .
this joint effort resulted in a number of methods that implemented some variations on the chat , being the proposed method one of them .
one - hot vectors one - hot vector is a coding technique for output patterns , which will be used in the proposed method instead of the original coding technique presented in the step 2 of the chat algorithm that was described in the previous section .
definition 3.3 : let be a set of translated fundamental output patterns of size p. the i - th component of each translated fundamental output pattern is coded according to the following expression : majority voting the classification phase consists of finding the output vector to which an unknown input pattern belongs .
majority voting is a procedure used during the classification phase to perform this task .
definition 3.4 : let be the number of different classes in the fundamental input set .
let be a set of fundamental input patterns where . for a class , a masking vector of size is coded according to the following expression : let m be a matrix generated during the learning phase of the chat - ohm and an unknown n - dimensional input pattern to be classified .
the recover pattern is determined as follow : for each class , a counting vector is obtained applying an " and " operator between and : finally is obtained using the following expression : chat - ohm algorithm let be a set of n - dimensional fundamental input patterns with real values in its components , which are grouped into m classes .
to each of the fundamental input patterns an output vector of size p is assigned .
this vector is coded according to the definition 3.3 .
calculate the mean vector of the set of input patterns according to definition 3.1 .
the mean vector is taken as the new origin of the coordinate axes .
translate the patterns of the input set according to definition 3.2 .
apply the learning phase , which is the same that as learning phase of the linear associator , to the translated set obtained in the previous step .
translate the patterns that have to be classified using the definition 3.2 .
apply the recovering phase , which is the same as the recovering phase of the lernmatrix , to the translated set obtained in the previous step .
because of the way in which the classes were coded , we will obtain an output vector of size p and not the desire output class of size m. our algorithm performs an extract step .
perform the majority voting explained in the previous section .
data sets this section provides a brief description of the dataset used during the experimental phase .
all the used datasets were taken from the university of california at irvine machine learning repository [ 17 ] .
a summary of the main characteristics of the datasets is shown in table 1 .
10.1371 / journal.pone.0095715.t001 table 1 characteristics of the datasets used in the experimetal phase .
dataset instances attributes missing values breast cancer 683 9 yes haberman 's survival 306 3 no liver disorders 345 6 no hepatitis disease 155 19 yes haberman 's survival dataset the dataset contains cases from a study conducted at the university of chicago 's billings hospital on the survival of patients who had undergone surgery for breast cancer .
the dataset contain 306 instances , which belong to two different classes ; 255 instances belong to the first class ( patients who survived 5 years or more ) and 81 instances belong to the second class ( patients who died within 5 years ) .
the dataset has 4 attributes including the class attribute .
the purpose of the dataset is to predict the survival status of patients that have undergone breast cancer surgery .
wisconsin breast cancer dataset this dataset was obtained from the university of wisconsin hospitals , madison from dr. william h. wolberg .
the dataset has information of clinical cases of breast cancer .
the dataset contains 699 instances belonging to two classes , 458 instances belong to the first class ( benign ) and 241 belong to the second class ( malignant ) .
each instance consists of 10 attributes , including the class attribute .
the dataset has 16 pattern with one missing values .
the instances with missing values were deleted from the original dataset and the resulting data set was used for the experimental phase .
liver disorders dataset the liver disorders dataset was created by bupa medical research ltd .
this dataset presents the results of a study of liver disorders that might arise from excessive alcohol consumption .
it contains 345 instances belonging to two classes , 145 instances belong to the first class and 200 instances belong to the second class .
each instance consists of 7 attributes , including the class attribute .
hepatitis disease dataset this dataset contains information of the clinical results of hepatitis patients .
it contains 155 instances belonging to two classes , 32 instances belong to the first class ( die ) and 123 instances belong to the second class ( alive ) .
each instance consists of 20 attributes , including the class attribute .
this dataset has multiple missing values .
due to the small size of the dataset and the considerable number of missing values , these cannot be discarded .
in this case the missing values were substituted by the class mode for categorical features and by the class mean for continuous values .
machine learning algorithms this section provides a short description of the algorithms used during the experimental phase .
all of these algorithms are implemented in the weka 3 : data mining software in java [ 18 ] .
further details on the implementation of these algorithms can be found in the following reference [ 19 ] .
ib1 ib1 is a basic nearest - neighbor instance - based learner that finds the training instance closest in euclidean distance to the given test instance and predicts the same class as this training instance .
if several instances qualify as the closest , the first one found is used [ 19 ] .
conjunctiverule conjunctiverule learns a simple conjunctive rule learner that predicts either a numeric or a nominal class value .
uncovered test instances are assigned the default class value of the uncovered training instances .
the information gain ( nominal class ) or variance reduction ( numeric class ) of each antecedent is computed , and rules are pruned using reduced - error pruning [ 19 ] .
randomtree trees built by randomtree test a given number of random features at each node , performing no pruning .
the tree is constructed considering k randomly chosen attributes at each node .
also has an option to allow estimation of class probabilities based on a hold - out set [ 19 ] .
randomforest randomforest constructs random forests by bagging ensembles of random trees .
a random forest is a classifier consisting of a collection of tree - structured classifiers and each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest [ 20 ] .
bftree bftree constructs a decision tree using a best - first expansion of nodes rather than depth - first expansion used by standard decision tree learners .
pre and post pruning option are available that are based on finding the best number of expansion to use via cross - validation on the training data .
while fully grown trees are the same for best - first and depth - first algorithms , the pruning mechanism used by bftree will yield a different pruned tree structure than that produced by depth - first methods [ 19 ] .
smo smo implements john platt 's sequential minimal optimization algorithm for training a support vector classifier , using kernel functions such as polynomial or gaussian kernels .
missing values are replaced globally , nominal attributes are transformed into binary ones , and attributes are normalized by default .
for further details of the implementation , see [ 21 ] .
adaboostm1 adaboostm1 is a variation of boosting , method for combining multiple models seeking models that complement one another .
this algorithm is constructed through the combination of various classifiers produced by repeatedly running t rounds a given " weak " learning algorithm on various distributions over the training data .
finally the booster combine the t " weak " hypotheses into a single final hypothesis [ 22 ] .
multiboostab multiboostab combines boosting with a variant of wagging to prevent overfitting .
multiboosting is an extension of adaboost technique [ 23 ] .
wagging is a technique that allow variance reduction , while adaboost perform both variance and bias reduction .
multiboost is achieved by wagging a set of sub - committees of classifiers , each sub - committee formed by adaboost .
when forming decision committee using c4.5 as the base learning algorithm , multiboost is demonstrated to produce committees with lower error than adaboost .
rbfnetwork rbfnetwork implements a normalized gaussian radial basis function network , deriving the centers and widths of hidden units using k - means and combining the outputs obtained from the hidden layer using logistic regression if the class is nominal and linear regression if it is numeric .
the activations of the basis functions are normalized to sum to 1 before they are fed into the linear models [ 19 ] .
naivebayes naivebayes implements the probabilistic naive bayes classifier .
the naivebayes algorithm is based on bayes rule and assumes that the attributes are conditional independent given the class , and it posits that no hidden or latent attributes influence the prediction process [ 24 ] .
bayesnet bayesian networks are alternative ways of representing a conditional probability distribution by means of directed acyclic graphs ( dags ) .
in this model , each node represents a random variable and an arrow connecting a parent node with a child node indicates a relationship between them [ 25 ] .
bayesnet learns bayesian nets under two assumptions : nominal attributes ( numeric ones are pre - discretized ) and no missing values ( any such values are replaced globally ) .
naivebayesmultinomial naivebayesmultinomial implements the multinomial bayes' classifier .
a naive bayes classifier is based on bayes rule but this does not take into account the number of occurrence of an element .
the naive bayes multinomial incorporates frequency to perform classification [ 19 ] .
complementnaivebayes complementnaivebayes builds a complement naive bayes classifier as described by rennie et al [ 26 ] .
in this work , they proposed heuristic solutions to some problems presented by the naive bayes classifiers .
they proposed a solution for skewed data , more training examples for one class than another that causes that the classifier prefer one class over the other .
decisiontable decisiontable builds a simple decision table majority classifier , this table has two components : a set of features that are included in the table and a body consisting of labeled instances from the space defined by the features [ 27 ] .
lwl lwl is a general algorithm for locally weighted learning .
it assigns weights using an instance - based method and builds a classifier from the weighted instances .
different classifiers can be selected , but a good choice is naive bayes for classification problems and linear regression for regression problems .
attribute normalization is turned on by default [ 19 ] .
dmnbtext another naive bayes scheme for text classification is dmnbtext .
this learns a multinomial naive bayes classifier in a combined generative and discriminative fashion .
dmnbtext injects a discriminative element into parameter learning by considering the current classifier 's prediction for a training instance before updating frequency counts .
when processing a given training instance , the counts are incremented by one minus the predicted probability for the instance 's class value .
dmnbtext allows users to specify how many iterations over the training data the algorithm will make , and whether word frequency information should be ignored , in which case , the method learns a standard naive bayes model rather than a multinomial one [ 19 ] .
multischeme multischeme selects the best classifier from a set of candidates using cross - validation of percentage accuracy or mean - squared error for classification and regression , respectively .
the number of folds is a parameter .
performance on training data can be used instead [ 19 ] .
vote vote provides a baseline method for combining classifiers .
the default scheme is to average their probability estimates or numeric predictions , for classification and regression , respectively .
other combination schemes are available - for example , using majority voting for classification [ 19 ] .
votedperceptron votedperceptron implements the voted perceptron algorithm .
the solution vector found by the perceptron algorithm depends greatly on the order in which the instances are encountered .
one way to make the algorithm more stable is to use all the weight vectors encountered during learning , not just the final one , letting them vote on a prediction .
each weight vector contributes a certain number of votes [ 19 ] .
normalization during the experiments performed over the original data , we observed that some of the datasets present large scale difference between features .
to avoid the effect that an overly large variable can have over the classification performance , the datasets were normalized and the experiments were performed with the normalized datasets .
normalization can prevent some features from dominating just because they have large numeric values .
subtracting the mean and dividing by the standard deviation can be an appropriate normalization method for this situation [ 31 ] .
the normalization was performed separately on each attribute .
normalization was calculated using the following expression : where is the normalized value of , mu is the mean of the population and sigma is the standard deviation of the population .
wilson 's edition one of the most popular filtering algorithms is the wilson 's edition [ 28 ] .
the general idea of this method is to identify and remove noisy or atypical patterns , primarily those which exist in the overlap area between two or more classes .
the process consists of applying the rule of the k nearest neighbor ( usually k = 3 ) to estimate the corresponding class of each pattern in the dataset .
those patterns whose class does not correspond to the majority class of the k - nearest neighbors will be discarded [ 28 ] .
algorithm comparison one of the objectives of this study is to perform a consistent comparison between the classification performance of our proposal and the classification performance of other well - known pattern classification algorithms .
there are two aspects that need to be addressed : select a suitable test set and the method to compare the classification performance of each algorithm .
to predict the performance of a classifier , we need to assess the success rate on a dataset that takes no part in the construction ( training phase ) of the classifier .
when the data available is big , there is no problem in the selection of a suitable test set , just use a large training set and a large test set .
but the question of predicting performance with limited data is still controversial .
there are many techniques , of which cross - validation is the method of choice in most situations .
kohavi [ 29 ] compared cross - validation and bootstrap , the results show that bootstrap has low variance , but extremely large bias for some problems ; as a consequence stratified 10 - fold cross - validation is recommended .
to perform the comparison of our proposal with other pattern classification algorithms , we used the 10 - fold cross - validation approach .
classification accuracy for classification problems , the performance of a classifier can be measured in term of the success rate .
the classifier predicts the class of each instance in the test set ; if the class is correct , it is counted as a success .
the success rate is the proportion of success over the whole set of test instances .
in this paper , the accuracy of the classifiers is expressed as a percentage , and was computed according to the following expression : validation method according to [ 19 ] the standard way of predicting the classification accuracy of a learning technique is to use stratified 10 - fold cross - validation .
this method divides the dataset into 10 parts in which each class is represented in approximately the same proportion as in the full dataset .
the classification algorithms will be executed 10 times , in each execution one different part will be used as the test set and the classification algorithm will be trained with the remaining nine parts .
the success rate will be calculated for each execution .
finally , the 10 success rates are averaged to yield an overall success rate .
experiments and discussion in this section we present and discuss the results obtained during the experimental phase , throughout which four datasets were used to obtain the classification performance of each of the compared classification algorithms .
the datasets used in this section were taken from the uci machine learning repository [ 17 ] .
a brief summary of the datasets is presented in table 1 .
the performance achieved by the proposed method is compared with the performances of 19 well - known methods taken from the weka 3 data mining software [ 18 ] .
further information about the used algorithms can be found in [ 19 ] .
all experiments were conducted using a personal computer with an intel core i3 - 2100 processor running ubuntu 13.04 64 - bits operating system with 4096 gb of ram .
to ensure valid comparison of classification performance , the same conditions and validation schemes were applied in each experiment .
classification performance of each of the algorithms was calculated using stratified 10 - fold cross - validation , with random re - ordering of the patterns before fold generation .
in order to account for the random re - ordering of the patterns , the experiments for each classification algorithm , including our proposal , were executed 10 times using the stratified 10 - fold cross - validation approach and the results averaged to obtain a final success rate for each algorithm .
these results are used to compare the performance of our proposal and the other classification algorithms .
original datasets in this subsection we analyze the classification accuracy results of each one of the compared algorithms , when applied to the original four datasets that were selected for this study .
table 2 shows the classification accuracy achieved by the original chat model and by our proposal in the four datasets .
it is worth noting that chat - ohm achieved the best classification accuracy for all the datasets .
in some cases the improvement in the classification accuracy is quite significant , as in the cases of the breast cancer dataset and the hepatitis disease dataset , with an improvement of 31.9 percent and 16.77 percent , respectively .
the improvement for the liver disorders dataset is 5.82 percent , which is still important .
the haberman 's survival dataset is where we observed the least improvement with only 0.41 percent .
10.1371 / journal.pone.0095715.t002 table 2 accuracy comparison with the original method (%) original data .
datasets algorithm breast cancer haberman 's survival hepatitis disease liver disorders chat 63.10 65.95 68.19 55.63 chat - ohm 95.00 66.36 84.96 61.45 table 3 shows the classification accuracy achieved by our proposal and the 19 classification algorithms from weka , against which we will compare our method .
for each dataset , the highest classification accuracy is emphasized with boldface .
10.1371 / journal.pone.0095715.t003 table 3 clasification accuracy comparison (%) original data .
dataset algorithm breast cancer haberman 's survival hepatitis disease liver disorders adaboostm1 95.05 74.02 89.8 65.96 bayesnet 97.34 71.73 87.68 56.85 bftree 94.88 72.33 88.98 66.44 chat - ohm 95.00 66.36 84.96 61.45 complementnaivebayes 85.43 73.87 77.54 56.57 conjunctiverule 91.98 72.97 88.99 56.06 decisiontable 95.69 71.90 88.40 59.11 dmnbtext 65.01 73.53 79.38 57.98 ib1 95.75 65.77 81.03 62.22 lwl 92.14 71.90 89.70 60.80 multiboostab 94.73 73.28 89.49 65.29 multischeme 65.01 73.53 79.38 57.98 naivebayes 96.26 74.80 87.50 54.89 naivebayesmultinomial 90.32 73.74 78.00 56.96 randomforest 96.47 67.94 90.61 68.44 randomtree 94.74 64.48 85.32 64.10 rbfnetwork 96.36 73.75 85.78 65.06 smo 96.87 73.33 88.83 57.98 vote 65.01 73.53 79.38 57.98 votedperceptron 91.08 73.82 78.09 63.53 as we can observe in table 3 , the chat - ohm does not surpass all the other classification algorithms , still it exhibits a competitive classification accuracy .
for the breast cancer dataset the chat - ohm achieved a performance of 95 % ( 9th place ) , only 2.34 % below the best performer , bayesnet .
for the liver disorders dataset the best classifier was randomforest , with a 68.44 % of classification accuracy , while the chat - ohm reached the 9th place with a difference of performance of 6.99 % .
for the case of haberman 's survival dataset chat - ohm achieved a classification accuracy of 66.36 % which leaves it in 18th place with 8.44 % below the best classifier , naivebayes .
the best performance for hepatitis disease dataset was achieved by randomforest with 90.61 % of classification accuracy , while the chat - ohm was positioned in the 13th place with a classification accuracy of 84.96 % .
notice , however , that despite not exhibiting the best performance for any given dataset , chat - ohm has a consistent behavior : the proposed method reached the 9th place for the breast cancer dataset and the liver disorders dataset , while being the 13th place for the hepatitis disease dataset and the 18th place for the haberman 's survival dataset .
on the other hand , bayes net is the best classifier for the breast cancer dataset , while being the 17th place for the liver disorders dataset , the 16th place at the haberman 's survival dataset , and the 9th place for the hepatitis disease dataset .
another example of this inconsistent performance is that of the naivebayes algorithm : it is the 5th place for brest cancer dataset , the worst method for the liver disorders dataset , the best at haberman 's survival dataset , and the 10th method for hepatitis disease dataset .
normalized datasets while performing the experiments , we noticed that some attribute values are significantly larger than the values of the rest of the attributes .
as recommended by [ 31 ] to avoid the impact of scale change , the dataset can be normalized .
the justification usually given for this normalization is that it prevents certain features from dominating merely because they have large numerical values .
table 4 shows the classification accuracy achieved by our proposal and the 19 classification algorithms from weka , when applied to normalized datasets .
for each dataset , the highest classification accuracy is emphasized with boldface .
in general , no significant variations were achieved with respect to the results of the datasets without normalization .
in most cases the improvement is less than 2 percent , with only two clear exceptions : votedperceptron and dmnbtext , which significantly increased their classification accuracy .
the former exhibits an improvement of 5.79 % for the breast cancer dataset and 6.94 % for the hepatitis disease dataset , while the latter shows an improvement of 24.99 % for the breast cancer dataset , 6.19 % for the liver disorders dataset and 9.61 % for the hepatitis disease dataset .
10.1371 / journal.pone.0095715.t004 table 4 clasification accuracy comparison (%) normalized data .
dataset algorithm breast cancer haberman 's survival hepatitis disease liver disorders adaboostm1 95.05 74.02 89.80 67.72 bayesnet 97.34 71.73 87.68 56.62 bftree 94.80 72.43 88.92 67.15 chat - ohm 95.52 62.45 89.52 58.5 complementnaivebayes n / a n / a n / a n / a conjunctiverule 91.98 72.97 89.05 56.28 decisiontable 95.69 71.90 88.46 58.83 dmnbtext 90.00 73.01 88.99 64.17 ib1 95.46 65.58 81.03 63.25 lwl 92.14 71.90 89.95 60.74 multiboostab 94.73 73.28 89.49 64.29 multischeme 65.01 73.53 79.38 57.98 naivebayes 96.11 74.66 87.36 55.42 naivebayesmultinomial n / a n / a n / a n / a randomforest 96.33 67.81 90.54 68.44 randomtree 94.79 64.96 83.70 62.75 rbfnetwork 96.36 73.75 85.78 64.81 smo 96.88 73.53 88.70 57.90 vote 65.01 73.53 79.38 57.98 votedperceptron 96.87 75.09 85.03 65.86 the performance of chat - ohm was not significantly affected by normalization , but for the hepatitis disease dataset the improvement of 4.56 % changes its rank from the 13th place ( table 3 ) to the 4th place ( table 4 ) .
the normalization method used in our experiments , produce both positive and negative normalized values .
this situation did not allow us to perform the experiment with two classification algorithms from weka : complementnaivebayes and naivebayesmultinomial , since these algorithms are unable to handle negative values .
outliers treatment during the testing phase , we also noticed the presence of some atypical pattern in the datasets .
to verify the presence of outliers in the datasets , a method for detection and deletion of outliers called wilson 's edition was applied to the datasets [ 28 ] .
table 5 shows the amount of outliers found and deleted from the four datasets using this technique .
the information presented by this table , shows that the breast cancer dataset presents only 3.22 % of outliers while haberman 's survival dataset presents 36.45 % .
the fact that most of the classifiers work much better for the breast cancer dataset , may be justified by the almost absence of outliers in this dataset .
the decision boundary between the classes appears to be better defined for the breast cancer dataset , thus the classification algorithms exhibit a higher classification accuracy than the one achieved with the other datasets , where the decision boundaries seem not so well defined .
10.1371 / journal.pone.0095715.t005 table 5 number of outliers for dataset .
original dataset outliers total instances class 1 instances class 2 instances class 1 outliers class 2 outliers total outliers outliers % breast cancer 683 444 239 11 11 22 3.22 haberman 's survival 345 145 200 71 55 126 36.52 hepatitis disease 306 225 81 25 59 84 27.45 liver disorders 155 32 123 14 13 27 17.41 table 6 shows the classification accuracy achieved by our proposal and the 19 classification algorithms from weka , when applied to datasets without outliers .
for each dataset , the highest classification accuracy is emphasized with boldface .
the removal of outliers leads to an improvement for all the classification algorithms presented in this work .
for the breast cancer dataset 22 outliers were removed , which represent the 3.22 % of the original dataset .
the improvement in the classification accuracy for this dataset varies from 0.5 % to 4.27 % .
the chat - ohm shows an improvement of 3.95 % for the breast cancer dataset , which changes its position from the 9th place ( table 3 ) to the 4th place ( table 6 ) , as mentioned before .
10.1371 / journal.pone.0095715.t006 table 6 clasification accuracy comparison (%) data without outliers .
dataset algorithm breast cancer haberman 's survival hepatitis disease liver disorders adaboostm1 98.41 94.84 96.24 83.44 bayesnet 99.46 87.59 93.72 76.98 bftree 97.84 93.39 96.40 84.43 chat - ohm 98.95 90.44 88.94 69.23 complementnaivebayes 86.79 90.60 86.72 64.44 conjunctiverule 93.78 89.52 95.25 66.53 decisiontable 97.16 88.32 94.31 77.73 dmnbtext 65.51 90.12 86.03 66.21 ib1 99.35 93.70 87.71 75.13 lwl 96.41 91.00 95.71 66.95 multiboostab 97.25 92.41 96.32 80.11 multischeme 65.51 90.12 86.03 66.21 naivebayes 97.93 95.25 97.63 58.54 naivebayesmultinomial 92.24 90.93 87.72 65.44 randomforest 98.55 93.79 96.10 87.36 randomtree 97.75 92.85 93.65 81.98 rbfnetwork 98.46 93.26 96.62 79.03 smo 99.50 91.24 92.63 66.21 vote 65.51 90.12 86.03 66.21 votedperceptron 92.90 90.39 85.40 73.89 table 7 shows the classification accuracy achieved by our proposal and the 19 classification algorithms from weka , when applied to normalized datasets without outliers .
for the liver disorder dataset , increases in the classification accuracy can be observed when compared with the experiments performed on the original datasets .
but if we compare the result of experiments with the datasets without outliers and the ones with the normalized datasets without outliers , the classification accuracy gets worse instead of better .
in general , it seems that for this dataset , it is better not to use normalization and instead rely on the removal of outliers .
on the other hand chat - ohm performed better with the normalized and outliers - free dataset .
the original performance was 61.45 % , the performance with the outliers - free dataset was 69.23 % and the performance with the normalized outliers - free dataset was 74.13 % ; with this improvement the classifier changes its rank from the 9th place with the original dataset ( table 3 ) to the 4th place with the normalized outliers - free dataset ( table 7 ) .
10.1371 / journal.pone.0095715.t007 table 7 clasification accuracy comparison (%) normalized data without outliers .
dataset algorithm breast cancer haberman 's survival hepatitis disease liver disorders adaboostm1 98.41 94.84 96.24 75.58 bayesnet 99.47 87.59 93.72 66.80 bftree 97.70 93.75 96.40 75.63 chat - ohm 97.69 90.53 89.18 74.13 complementnaivebayes n / a n / a n / a n / a conjunctiverule 93.78 89.52 96.33 61.41 decisiontable 97.17 88.32 94.31 68.28 dmnbtext 94.49 90.12 93.02 69.28 ib1 99.35 93.70 87.71 69.19 lwl 96.41 91.00 95.71 63.85 multiboostab 97.25 92.42 96.32 72.20 multischeme 65.51 90.12 86.03 62.10 naivebayes 97.79 95.20 97.02 56.85 naivebayesmultinomial n / a n / a n / a n / a randomforest 98.70 93.75 96.39 77.26 randomtree 97.65 92.77 91.67 71.97 rbfnetwork 98.46 93.26 96.92 71.92 smo 99.50 91.33 92.63 62.05 vote 65.51 90.12 86.03 62.10 votedperceptron 99.88 93.63 86.97 72.10 improvement analysis from the results presented in table 3 , 4 , 6 , and 7 , it is shown that there is no specific classification algorithm that exceed all the other algorithms in all the presented problems .
this claim is supported by the no - free - lunch theorems presented by wolpert and macready [ 30 ] , which establish that for an algorithm , any performance gain in one kind of problem is offset by its performance loss in other kind of problems .
table 8 , 9 , 10 , and 11 show the percentage of improvement achieved by our proposal and the 19 classification algorithms from weka , when applied to normalized datasets , datasets without outliers , and normalized datasets without outliers , for each of the four datasets used .
10.1371 / journal.pone.0095715.t008 table 8 comparison of classification improvement (%) for breast cancer dataset .
breast cancer algorithm normalization without outliers without outliers normalized adaboostm1 0.00 3.36 3.36 bayesnet 0.00 2.12 2.13 bftree - 0.08 2.96 2.82 chat - ohm 0.52 3.95 2.69 complementnaivebayes n / a 1.36 n / a conjunctiverule 0.00 1.80 1.80 decisiontable 0.00 1.47 1.48 dmnbtext 24.99 0.50 29.48 ib1 - 0.29 3.60 3.60 lwl 0.00 4.27 4.27 multiboostab 0.00 2.52 2.52 multischeme 0.00 0.50 0.50 naivebayes - 0.15 1.67 1.53 naivebayesmultinomial n / a 1.92 n / a randomforest - 0.14 2.08 2.23 randomtree 0.05 3.01 2.91 rbfnetwork 0.00 2.10 2.10 smo 0.01 2.63 2.63 vote 0.00 0.50 0.50 votedperceptron 5.79 1.82 8.8 10.1371 / journal.pone.0095715.t009 table 9 comparison of classification improvement (%) for haberman 's survival dataset .
haberman 's survival algorithm normalization without outliers without outliers normalized adaboostm1 0.00 20.82 20.82 bayesnet 0.00 15.86 15.86 bftree 0.10 21.06 21.42 complementnaivebayes n / a 16.73 n / a conjunctiverule 0.00 16.55 16.55 decisiontable 0.00 16.52 16.42 dmnbtext - 0.52 16.59 16.59 ib1 - 0.19 27.93 27.93 lwl 0.00 19.10 19.10 multiboostab 0.00 19.13 19.13 multischeme 0.00 16.59 16.59 naivebayes - 0.14 20.45 20.40 naivebayesmultinomial n / a 17.29 n / a randomforest - 0.13 25.85 25.81 randomtree 0.48 28.37 28.29 rbfnetwork 0.00 19.51 19.51 smo 0.00 17.91 18.00 vote 0.00 16.59 16.59 votedperceptron 1.27 16.57 19.81 chat - ohm - 3.91 24.08 24.17 10.1371 / journal.pone.0095715.t010 table 10 comparison of classification improvement (%) for hepatitis disease dataset .
hepatitis disease algorithm normalization without outliers without outliers normalized adaboostm1 0.00 6.44 6.44 bayesnet 0.00 6.04 6.04 bftree - 0.06 7.42 7.42 chat - ohm 4.56 3.98 4.22 complementnaivebayes n / a 9.18 n / a conjunctiverule 0.06 6.26 7.34 decisiontable 0.06 5.91 5.91 dmnbtext 9.61 6.65 13.64 ib1 0.00 6.68 6.68 lwl 0.25 6.01 6.01 multiboostab 0.00 6.83 6.83 multischeme 0.00 6.65 6.65 naivebayes - 0.14 10.13 9.52 naivebayesmultinomial n / a 9.72 n / a randomforest - 0.07 5.49 5.78 randomtree - 1.62 8.33 6.35 rbfnetwork 0.00 11.14 11.14 smo - 0.13 3.80 3.80 vote 0.00 6.65 6.65 votedperceptron 6.94 7.31 8.88 10.1371 / journal.pone.0095715.t011 table 11 comparison of classification improvement (%) for liver disorders dataset .
liver disorders algorithm normalization without outliers without outliers normalized adaboostm1 1.76 17.48 9.62 bayesnet - 0.23 20.13 9.95 bftree 0.71 17.99 9.19 chat - ohm - 2.95 7.78 12.68 complementnaivebayes n / a 7.87 n / a conjunctiverule 0.22 10.47 5.35 decisiontable - 0.28 18.62 9.17 dmnbtext 6.19 8.23 11.30 ib1 1.03 12.91 6.97 lwl - 0.06 6.15 3.05 multiboostab - 1.00 14.82 6.91 multischeme 0.00 8.23 4.12 naivebayes 0.53 3.65 1.96 naivebayesmultinomial n / a 8.48 n / a randomforest - 0.68 18.92 8.82 randomtree - 1.35 17.88 7.87 rbfnetwork - 0.25 13.97 6.86 smo - 0.08 8.23 4.07 vote 0.00 8.23 4.12 votedperceptron 2.33 10.36 8.57 for the brest cancer dataset , chat - ohm exhibits an improvement of 3.95 % , being the second algorithm with higher improvement when removing the outliers .
the dataset that presented greatest improvements with the removal of outliers was haberman 's survival .
on average the classification accuracy improved from 71.82 % to 91.49 % .
the improvements for this dataset vary from 15.86 % to 28.37 % .
the chat - ohm exhibit an improvement of 24.08 % when removing the outliers , positioning itself in the fourth place of the algorithms with higher improvements .
for the liver disorders dataset the improvements when removing the outliers vary from 3.65 % to 20.13 % .
the chat - ohm shows an improvement of 7.78 % , which is relatively low when compared with the improvements presented by the rest of the algorithms for this dataset. with the normalized outliers - free datasets , chat - ohm shows an improvement of 12.68 % with the liver disorder dataset and its rank changes from the 9th place to the 4th place .
also , it was the classifier with the best improvement for this dataset .
for the haberman 's survival dataset the model exhibit a 24.08 % of increase in its performance and it was the fourth best improvement for this dataset .
conclusions in this paper , we present a method that combines a hybrid associative classifier , a coding technique for output classes and a procedure of majority voting during the classification phase .
this method is called chat - ohm .
during the experimental phase , this method is applied to four different datasets related to the medical field .
the performance of the method is compared with 19 machine learning algorithms implemented in weka data mining software .
the proposed method uses an associative classifier , the chat [ 16 ] , combined with a novel coding technique and a voting procedure .
the results obtained demonstrate that the proposed method improved the result obtained by the chat .
however the experiments show that the chat - ohm is sensitive to the presence of outliers .
to improve the classification accuracy of this algorithm , it has to be combined with a method of detection and removal of outliers .
in the present work we use wilson 's edition as such method .
the chat - ohm presented fairly good results and a consistent behavior when applied to the four datasets used in this study .
the performance of the model was not significant affected by the normalization process .
on the other hand it was positive affected by the removal of outliers , displaying remarkable improvement in its performance , such as the ranking improvement for the breast cancer ( 4th place ) with a performance increase of 3.95 % .
another significant performance enhancement was obtained with the liver disorders dataset using normalization and outliers removal , the chat - ohm improved its rank to the 4th place with an increase of the classification accuracy of 12.68 % .
it should be mentioned that our proposal is part of a family of methods based on the chat classifier .
the main difference between these methods is the coding technique of each one , such as : modified johnson - mobius binary coding , gray coding , among others .
the authors would like to thank the instituto politecnico nacional ( secretaria academica , cofaa , sip , cic , and cidetec ) , the conacyt , and sni for their economical support to develop this work .
references 1 steinbuch k ( 1961 ) die lernmatrix . kybernetik 1 : 36 - 45 . 2 willshaw dj , buneman op , longuet - higgins hc ( 1969 ) non - holographic associative memory . nature 222 : 960 - 962 . 5789326 3 anderson ja ( 1972 ) a simple neural network generating an interactive memory . mathematical biosciences 14 : 197 - 220 . 4 kohonen t ( 1972 ) correlation matrix memories . ieee transactions on computers c - 21 : 353 - 359 . 5 nakano k ( 1972 ) associatron - a model of associative memory . ieee transactions on systems , man , and cybernetics smc - 2 : 380 - 388 . 6 amari si ( 1972 ) pattern learning by self - organizing nets of threshold elements . system and computing controls 3 : 15 - 22 . 7 hopfield jj ( 1982 ) neural networks and physical systems with emergent collective computational abilities . proceedings of the national academy of sciences 79 : 2554 - 2558 . 8 kosko b ( 1980 ) bidirectional associative memories . ieee transactions on systems , man , and cybernetics 18 : 49 - 60 . 9 ritter g , sussner p , diaz - de leon j ( 1998 ) morphological associative memories . ieee transactions on neural networks 9 : 281 - 293 . 18252452 10 lopez - yanez i , arguelles - cruz aj , camacho - nieto o , yanez - marquez c ( 2011 ) pollutants time - series prediction using the gamma classifier . international journal of computational intelligence systems 4 : 680 - 711 . 11 mathai g , upadhyaya b ( 1989 ) performance analysis and application of the bidirectional associative memory to industrial spectral signatures . international joint conference on neural networks 1 : 33 - 37 . 12 guzman e , pogrebnyak ob , yanez c , moreno ja ( 2006 ) image compression algorithm based on morphological associative memories . progress in pattern recognition , image analysis and applications 4225 : 519 - 528 . 13 chartier s , lepage r ( 2002 ) learning and extracting edges from images by a modified hopfield neural network . proceedings of the 16th international conference on pattern recognition 3 : 431 - 434 . 14 yanez - marquez c , diaz de leon jl ( 2001 ) .
lernmatrix de steinbuch , mexico : it 48 , serie verde , cic - ipn .
15 yanez - marquez c , diaz de leon jl ( 2001 ) .
linear associator de anderson - kohonen , mexico : it 50 , serie verde , cic - ipn .
16 santiago - montero r ( 2003 ) .
clasificador hibrido de patrones basado en la lernmatrix de steinbuch y el linear associator de anderson - kohonen .
msc thesis , cic , ipn , mexico .
17 university of california , irvine machine learning repository website .
bache k , lichman m ( 2013 ) uci machine learning repository .
available : http :// archive.ics.uci.edu / ml irvine , ca : university of california , school of information and computer science .
accessed 2014 april 3 .
18 weka website .
hall m , frank e , holmes g , pfahringer b , reutemann p , witten ih ( 2010 ) weka 3 : data mining software in java .
available : http :// www.cs.waikato.ac.nz / ml / weka / accessed 2014 april 3 .
19 witten ih , frank e , hall m a ( 2011 ) data mining practical machine learning tools and techniques. elsevier .
20 breiman l ( 2001 ) random forests . machine learning 45 : 5 - 32 . 21 platt jc ( 1998 ) fast training of support vector machines using sequential minimal optimization .
in : schoelkopf b , burges c , smola a ( eds. ) .
advances in kernel methods - support vector learning .
mit press .
22 freund y , schapire re ( 1996 ) experiments with a new boosting algorithm .
thirteenth international conference on machine learning : 148 - 156 .
23 webb gi ( 2000 ) multiboosting : a technique for combining boosting and wagging . machine learning 40 : 159 - 196 . 24 john gh , langley p ( 1995 ) estimating continuous distributions in bayesian classifiers .
eleventh conference on uncertainty in artificial intelligence : 338 - 345 .
25 christofides n ( 1975 ) graph theory : an algorithmic approach ( computer science and applied mathematics ) .
orlando : academic press , inc .
26 rennie jdm , shih l , teevan j , karger dr ( 2003 ) tackling the poor assumptions of naive bayes text classifiers .
proceeding of the twentieth international conference on machine learning : 616 - 623 .
27 kohavi r ( 1995 ) the power of decision tables .
8th european conference on machine learning : 174 - 189 .
28 wilson dl ( 1972 ) asymptotic properties of nearest neighbor rule using edited data . ieee transactions on systems , man , and cybernetics 3 : 408 - 421 . 29 kohavi r ( 1995 ) a study of cross - validation and bootstrap for accuracy estimation and model selection . proceedings of the fourteenth international joint conference on artificial intelligence 2 : 1137 - 1145 . 30 wolpert dh , macready wg ( 1997 ) no free lunch theorems for optimization . ieee transactions on evolutionary computation 1 : 67 - 82 . 31 duda ro , hart pe , stork dg ( 2001 ) pattern classification .
united state : wiley .