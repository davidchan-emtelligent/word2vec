biomed res int biomed res int bmri biomed research international 2314 - 6133 2314 - 6141 hindawi publishing corporation 25254202 4165507 10.1155 / 2014 / 134023 review article managing , analysing , and integrating big data in medical bioinformatics : open problems and future perspectives merelli ivan 1 perez - sanchez horacio 2 gesing sandra 3 d'agostino daniele 4 * 1bioinformatics research unit , institute for biomedical technologies , national research council of italy , segrate , 20090 milan , italy 2bioinformatics and high performance computing research group ( bio - hpc ) , computer science department , universidad catolica san antonio de murcia ( ucam ) , 30107 murcia , spain 3department of computer science and engineering , center for research computing , university of notre dame , p.o. box 539 , notre dame , in 46556 , usa 4advanced computing systems and high performance computing group , institute of applied mathematics and information technologies , national research council of italy , 16149 genoa , italy * daniele d'agostino : dagostino @ ge.imati.cnr.it academic editor : carlo cattani 2014 1 9 2014 2014 134023 18 6 2014 13 8 2014 copyright ( c ) 2014 ivan merelli et al .
2014 this is an open access article distributed under the creative commons attribution license , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .
the explosion of the data both in the biomedical research and in the healthcare systems demands urgent solutions .
in particular , the research in omics sciences is moving from a hypothesis - driven to a data - driven approach .
healthcare is additionally always asking for a tighter integration with biomedical data in order to promote personalized medicine and to provide better treatments .
efficient analysis and interpretation of big data opens new avenues to explore molecular biology , new questions to ask about physiological and pathological states , and new ways to answer these open issues .
such analyses lead to better understanding of diseases and development of better and personalized diagnostics and therapeutics .
however , such progresses are directly related to the availability of new solutions to deal with this huge amount of information .
new paradigms are needed to store and access data , for its annotation and integration and finally for inferring knowledge and making it available to researchers .
bioinformatics can be viewed as the " glue " for all these processes .
a clear awareness of present high performance computing ( hpc ) solutions in bioinformatics , big data analysis paradigms for computational biology , and the issues that are still open in the biomedical and healthcare fields represent the starting point to win this challenge .
1. introduction the increasing availability of omics data resulting from improvements in the acquisition of molecular biology results and in systems biology simulation technologies represents an unprecedented opportunity for bioinformatics researchers , but also a major challenge .
a similar scenario arises for the healthcare systems , where the digitalization of all clinical exams and medical records is becoming a standard in hospitals .
such huge and heterogeneous amount of digital information , nowadays called big data , is the basis for uncovering hidden patterns in data , since it allows the creation of predictive models for real - life biomedical applications .
but the main issue is the need of improved technological solutions to deal with them .
a simple definition of big data is based on the concept of data sets whose size is beyond the management capabilities of typical relational database software .
a more articulated definition of big data is based on the three versus paradigm : volume , variety , and velocity [ 1 ] .
the volume recalls for novel storage scalability techniques and distributed approaches for information query and retrieval .
the second v , the variety of the data source , prevents the straightforward use of neat relational structures .
finally , the increasing rate at which data is generated , the velocity , has followed a similar pattern as the volume .
this " need for speed ," particularly for web - related applications , has driven the development of techniques based on key - value stores and columnar databases behind portals and user interfaces , because they can be optimized for the fast retrieval of precomputed information .
thus , smart integration technologies are required for merging heterogeneous resources : promising approaches are the use of technologies relying on lighter placement with respect to relational databases ( i.e. , nosql databases ) and the exploitation of semantic and ontological annotations .
although the big data definition can still be considered quite nebulous , it does not represent just a keyword for researchers or an abstract problem : the usa administration launched a 200 million dollar " big data research and development initiative " in march 2012 , with the aim to improve tools and techniques for the proper organization , efficient access , and smart analysis of the huge volume of digital data [ 2 ] .
such a high amount of investments is justified by the benefit that is expected from processing the data , and this is particularly true for omics science .
a meaningful example is represented by the projects for population sequencing .
the first one is the 1000 genomes [ 3 ] , which provides researchers with an incredible amount of raw data .
then , the encode project [ 4 ] , a follow - up to the human genome project ( genomic research ) [ 5 ] , is having the aim of identifying all functional elements in the human genome .
presently , this research is moving at a larger scale : as clearly appears considering the genome 10k project [ 6 ] and the more recent 100k genomes project [ 7 ] .
just to provide an order of magnitude , the amount of data produced in the context of the 1000 genomes project is estimated in 100 terabytes ( tb ) , and the 100k genomes project is likely to produce 100 times such data .
the targeting cost for sequencing a single individual will reach soon $ 1000 [ 8 ] , which is affordable not only for large research projects but also for individuals .
we are running into the paradox that the cheapest solution to cope with these data will be to resequence genomes when analyses are needed instead of storing them for future reuse [ 9 ] .
storage represents only one side of the medal .
the final goal of research activities in omics sciences is to turn such amount of data into usable information and real knowledge .
biological systems are very complex , and consequently the algorithms involved in analysing them are very complex as well .
they still require a lot of effort in order to improve their predictive and analytical capabilities .
the real challenge is represented by the automatic annotation and / or integration of biological data in real - time , since the objective to reach is to understand them and to achieve the most important goal in bioinformatics : mining information .
in this work we present a brief review of the technological aspects related to big data analysis in biomedical informatics .
the paper is structured as follows .
in section 2 architectural solutions for big data are described , paying particular attention to the needs of the bioinformatics community .
section 3 presents parallel platforms for big data elaboration , while section 4 is concerned with the approaches for data annotation , specifically considering the methods employed in the computational biology field .
section 5 introduces data access measures and security for biomedical data .
finally , section 6 presents some conclusions and future perspective .
a tag crowd diagram of the concepts presented in the paper is shown in figure 1 .
2. big data architectures domains concerned with data - intensive applications have in common the abovementioned three versus , even though the actual way by which this information is acquired , stored , and analysed can vary a lot from field to field .
the main common aspect is represented by the requirements for the underlying it architecture .
the mere availability of disk arrays of several hundreds of tbs is in fact not sufficient , because the access to the data will have , statistically , some fails [ 10 ] .
thus , reliable storage infrastructures have to be robust with respect to these problems .
moreover , the analysis of big data needs frequent data access for the analysis and integration of information , resulting in considerable data transfer operations .
even though we can assume the presence of a sufficient amount of bandwidth inside a cluster , the use of distributed computing infrastructure requires adopting effective solutions .
other aspects have also to be addressed , as secure access policies to both the raw data and the derived results .
choosing a specific architecture and building an appropriate big data system are challenging because of diverse heterogeneous factors .
all the major vendors as ibm [ 11 ] , oracle [ 12 ] , and microsoft [ 13 ] propose solutions ( mostly business - oriented ) based on their software ecosystems .
here we will discuss the major architectural aspects taking into account open source projects and scientific experiences .
2.1 .
managing and accessing big data the first and obvious concern with big data is the volume of information that researchers have to face , especially in bioinformatics .
at lower level this is an it problem of file systems and storage reliability , whose solution is not obvious and not unique .
open questions are what file system to choose and will the network be fast enough to access this data .
the issue arising in data access and retrieval can be highlighted with a simple consideration [ 14 ] : scanning data on a modern physical hard disk can be done with a throughput of about 100 megabytes / s. therefore , scanning 1 terabyte takes 5 hours and 1 petabyte takes 5000 hours .
the big data problem does not only rely in archiving and conserving huge quantity of data .
the real challenge is to access such data in an efficient way , applying massive parallelism not only for the computation , but also for the storage .
moreover , the transfer rate is inversely proportional to the distance to cover .
hpc clusters are typically equipped with high - level interconnections as infiniband , which have a latency of 2000 ns ( only 20 times slower than ram and 200 times faster than a solid state disk ) and data rates ranging from 2.5 gigabit per second ( gbps ) with a single data rate link ( sdr ) , up to 300 gbps with a 12 - link enhanced data rate ( edr ) connection .
but this performance can only be achieved on a lan , and the real problems arise when it is necessary to transfer data between geographically distributed sites , because the internet connection might not suitable to transfer big data .
although several projects , such geant [ 15 ] , the pan - european research and education network , and its us counterpart internet2 [ 16 ] , have been funded to improve the network interconnections among states ; the achievable bandwidth is insufficient .
for example , bgi , the world largest genomics service provider , uses fedex for delivering results [ 17 ] .
if a local infrastructure is exploited for the analysis of big data , one effective solution is represented by the use of client / server architectures where the data storage is spread among several devices and made accessible through a ( local ) network .
the available tools can be mainly subdivided in distributed file systems , cluster file systems , and parallel file systems .
distributed file systems consist of disk arrays physically attached to a few i / o servers through fast networks and then shared to the other nodes of a cluster .
in contrast , cluster file systems provide direct disk access from multiple computers at the block level ( access control must take place on the client node ) .
parallel file systems are like cluster file systems , where multiple processes are able to concurrently read and write a single file , but exploit a client - server approach , without direct access to the block level .
an exhaustive taxonomy and a survey are provided in [ 18 ] .
two of the highest performance parallel file systems are the general parallel file system ( gpfs ) [ 19 ] , developed by ibm , and lustre [ 20 ] , an open source solution .
most of the supercomputers employ them : in particular , lustre is used in titan , the second supercomputer of the top500 list ( november 2013 ) [ 21 ] .
the titan storage subsystem contains 20,000 disks , resulting in 40 petabyte of storage and about 1 terabyte / sec of storage bandwidth .
as regards gpfs , it was recently exploited within the elastic storage [ 22 ] , the ibm file management solution working as a control plane for smart data handling .
the software can automatically move less frequently accessed data to the less expensive storage available in the infrastructure , while leaving faster and more expensive storage resources ( i.e. , ssd disk or flash ) for more important data .
the management is guided by analytics , using patterns , storage characteristics , and the network to determine where to move the data .
a different solution is represented by the hadoop distributed file system ( hdfs ) [ 23 ] , a java - based file system that provides scalable and reliable data storage that is designed to span large clusters of commodity servers .
it is an open source version of the googlefs [ 24 ] introduced in 2003 .
the design principles were derived from google 's needs , as the fact that most files only grow because new data have to be appended , rather than overwriting the whole file , and that high sustained bandwidth is more important than low latency .
as regards i / o operations , the key aspects are the efficient support for large streaming or small random reads , besides large and sequential writes to append data to files .
the other operations are supported as well , but they can be implemented in a less efficient way .
at the architectural level , hdfs requires two processes : a namenode service , running on one node in the cluster and a datanode process running on each node that will process data .
hdfs is designed to be fault - tolerant due to replication and distribution of data , since every loaded file is replicated ( 3 times is the default value ) and split into blocks that are distributed across the nodes .
the namenode is responsible for the storage and management of metadata , so that when an application requires a file , the namenode informs about the location of the needed data .
whenever a data node is down , the namenode can redirect the request to one of the replicas until the data node is back online .
since the cluster size can be very large ( it was demonstrated with clusters up to 4,000 nodes ) , the single namenode per cluster forms a possible bottleneck and single point of failure .
this can be mitigated by the fact that metadata can be stored in the main memory and the recent hdfs high availability feature provides the user with the option of running two redundant namenodes in the same cluster , one of them in standby , but ready to intervene in case of failure of the other .
2.2 .
middleware for big data the file system is the first level of the architecture .
the second level corresponds to the framework / middleware supporting the development of user - specific solutions , while applications represent the third level .
besides the general - purpose solutions for parallel computing like the message passing interface [ 25 ] , hybrid solutions based on it [ 26 , 27 ] , or extension to specific frameworks like the r software environment for statistical computing [ 28 ] , there are several tools specifically developed for big data analysis .
the first and more famous example is the abovementioned apache hadoop [ 23 ] , an open source software framework for large - scale storing and processing of data sets on distributed commodity hardware .
hadoop is composed of two main components , hdfs and mapreduce .
the latter is a simple framework for distributed processing based on the map and reduce functions , commonly used in functional programming .
in the map step the input is partitioned by a master process into smaller subproblems and then distributed to worker processes .
in the reduce step the master process collects the results and combines them in some way to provide the answer to the problem it was originally trying to solve .
hadoop was designed as a platform for an entire ecosystem of capabilities [ 29 ] , used in particular by a large number of companies , vendors , and institutions [ 30 ] .
to provide an example in the bioinformatics field , in the cancer genome atlas project researchers implemented the process of " sharding ," splitting genome data into smaller more manageable chunks for cluster - based processing , utilising the hadoop framework and the genome analysis toolkit ( gatk ) [ 31 , 32 ] .
many other works based on hadoop are present in literature [ 33 - 36 ] ; for example , some specific libraries were developed as hadoop - bam , a library for distributed processing of genetic data from next generation sequencer machines [ 37 ] , and seal , a suite of distributed applications for aligning short dna reads , manipulating short read alignments , and analyse the achieved results [ 38 ] , was also developed relying on this framework .
hadoop was the basis for other higher - level solutions as apache hive [ 39 ] , a distributed data warehouse infrastructure for providing data summarization , query , and analysis .
apache hive supports analysis of large datasets stored in hdfs and compatible file systems such as the amazon s3 file system .
it provides an sql - like language called hiveql while maintaining full support for map / reduce .
to accelerate queries , it provides indexes , including bitmap indexes , and it is worth exploiting in several bioinformatics applications [ 40 ] .
apache pig [ 41 ] has the similar aim to allow domain experts , who are not necessarily it specialists , to write complex mapreduce transformations using a simpler scripting language .
it was used for example for sequence analysis [ 33 ] .
hadoop is considered almost as synonymous for big data .
however , there are some alternatives based on the same mapreduce paradigm like disco [ 42 ] , a distributed computing framework aimed at providing a mapreduce platform for big data processing using python application , that can be coupled with the biopython toolkit [ 43 ] of the open bioinformatics foundation , storm [ 44 ] , a distributed real - time computation system for processing fast , and large streams of data and proprietary systems , for example , from software ag , lexisnexis , and parstream ( see their websites for more information ) .
3. computational facilities for analysing big data the traditional platforms for operating the software frameworks that facilitate big data analysis are hpc clusters , possibly accessed via grid computing infrastructures [ 45 ] .
such approach has however the possible drawback to provide an insufficient possibility to customize the computational environment if the computing facilities are not owned by the scientists that will analyze the data .
this is one of the reasons why cloud computing services increased their importance as economic solutions to perform large - scale analysis on an as - needed basis , in particular for medium - small laboratories that cannot afford the cost to buy and maintain a sufficiently powerful infrastructure [ 46 ] .
in this section we will very briefly review bioinformatics applications or projects exploiting these platforms .
3.1 .
cluster computing the data parallel approach , that is , the parallelization paradigm that subdivides the data to analyse among almost independent processes , is a suitable solution for many kinds of big data analysis that results in high scalability and performance figures .
the key issues while developing applications using data parallelism are the choice of the algorithm , the strategy for data decomposition , load balancing among possibly heterogeneous computing nodes , and the overall accuracy of the results [ 47 ] .
an example of big data analysis based on this approach in the field of bioinformatics concerns the de novo assembly algorithms , which typically work finding the fragments that " overlap " in the sequenced reads and recording these overlaps in a huge diagram called de bruijn ( or assembly ) graph [ 48 ] .
for a large genome , this graph can occupy many terabytes of ram , and completing the genome sequence can require days of computation on a world - class supercomputer .
this is the reason why memory distributed approaches , such as abyss [ 49 ] , are now widely exploited , although the implementation of efficient multisever approaches has required huge effort and is still under active development .
generally , we can say that data parallel approaches are straightforward solutions for inferring correlations , but not for causality .
in these cases semantics and ontological techniques are of particular importance , as those described in section 4 .
3.2 .
gpu computing hpc technologies are the forefront of accelerated data analysis revolutions , making it possible to carry out processing breakthroughs that would directly translate into real benefits for the society and the environment .
the use of accelerator devices such as gpus represents a cost - effective solution able to support up to 11.5 teraflops within a single device ( i.e. , the amd radeon r9 295x2 graphics card ) at about $ 1,500 .
moreover , large clusters are adopting the use of these relatively inexpensive and powerful devices as a way of accelerating parts of the applications they are running .
presently , most of the top 10 systems from the top500 list ( november 2013 ) are equipped with accelerators , and in particular titan , the second system in the list , achieved 17.59 petaflop on the linpack benchmark also thanks to its 18,688 nvidia tesla k20 devices .
driven by the demand of the game industry , gpus have completed a steady transition from mainframes to workstations pc cards , where they emerge nowadays like a solid and compelling alternative to traditional computing platforms .
gpus deliver extremely high floating - point performance and massively parallelism at a very low cost , thus promoting a new concept of the high performance computing market .
for example , in heterogeneous computing , processors with different characteristics work together to enhance the application performance taking care of the power budget .
this fact has attracted many researchers and encouraged the use of gpus in a broader range of applications , particularly in the field of bioinformatics .
developers are required to leverage this new landscape of computation with new programming models , which ease the developers task of writing programs to run efficiently on such platforms altogether [ 50 ] .
the most popular microprocessor companies such as nvidia , ati / amd , or intel , have developed hardware products aimed specifically at the heterogeneous or massively parallel computing market : tesla products are from nvidia , fire - stream is the amds device line , and intel xeon phi comes from intel .
they have also released software components , which provide simpler access to this computing power .
cuda ( compute unified device architecture ) is nvidias solution as a simple block - based api for programming ; amds alternative was called stream computing , while intel relies directly on x86 - based programming .
opencl [ 51 ] emerged as an attempt to unify all of these models with a superset of features , being the best broadly supported multiplatform data parallel programming interface for heterogeneous computing , including gpus , accelerators , and similar devices .
although these efforts in developing programming models have made great contributions to leverage the capabilities of these platforms , developers have to deal with a massively parallel and high - throughput - oriented architecture [ 52 ] , which is quite different than traditional computing architectures .
moreover , gpus are being connected with cpus through pci express bus to build heterogeneous parallel computers , presenting multiple independent memory spaces , a wide spectrum of high speed processing functions , and some communication latencies between them .
these issues drastically increase scaling to a gpu cluster , bringing additional sources of complexity and latency .
therefore , programmability on these platforms is still a challenge , and thus many research efforts have provided abstraction layers avoiding dealing with the hardware particularities of these accelerators and also extracting transparently high level of performance , providing portability across operating systems , host cpus , and accelerators .
for example , libraries and interfaces exist for developing with popular programming languages like ompss for openmp or openacc api , which describes a collection of compiler directives to loops specific regions of code in standard programming languages such as c , c ++ , or fortran .
although the complexity of these architectures is high , the performance that such devices are able to provide justifies the great interest and efforts in porting bioinformatics application on them [ 53 ] .
3.3 .
xeon phi based on intel 's many integrated core ( mic ) x86 - based architecture , intel xeon phi coprocessors provide up to 61 cores and 1.2 teraflops of performance .
these devices equip the first supercomputer of the top500 list ( november 2013 ) , tianhe - 2 .
in terms of usability , there are two ways an application can use an intel xeon phi : in offload mode or in native mode .
in offload mode the main application is running on the host , and it only offloads selected ( highly parallel , computationally intensive ) work to the coprocessor .
in native mode the application runs independently , on the xeon phi only , and can communicate with the main processor or other coprocessors through the system bus .
the performance of these devices heavily depends on how well the application fits the parallelization paradigm of the xeonphi and in relation to the optimizations that are performed .
in fact , since the processors on the xeon phi have a lower clock frequency with respect to the common intel processor unit ( such as for example the sandy bridge ) , applications that have long sequential part of the algorithm are absolutely not suitable for the native mode .
on the other hand , even if the programming paradigm of these devices is standard c / c ++ , which makes their use simpler with respect to the necessity of exploiting a different programming language such as cuda , in order to achieve good performance , the code must be heavily optimized to fit the characteristics of the coprocessor ( i.e. , exploiting optimizations introduced by the intel compiler and the mkl library ) .
looking at the performance tests released by intel [ 54 ] , the baseline improvement of supporting two intel sandy bridge by offloading the heavy parallel computational to an intel xeon phi gives an average improvement of 1.5 in the scalability of the application that can reach up to 4.5 of gain after a strong optimization of the code .
for example , considering typical tools for bioinformatics sequence analysis : bwa ( burrows - wheeler alignment ) [ 55 ] reached a baseline improvement of 1.86 and hmmer of 1.56 [ 56 ]. with a basic recompilation of blastn for the intel xeon phi [ 57 ] there is an improvement of 1.3 , which reaches 4.5 after some modifications to the code in order to improve the parallelization approach .
same scalability figures for abyss , which scales 1.24 with a basic porting and 4.2 with optimizations in the distribution of the computational load .
really good performance is achieved for bowtie , which improves the code passing from a scalability of 1.3 to 18.4 .
clearly , the real competitors of the intel xeon phi are the gpus devices .
at the moment , the comparison between the best devices provided by intel - - xeon phi 7100 - - and nvidia - - k40 - - shows that the gpu is on average 30 % more performing [ 58 ] , but the situation can vary in the future .
3.4 .
cloud computing advances in life sciences and information technology bring profound influences on bioinformatics due to its interdisciplinary nature .
for this reason , bioinformatics is experiencing a new trend in the way analysis is performed : computation is moving from in - house computing infrastructure to cloud computing delivered over the internet .
this has been necessary in order to handle the vast quantities of biological data generated by high - throughput experimental technologies .
cloud computing in particular promises to address big data storage and analysis issues in many fields of bioinformatics .
but moving data to the cloud can also be a problem ; so hybrid solutions of cloud computing are arising .
the point can be summarized as follows : data that is too big to be processed conventionally is also too big to be transported anywhere .
it is undergoing an inversion of priorities : the code to perform the analysis has to be moved , not the data .
virtualization represents an enabling technology to achieve this result .
one of the most famous free portal / software for the analysis of bioinformatics data , galaxy by the penn state university , is available on cloud [ 59 ] .
the idea is that with sporadic availability of data , individuals and labs may have a need to , over a period of time , process greatly variable amounts of data .
such variability in data volume imposes variable requirements on availability of compute resources used to process given data .
rather than having to purchase and maintain desired compute resources or having to wait a long time for data processing jobs to complete , the galaxy team has enabled galaxy to be instantiated on cloud computing infrastructures , primarily amazon elastic compute cloud ( ec2 ) .
an instance of galaxy on the cloud behaves just like a local instance of galaxy except that it offers the benefits of cloud computing resource availability and pay - as - you - go resource ownership model .
having simple access to galaxy on the cloud enables as many instances of galaxy to be acquired and started as is needed to process given data .
once the need subsides , those instances can be released as simply as they were acquired. with such a paradigm , one pays only for the resources they need and use , while all the other concerns and costs are eliminated .
for a complete review on bioinformatics clouds for big data manipulation , see [ 60 ] .
concerning the exploitation of cloud computing to cope with the data flow produced by high - throughput molecular biology technologies , see [ 61 ] .
4. semantics , ontologies , and open format for data integration the previous sections focused mainly on how to analyse big data for inferring correlations , but the extraction of actual new knowledge requires something more .
the key challenges in making use of big data lie , in fact , in finding ways of dealing with heterogeneity , diversity , and complexity of the information , while its volume and velocity hamper solutions are available for smaller datasets such as manual curation or data warehousing .
semantic web technologies are meant to deal with these issues .
the development of metadata for biological information on the basis of semantic web standards can be seen as a promising approach for a semantic - based integration of biological information [ 62 ] .
on the other hand , ontologies , as formal models for representing information with explicitly defined concepts and relationships between them , can be exploited to address the issue of heterogeneity in data sources .
in domains like bioinformatics and biomedicine , the rapid development and adoption of ontologies [ 63 ] prompted the research community to leverage them for the integration of data and information .
finally , since the advent of linked data a few years ago , it has become an important technology for semantic and ontologies research and development .
we can easily understand linked data as being a part of the greater big data landscape , as many of the challenges are the same .
the linking component of linked data , however , puts an additional focus on the integration and conflation of data across multiple sources .
4.1 .
semantics the semantic web is a collaborative movement , which promoted standard for the annotation and integration of data. by encouraging the inclusion of semantic content in data accessible through the internet , the aim is to convert the current web , dominated by unstructured and semistructured documents , into a web of data .
it involves publishing information in languages specifically designed for data : resource description framework ( rdf ) , web ontology language ( owl ) , sparql ( which is a protocol and query language for semantic web data sources ) , and extensible markup language ( xml ) [ 64 ] .
rdf represents data using subject - predicate - object triples , also known as " statements. " this triple representation connects data in a flexible piece - by - piece and link - by - link fashion that forms a directed labelled graph .
the components of each rdf statement can be identified using uniform resource identifiers ( uris ) .
alternatively , they can be referenced via links to rdf schemas ( rdfs ) , web ontology language ( owl ) , or to other ( nonschema ) rdf documents .
in particular , owl is a family of knowledge representation languages for authoring ontologies or knowledge bases .
the languages are characterised by formal semantics and rdf / xml - based serializations for the semantic web .
in the field of biomedicine , a notable example is the open biomedical ontologies ( obo ) project , which is an effort to create controlled vocabularies for shared use across different biological and medical domains .
obo belongs to the resources of the u.s. national center for biomedical ontology ( ncbo ) , where it will form a central element of the ncbo 's bioportal .
the interrogation of these resources can be performed using sparql , which is an rdf query language similar to sql , for the interrogations of databases , able to retrieve and manipulate data stored in rdf format .
for example , biogateway [ 65 ] organizes the swissprot database , along with gene ontology annotations ( goa ) , into an integrated rdf database that can be accessed through a sparql query endpoint , allowing searches for proteins based on a combination of go and swissprot data .
the support for rdf in high - throughput bioinformatics applications is still small , although researchers can already download the uniprotkb and its taxonomy information using this format or they can get ontologies in owl format , such as go [ 66 ] .
the biggest impact rdf and owl can have in bioinformatics , though , is to help integrate all data formats and standardise existing ontologies .
if unique identifiers are converted to uri references , ontologies can be expressed in owl , and data can be annotated via these rdf - based resources .
the integration between them is a matter of merging and aligning the ontologies ( in case of owl using the " rdf : sameas " statement ) .
after the data has been integrated we can use the plus that comes with rdf for reasoning : context embeddedness .
organizations in the life sciences are currently using rdf for drug target assessment [ 67 , 68 ] , and for the aggregation of genomic data [ 69 ] .
in addition , semantic web technologies are being used to develop well - defined and rich biomedical ontologies for assisting data annotation and search [ 70 , 71 ] , the integration of rules to specify and implement bioinformatics workflows [ 72 ] , and the automation for discovering and composing bioinformatics web services [ 73 ] .
4.2 .
ontologies an ontology layer is often an invaluable solution to support data integration [ 74 ] , particularly because it enables the mapping of relations among data stored in a database .
belonging to the field of knowledge representation , an ontology is a collection of terms within a particular domain organised in a hierarchical structure that allows searching at various levels of specificity .
ontologies provide a formal representation of a set of concepts through the description of individuals , which are the basic objects , classes , that are the categories which objects belong to , attributes , which are the features the objects can present , and relations , that are the ways objects can be related to one another .
due to this " tree " ( or , in some cases , " graph ") representation , ontologies allow the link of terms from the same domain , even if they belong to different sources in data integration contexts , and the efficient matching of apparently diverse and distant entities .
the latter aspect can not only improve data integration , but even simplify the information searching .
in the biomedical context a common problem concerns , for example , the generality of the term cancer [ 75 ] .
a direct query on that term will retrieve just the specific word in all the occurrences found into the screened resource .
employing a specialised ontology ( i.e. , the human disease ontology - - doid ) [ 76 ] the output will be richer , including terms such as sarcoma and carcinoma that will not be retrieved otherwise .
ontology - based data integration involves the use of ontologies to effectively combine data or information from multiple heterogeneous sources [ 63 ] .
the effectiveness of ontology - based data integration is closely tied to the consistency and expressivity of the ontology used in the integration process .
many resources exist that have ontology support : snpranker [ 77 ] , g2sbc [ 78 ] , nsd [ 79 ] , tmarepdb [ 80 ] , surface [ 81 ] , and cell cycle db [ 82 ] .
a useful instrument for ontology exploration has been developed by the european bioinformatics institute ( ebi ) , which allows easily visualising and browsing ontologies in the obo format : the open source ontology lookup service ( ols ) [ 83 ] .
the system provides a user - friendly web - based single point to look into the ontologies for a single specific term that can be queried using a useful autocompletion search engine .
otherwise it is possible to browse the complete ontology tree using an ajax library , querying the system through a standard soap web service described by a wsdl descriptor .
the following ontologies are commonly used for annotation and integration of data in the biomedical and bioinformatics .
gene ontology ( go ) [ 84 ] , which is the most exploited multilevel ontology in the biomolecular domain .
it collects genome and proteome related information in a graph - based hierarchical structure suitable for annotating and characterising genes and proteins with respect to the molecular function ( i.e. , go : 0070402 : nadph binding ) and biological process they are involved in ( i.e. , go : 0055114 : oxidation reduction ) , and the spatial localisation they present within a cell ( i.e. , go : 0043226 : organelle ) .
kegg ontology ( kont ) , which provides a pathway based annotation of the genes in all organisms .
no obo version of this ontology was found , since it has been generated directly starting from data available in the related resource [ 85 ] .
brenda tissue ontology ( bto ) [ 86 ] , to support the description of human tissues .
cell ontology ( cl ) [ 87 ] , to provide an exhaustive organisation about cell types .
disease ontology ( doid ) [ 76 ] , which focus on the classification of breast cancer pathology compared to the other human diseases .
protein ontology ( pro ) [ 88 ] , which describes the protein evolutionary classes to delineate the multiple protein forms of a gene locus .
medical subject headings thesaurus ( mesh ) [ 89 ] , which is a hierarchical controlled vocabulary able to index biomedical and health - related information .
protein structure classification ( cath ) [ 90 ] , which is a structured vocabulary used for the classification of protein structures .
4.3 .
linked data linked data describes a method for publishing structured data so that they can be interlinked , making clearer the possible interdependencies .
this technology is built upon the semantic web technologies previously described ( in particular it uses http , rdf , and uris ) , but rather than using them to serve web pages for human readers , it extends them to share information in a way that can be read automatically by it systems .
the linked data paradigm is one approach to cope with big data as it advances the hypertext principle from a web of documents to a web of rich data .
the idea is that after making data available on the web ( in an open format and with an open license ) and structuring them in a machine - readable fashion ( e.g. , excel instead of image scan of a table ) , researchers must work to annotate this information with open standards from w3c ( rdf and sparql ) , so that people can link their own data to other people 's data to provide context information .
in the field of bioinformatics , a first attempt to publish linked data has been performed by the bio2rdf project [ 91 ] .
the project 's goal is to create a network of coherently linked data across the biological databases .
as part of the bio2rdf project , an integrated bioinformatics warehouse on the semantic web has been built .
bio2rdf has created a rdf warehouse that serves over 70 million triples describing the human and mouse genomes [ 92 ] .
a very important step towards the use of linked data in the computational biology field has been done by the abovementioned ebi [ 93 ] , which developed an infrastructure to access its information by exploiting this paradigm .
in detail , the ebi rdf platform allows explicit links to be made between datasets using shared semantics from standard ontologies and vocabularies , facilitating a greater degree of data integration .
sparql provides a standard query language for querying rdf data .
data that have been annotated using ontologies , such as doid and the go , can be integrated with other community datasets , providing the semantics support to perform rich queries .
publishing such datasets as rdf , along with their ontologies , provides both the syntactic and semantic integration of data , long promised by semantic web technologies .
as the trend toward publishing life science data in rdf increases , we anticipate a rise in the number of applications consuming such data .
this is evident in efforts such as the open phacts platform [ 94 ] and the atlasrdf - r package [ 95 ] .
the final aim is that ebi rdf platform can enable such applications to be built by releasing production quality services with semantically described rdf , enabling real biomedical use cases to be addressed .
5. data access and security besides improving the search capabilities via ontologies , metadata , and linked data for accessing data efficiently , the usability aspect is also a fundamental topic for big data .
scientists want to focus on their specific research while creating and analysing data without the need to know all the low - level burdens related to the underlying data management infrastructures .
this demand can be addressed by science gateways that are single points of entry to applications and data across organizational boundaries .
data security is another aspect that must be addressed when providing access to big data , in particular while working in the healthcare sector .
5.1 .
science gateways the overall goal of science gateways is to hide the complex underlying infrastructure and to offer intuitive user interfaces for job , data , and workflow management .
in the last decade diverse mature frameworks , libraries , and apis have been evolved , which allow the enhanced development of science gateways .
while distributed job and workflow management are widely supported in frameworks like enginframe [ 96 ] , implemented on top of the standards - based portal framework liferay [ 97 ] or the proprietary workflow - enabled science gateway galaxy [ 98 - 100 ] , the data management is often not completely satisfactory .
also content management systems such as drupal [ 101 ] and joomla [ 102 ] or the high - level framework django [ 103 ] still lack the support of sophisticated data management features for data on a large scale .
vectorbase [ 104 ] , for example , is a mature science gateway for invertebrate vectors of human pathogens developed in drupal offering large sets of data and tools for further analysis .
while this science gateway is widely used with a user community of over 100,000 users , the data management is directly dependent on the underlying file system , without additional data management features tailored to big data .
the sophisticated metadata management in vectorbase has been developed by the vectorbase team from scratch since drupal lacks metadata management capabilities .
the requirement for processing big data in a science gateway is reflected in a few current developments in the context of standardized apis and frameworks .
agave [ 105 ] provides powerful api for developing intuitive user interfaces for distributed job management , which has been extended in a first prototype with metadata management capabilities , and allows the integration with distributed file systems .
globus transfer [ 106 ] forms a data bridge , which supports the storage protocol gridftp [ 107 ] .
it offers a web - based user interface and community features similar to dropbox [ 108 ] .
the users are enabled to easily share data and manage it in distributed environments .
the biomedical research informatics network ( birn ) [ 109 ] , for example , is a national initiative to advance biomedical research via data sharing and collaborations and the corresponding infrastructure applies globus transfer for moving large datasets .
data avenue [ 110 ] follows an analogous concept as globus transfer and provides a web - based user interface as well .
additionally , it will be integrated in the workflow - enabled science gateway ws - pgrade [ 111 ] , which is the flexible user interface of guse .
the extension by data avenue enhances the data management in the science gateway framework significantly so that not only jobs and workflows can be distributed to diverse grid , cloud , cluster , and collaborative infrastructures , but also distributed data can be efficiently managed via storage protocols like http ( s ) , secure ftp ( sftp ) , gridftp , storage resource management ( srm ) [ 112 ] , amazon simple storage service ( s3 ) [ 113 ] , and integrated rule - oriented data system ( irods ) [ 114 ] .
an example of a science gateway in the bioinformatics filed is mosgrid ( molecular simulation grid ) [ 115 ] that supports the molecular simulation community with an intuitive user interface in the areas of quantum chemistry , molecular dynamics , and docking screening .
it has been developed on the top of ws - pgrade / guse and features metadata management with search capabilities via lucene [ 116 ] and distributed data management via the object - based distributed file system xtreemfs [ 117 ] .
while these capabilities have been developed before data avenue was available in ws - pgrade , the layered architecture of the mosgrid science gateways has been designed to allow the integration of further data management systems and thus can be extended for data avenue .
5.2 .
security whatever technology is used , the distributed data management for biomedical applications with community options for sharing data requires especially secure authentication and secure measures to assure strict access policies and the integrity of data [ 118 ] .
medical bioinformatics , in fact , is often concerned with sensitive and expensive data such as projects contributing to computer - aided drug design or in environments like hospitals .
the distribution of data increases the complexity and involves data transfer through many network devices .
thus , data loss or corruption can occur .
gsi ( grid security infrastructure ) [ 119 ] has been designed for the authentication via x.509 certificates and assures the secure access to data .
irods and xtreemfs , for example , support gsi for authentication .
both use enhanced replication mechanisms to warrant the integrity of data including the prevention of loss .
amazon s3 follows a username and password approach while owner of an instance can grant access to the data via acls ( access control lists ) .
the corresponding amazon web services also replicate the data over diverse instances and examine md5 checksums to check whether the data transfer was fully successful and the transferred files unchanged .
the security mechanisms in data avenue as well as in globus transfer are based on gsi .
globus transfer applies globus nexus [ 105 ] as security platform , which is capable of creating a full identity management with authentication and group mechanisms .
globus nexus can serve as security service layer for distributed data management systems and can connect to diverse storages like amazon s3 .
6. perspectives and open problems data is considered the fourth paradigm in science [ 120 ] , besides experimental , theoretical , and computational sciences .
this is becoming particularly true in computational biology , where , for example , the approach " sequence first , think later " is rapidly overcoming the hypothesis - driven approach .
in this context , big data integration is really critical for bioinformatics that is " the glue that holds biomedical research together. " there are many open issues for big data management and analysis , in particular in the computational biology and healthcare fields .
some characteristics and open issues of these challenges have been discussed in this paper , such as architectural aspects and the capability of being flexible enough to collect and analyse different kind of information .
it is critical to face the variety of the information that should be managed by such infrastructures , which should be organized in scheme - less contexts , combining both relaxed consistency and a huge capacity to digest data .
therefore , a critical point is that relational databases are not suitable for big data problems .
they lack horizontal scalability , need hard consistency , and become very complex when there is the need to represent structured relationships .
nonrelational databases ( nosql ) are the interesting alternative to data storage because they combine the scalability and flexibility. from the computational point of view , the novel idea is that jobs are directly responsible of managing input data , through suitable procedures for partitioning , organizing , and merging intermediate results .
novel algorithm will contain large parts of not functional code , but essential for exploiting housekeeping tasks .
due to the practical impossibility of moving all the data across geographical dispersed sites , there is the need of computational infrastructure able to combine large storage facilities and hpc .
virtualization can be the key in this challenge , since it can be exploited to achieve storage facilities able to leverage in - memory key / value databases to accelerate data - intensive tasks .
the most important initiatives for the usage of big data techniques in medical bioinformatics are related to scientific research efforts , as described in the paper .
nevertheless , some commercial initiatives are available to cope with the huge quantity of data produced nowadays in the field of molecular biology exploiting high - throughput omics technologies for real - life problems .
these solutions are designed to support researchers working in computational biology mainly using cloud infrastructures .
examples are era7 bioinformatics [ 121 ] , dnanexus [ 122 ] , seven bridge genomics [ 123 ] , eaglegenomics [ 124 ] , and maverixbio [ 125 ] .
noteworthy , also large providers of molecular biology instrumentations , such as illumina [ 126 ] , and huge service providers , such as bgi [ 127 ] , have cloud - based services to support their customers .
hospitals are also considering to hiring big data solutions in order to provide " support services for researchers who need assistance with managing and analyzing large medical data sets " [ 128 ] .
in particular , mckinsey & company stated already in april 2013 that big data will be able to revolutionize pharmaceutical research and development within clinical environments , by targeting the diverse user roles physicians , consumers , insurers , and regulators [ 129 ] .
in 2014 they predicted that big data could lead to a reduction of research and development costs for the pharmaceutical industry by approximately 35 % ( about $ 40 billion ) [ 130 ] .
drugmakers , healthcare providers , and health analysis companies are collaborating on this topic ; for example , drugmaker glaxosmithkline plc and the health analysis company sas institute inc .
work on a private cloud for pharmaceutical industry sharing securely anonymized data [ 130 ] .
open data especially can be exploited by patient communities such as patientslikeme.com [ 131 ] containing invaluable information for further data mining .
it is therefore clear that in the big data field there is much to do in terms of making these technologies efficient and easy - to - use , especially considering that even small and medium - size laboratories are going to use them in a close future .
acknowledgments the work has been supported by the italian ministry of education and research through the flagship ( pb05 ) " interomics ," " hirma " ( rbap11ys7 k ) projects , and the european " mimomics " ( 305280 ) project .
conflict of interests the authors declare that there is no conflict of interests regarding the publication of this paper .
1 genovese y prentice s pattern - based strategy : getting value from big data gartner special report 2011 g00214032 g00214032 2 the big data research and development initiative , http :// www.whitehouse.gov / sites / default / files / microsites / ostp / big _ data _ press _ release _ final _ 2.pdf 3 durbin rm abecasis gr altshuler rm a map of human genome variation from population - scale sequencing nature 2010 467 1061 1073 20981092 4 raney bj cline ms rosenbloom kr encode whole - genome data in the ucsc genome browser ( 2011 update ) nucleic acids research 2011 39 1 d871 d875 2 - s2.0 - 78651338017 21037257 5 international human genome sequencing consortium initial sequencing and analysis of the human genome nature 2001 409 860 921 11237011 6 the genome 10k projecthttps :// genome10k.soe.ucsc.edu / 7 the 100,000 genomes projecthttp :// www.genomicsengland.co.uk / 8 hayden ec the $ 1,000 genome nature 2014 507 7492 294 295 24646979 9 sboner a mu xj greenbaum d auerbach rk gerstein mb the real cost of sequencing : higher than you think !
genome biology 2011 12 8 , article 125 2 - s2.0 - 80052028523 10 pinheiro e weber w - d barroso la failure trends in a large disk drive population proceedings of the 5th usenix conference on file and storage technologies ( fast ' 07 ) 2007 17 29 11 ibm , big data and analytics , tools and technologies for architects and developers , http :// www.ibm.com / developerworks / library / bd - archpatterns1 / index.html ? ca = drs 12 oracle and big data , http :// www.oracle.com / us / technologies / big - data / index.html 13 microsoft , reimagining your business with big data and analytics , http :// www.microsoft.com / enterprise / it - trends / big - data / default.aspx ? search = true # fbid = sptczlfs2h _ 14 torres j big data challenges in bioinformatics 2014 http :// www.jorditorres.org / wp - content / uploads / 2014 / 02 / xerradabib.pdf 15 the geant pan - european research and education network http :// www.geant.net / pages / default.aspx 16 the internet2 networkhttp :// www.internet2.edu / research - solutions / case - studies / xsedenet - advanced - network - advancing - science / 17 renee boucher ferguson big data : service from the cloud http :// sloanreview.mit.edu / article / big - data - service - from - the - cloud / 18 thanh td mohan s choi e sangbum k kim p a taxonomy and survey on distributed file systems 1 proceedings of the 4th international conference on networked computing and advanced information management ( ncm ' 08 ) september 2008gyeongju , republic of korea 144 149 2 - s2.0 - 57849118842 19 the ibm general parallel file system , http :// www - 03.ibm.com / software / products / en / software 20 the opensfs and lustre community portalhttp :// lustre.opensfs.org / 21 the top500 list 2014 , http :// www.top500.org / 22 ibm elastic storagehttp :// www - 03.ibm.com / systems / platformcomputing / products / gpfs / 23 the apache hadoop project , 2014 , http :// hadoop.apache.org / 24 ghemawat s gobioff h leung s the google file system proceedings of the 19th acm symposium on operating systems principles ( sosp ' 03 ) october 2003 29 43 2 - s2.0 - 21644437974 25 the message passing interface2014 , http :// www.mpi - forum.org / 26 plimpton sj devine kd mapreduce in mpi for large - scale graph algorithms parallel computing 2011 37 9 610 632 2 - s2.0 - 80052031568 27 lu x liang f wang b zha l xu z datampi : extending mpi to hadoop - like big data computing proceedings of the 28th ieee international parallel and distributed processing symposium ( ipdps ' 14 ) 2014 28 ostrouchov g schmidt d chen w - c patel p combining r with scalable libraries to get the best of both for big data international association for statistical computing satellite conference for the 59th isi world statistics congress2013 85 90 29 the hadoop ecosystem table , 2014 , http :// hadoopecosystemtable.github.io / 30 list of institutions that are using hadoop for educational or production uses , 2014 , http :// wiki.apache.org / hadoop / poweredby 31 the genome analysis toolkit2014 , http :// www.broadinstitute.org / gatk / 32 mckenna a hanna m banks e the genome analysis toolkit : a mapreduce framework for analyzing next - generation dna sequencing data genome research 2010 20 9 1297 1303 2 - s2.0 - 77956295988 20644199 33 nordberg h bhatia k wang k wang z biopig : a hadoop - based analytic toolkit for large - scale sequence data bioinformatics 2013 29 23 3014 3019 24021384 34 zou q li xb jiang wr lin zy li gl chen k survey of mapreduce frame operation in bioinformatics briefings in bioinformatics 2014 15 4 637 647 23396756 35 taylor rc an overview of the hadoop / mapreduce / hbase framework and its current applications in bioinformatics bmc bioinformatics 2010 11 12 , article s1 2 - s2.0 - 78650811522 36 hadoop bioinformatics applications on quora http :// hadoopbioinfoapps.quora.com / 37 niemenmaa m kallio a schumacher a klemela p korpelainen e heljanko k hadoop - bam : directly manipulating next generation sequencing data in the cloud bioinformatics 2012 28 6 876 877 2 - s2.0 - 84859075799 22302568 38 pireddu l leo s zanetti g seal : a distributed short read mapping and duplicate removal tool bioinformatics 2011 27 15 2159 2160 2 - s2.0 - 79960410567 21697132 39 the apache hive data warehouse software2014 , http :// hive.apache.org / 40 krzywinski m birol i jones sj marra ma hive plots - rational approach to visualizing networks briefings in bioinformatics 2012 13 5 627 644 2 - s2.0 - 84860916660 22155641 41 the apache pig platform http :// pig.apache.org / 42 the disco project , 2014 , http :// discoproject.org / 43 cock pja antao t chang jt biopython : freely available python tools for computational molecular biology and bioinformatics bioinformatics 2009 25 11 1422 1423 2 - s2.0 - 65649092976 19304878 44 the apache storm system http :// storm.incubator.apache.org / 45 merelli i perez - sanchez h gesing s d'agostino d latest advances in distributed , parallel , and graphic processing unit accelerated approaches to computational biology concurrency and computation : practice and experience 2014 26 10 1699 1704 46 d'agostino d clematis a quarati a cloud infrastructures for in silico drug discovery : economic and practical aspects biomed research international 2013 2013 19 pages 138012 47 rencuzogullari u dwarkadas s dynamic adaptation to available resources for parallel computing in an autonomous network of workstations proceedings of the 8th acm sigplan symposium on principles and practice of parallel programmingjune 2001 72 81 2 - s2.0 - 0034819363 48 compeau pec pevzner pa tesler g how to apply de bruijn graphs to genome assembly nature biotechnology 2011 29 11 987 991 2 - s2.0 - 80755159050 49 simpson jt wong k jackman sd schein je jones sjm birol i abyss : a parallel assembler for short read sequence data genome research 2009 19 6 1117 1123 2 - s2.0 - 66449136667 19251739 50 garland m le grand s nickolls j parallel computing experiences with cuda ieee micro 2008 28 4 13 27 2 - s2.0 - 53749092570 51 the open computing language standard , 2014 , https :// www.khronos.org / opencl / 52 garland m kirk db understanding throughput - oriented architectures communications of the acm 2010 53 11 58 66 2 - s2.0 - 78149258346 53 gpu applications for bioinformatics and life sciences , http :// www.nvidia.com / object / bio _ info _ life _ sciences.html 54 the intel xeon phi coprocessor performance http :// www.intel.com / content / www / us / en / processors / xeon / xeon - phi - detail.html 55 li h durbin r fast and accurate short read alignment with burrows - wheeler transform bioinformatics 2009 25 14 1754 1760 2 - s2.0 - 67649884743 19451168 56 finn rd clements j eddy sr hmmer web server : interactive sequence similarity searching nucleic acids research 2011 39 2 w29 w37 2 - s2.0 - 79959931985 21593126 57 altschul sf gish w miller w myers ew lipman dj basic local alignment search tool journal of molecular biology 1990 215 3 403 410 2 - s2.0 - 0025183708 2231712 58 fang j sips h zhang l xu c varbanescu al test - driving intel xeon phi proceedings of the 5th acm / spec international conference on performance engineering ( icpe'14 ) 2014 137 148 59 afgan e baker d coraor n chapman b nekrutenko a taylor j galaxy cloudman : delivering cloud compute clusters bmc bioinformatics 2010 11 supplement 12 , article s4 2 - s2.0 - 78650841579 60 dai l gao x guo y xiao j zhang z bioinformatics clouds for big data manipulation biology direct 2012 7 , article 43 2 - s2.0 - 84870057823 61 cava c gallivanone f salvatore c della rosa p castiglioni i bioinformatics clouds for high - throughput technologies handbook of research on cloud infrastructures for big data analytics 2014 chapter 20 igi global 62 cannata n schroder m marangoni r romano p a semantic web for bioinformatics : goals , tools , systems , applications bmc bioinformatics 2008 9 supplement 4 , article s1 2 - s2.0 - 44649148469 63 wache h vogele t visser u ontology - based integration of information a survey of existing approaches proceedings of the workshop on ontologies and information sharing ( ijcai ' 01 ) 2001 108 117 64 owl web ontology language overview w3c recommendation , 2004 , http :// www.w3.org / tr / owl - features / 65 antezana e blonde w egana m biogateway : a semantic systems biology tool for the life sciences bmc bioinformatics 2009 10 10 , article s11 2 - s2.0 - 70449404671 66 the go file format guide , http :// geneontology.org / book / documentation / file - format - guide 67 west a ml , rdf , owl and lsid : ontology integration within evolving " omic " standards proceedings of the invited talk international oracle life sciences and healthcare user group meeting2006boston , mass , usa 68 wang x gorlitsky r almeida js from xml to rdf : how semantic web technologies will change the design of " omic " standards nature biotechnology 2005 23 9 1099 1103 2 - s2.0 - 27144497798 69 stephens s lavigna d dilascio m luciano j aggregation of bioinformatics data using semantic web technology journal of web semantics 2006 4 3 216 221 2 - s2.0 - 33748436824 70 sioutos n coronado sd haber mw hartel fw shaiu w wright lw nci thesaurus : a semantic model integrating cancer - related clinical and molecular information journal of biomedical informatics 2007 40 1 30 43 2 - s2.0 - 33751431796 16697710 71 golbeck j fragoso g hartel f hendler j oberthaler j parsia b the national cancer institute 's thesaurus and ontology web semantics 2003 1 1 75 80 2 - s2.0 - 33845512446 72 kozlenkov a schroeder m prova : rule - based java - scripting for a bioinformatics semantic web data integration in the life sciences 2004 2994 berlin , germany springer 17 30 lecture notes in computer science 73 stevens rd tipney hj wroe cj exploring williams - beuren syndrome using mygrid bioinformatics 2004 20 supplement 1 i303 i310 2 - s2.0 - 18244376653 15262813 74 blake ja bult cj beyond the data deluge : data integration and bio - ontologies journal of biomedical informatics 2006 39 3 314 320 2 - s2.0 - 33646256547 16564748 75 viti f merelli i calabria a ontology - based resources for bioinformatics analysis international journal of metadata , semantics and ontologies 2011 6 1 35 45 2 - s2.0 - 80052890769 76 osborne jd flatow j holko m annotating the human genome with disease ontology bmc genomics 2009 10 supplement 1 , article s6 2 - s2.0 - 66349110163 77 merelli i calabria a cozzi p viti f mosca e milanesi l snpranker 2.0 : a gene - centric data mining tool for diseases associated snp prioritization in gwas bmc bioinformatics 2013 14 supplement 1 , article s9 2 - s2.0 - 84872419930 78 mosca e alfieri r merelli i viti f calabria a milanesi l a multilevel data integration resource for breast cancer study bmc systems biology 2010 4 , article 76 2 - s2.0 - 77953015096 79 mosca e alfieri r viti f merelli i milanesi l nervous system database ( nsd ) : data integration spanning molecular and system levels proceedings of the frontiers in neuroinformatics conference abstract : neuroinformatics2009 80 viti f merelli i caprera a lazzari b stella a milanesi l ontology - based , tissue microarray oriented , image centered tissue bank bmc bioinformatics 2008 9 , supplement 4 , article s4 2 - s2.0 - 44649133998 81 merelli i cozzi p d'agostino d clematis a milanesi l image - based surface matching algorithm oriented to structural biology ieee / acm transactions on computational biology and bioinformatics 2011 8 4 1004 1016 2 - s2.0 - 79957581569 21566253 82 alfieri r merelli i mosca e milanesi l the cell cycle db : a systems biology approach to cell cycle analysis nucleic acids research 2008 36 1 d641 d645 2 - s2.0 - 38549092068 18160409 83 cote rg jones p apweiler r hermjakob h the ontology lookup service , a lightweight crossplatform tool for controlled vocabulary queries bmc bioinformatics 2006 7 , article 97 2 - s2.0 - 33645348241 84 the gene ontology consortium the gene ontology 's reference genome project : a unified framework for functional annotation across species plos computational biology 2009 5 7 e1000431 85 the kegg orthology database , http :// www.genome.jp / kegg / ko.html 86 chang a scheer m grote a schomburg i schomburg d brenda , amenda and frenda the enzyme information system : new content and tools in 2009 nucleic acids research 2009 37 1 d588 d592 2 - s2.0 - 58149185126 18984617 87 bard j rhee sy ashburner m an ontology for cell types .
genome biology 2005 6 2 p .
r21 2 - s2.0 - 21944445759 15693950 88 natale da arighi cn barker wc framework for a protein ontology bmc bioinformatics 2007 8 9 supplement 9 p .
s1 2 - s2.0 - 38449100150 18047708 89 nelson sj schopen m savage ag schulman j arluk n the mesh translation maintenance system : structure , interface design , and implementation studies in health technology and informatics 2004 107 , part 1 67 69 2 - s2.0 - 21644485383 15360776 90 orengo ca michie ad jones s jones dt swindells mb thornton jm cath - - a hierarchic classification of protein domain structures structure 1997 5 8 1093 1108 2 - s2.0 - 0030777303 9309224 91 the bio2rdf project , 2014 , http :// bio2rdf.org / 92 belleau f nolin m tourigny n rigault p morissette j bio2rdf : towards a mashup to build bioinformatics knowledge systems journal of biomedical informatics 2008 41 5 706 716 2 - s2.0 - 48249158190 18472304 93 jupp s malone j bolleman j the ebi rdf platform : linked open data for the life sciences bioinformatics 2014 30 9 1338 1339 24413672 94 the open phacts platform , http :// www.openphacts.org 95 the atlasrdf - r package , 2014 , https :// github.com / jamesmalone / atlasrdf - r 96 torterolo l porro i fato m melato m calanducci a barbera r building science gateways with enginframe : a life science example proceedings of the international workshop on portals for life sciences ( iwpls ' 09 ) 2009 97 liferay2014 , https :// http :// www.liferay.com / 98 goecks j nekrutenko a taylor j galaxy : a comprehensive approach for supporting accessible , reproducible , and transparent computational research in the life sciences genome biology 2010 11 8 article r86 2 - s2.0 - 77955801615 99 blankenberg d kuster gv coraor n galaxy : a web - based genome analysis tool for experimentalists current protocols in molecular biology 2010 2 - s2.0 - 75949108066 100 giardine b riemer c hardison rc galaxy : a platform for interactive large - scale genome analysis genome research 2005 15 10 1451 1455 2 - s2.0 - 25844449770 16169926 101 drupalhttps :// www.drupal.org / 102 joomla2014 , http :// www.joomla.org / 103 django , 2014 , https :// http :// www.djangoproject.com / 104 megy k emrich sj lawson d vectorbase : improvements to a bioinformatics resource for invertebrate vector genomics nucleic acids research 2012 40 1 d729 d734 2 - s2.0 - 84858286227 22135296 105 dooley r vaughn m stanzione d terry s skidmore e software - as - a - service : the iplant foundation api proceedings of the 5th ieee workshop on many - task computing on grids and supercomputers ( mtags ' 12 ) 2012 106 ananthakrishnan r chard k foster i tuecke s globus platform - as - a - service for collaborative science applications concurrency and computation : practice and experience 2014 107 allcock w gridftp : protocol extensions to ftp for the grid global grid forum gfd - r - p.020 proposed recommendation , 2003 108 dropbox , 2014 , https :// http :// www.dropbox.com 109 helmer kg ambite jl ames j enabling collaborative research using the biomedical informatics research network ( birn ) journal of the american medical informatics association 2011 18 416 422 21515543 110 hajnal a farkas z kacsuk p data avenue : remote storage resource management in ws - pgrade / guse proceedings of the 6th international workshop on science gateways ( iwsg 2014 ) june 2014 111 kacsuk p farkas z kozlovszky m ws - pgrade / guse generic dci gateway framework for a large variety of user communities journal of grid computing 2012 10 4 601 630 2 - s2.0 - 84871645601 112 shoshani a sim a gu j storage resource managers : essential components for the grid grid resource management 2003 kluwer academic publishers 113 amazon simple storage servicehttp :// aws.amazon.com / s3 114 integrated rule - oriented data system 2014 , https :// www.irods.org 115 kruger j grunzke r gesing s the mosgrid science gateway - - a complete solution for molecular simulations journal of chemical theory and computation 2014 10 6 2232 2245 26580747 116 apache lucene project , 2014 , http :// lucene.apache.org / 117 hupfeld f cortes t kolbeck b the xtreemfs architecture : a case for object - based file systems in grids concurrency computation practice and experience 2008 20 17 2049 2060 2 - s2.0 - 57349095550 118 razick s mocnik r thomas lf ryeng e drablos f saetrom p the egenvar data management system - cataloguing and sharing sensitive data and metadata for the life sciences database 2014 119 foster i kesselman c tsudik g tuecke s a security infrastructure for computational grids proceedings of the 5th acm conference on computer and communications security1998 83 92 120 hey t tansley s tolle k the fourth paradigm : data - intensive scientific discovery 2009 redmond , va , usa microsoft research 121 era7 bioinformatics , 2014 , http :// era7bioinformatics.com 122 dnanexus2014 , https :// dnanexus.com / 123 seven bridge genomicshttps :// http :// www.sbgenomics.com 124 eaglegenomics , 2014 , http :// www.eaglegenomics.com 125 maverixbio2014 , http :// www.maverixbio.com 126 illumina2014 , http :// www.illumina.com 127 bgi , http :// www.genomics.cn / en / index 128 downs eh uf hires bioinformatics expert 2014 , https :// m.ufhealth.org / news / 2014 / uf - hires - bioinformatics - expert 129 mckinsey & company how big data can revolutionize pharmaceutical r & d http :// www.mckinsey.com / insights / health _ systems _ and _ services / how _ big _ data _ can _ revolutionize _ pharmaceutical _ r _ and _ d 130 medill reports , 2014 , http :// news.medill.northwestern.edu / chicago / news.aspx ? id = 228875 131 patientslikeme , http :// www.patientslikeme.com / figure 1 tag crowd of the concepts presented in the paper .