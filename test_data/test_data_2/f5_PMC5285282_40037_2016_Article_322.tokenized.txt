perspect med educ perspect med educ perspectives on medical education 2212 - 2761 2212 - 277x bohn stafleu van loghum houten 28050882 5285282 322 10.1007 / s40037 - 016 - 0322 - 0 original article ensuring the quality of multiple - choice exams administered to small cohorts : a cautionary tale young meredith meredith.young @ mcgill.ca 12 meredith young phd , is an assistant professor in the department of medicine and a research scientist in the centre for medical education at mcgill university .
cummings beth - ann 12 beth - ann cummings mdcm , msc ( hpe ) , frcpc is an associate professor in the department of medicine , a core member of the centre for medical education , and the associate dean , undergraduate medical education at mcgill university .
st - onge christina 34 christina st - onge phd is an associate professor in the department of medicine and holds the paul grand'maison - societe des medecins de l'universite de sherbrooke research chair in medical education .
1 0000 0004 1936 8649grid.14709.3bdepartment of medicine , mcgill university , montreal , quebec canada 2 0000 0004 1936 8649grid.14709.3bcentre for medical education , mcgill university , montreal , quebec canada 3 0000 0000 9064 6198grid.86715.3ddepartment of medicine , universite de sherbrooke , sherbrooke , quebec canada 4 0000 0000 9064 6198grid.86715.3dcentre de pedagogie des sciences de la sante , universite de sherbrooke , sherbrooke , quebec canada 3 1 2017 3 1 2017 2 2017 6 1 21 28 ( c ) the author ( s ) 2016 open access this article is distributed under the terms of the creative commons attribution 4.0 international license ( http :// creativecommons.org / licenses / by / 4.0 /) , which permits unrestricted use , distribution , and reproduction in any medium , provided you give appropriate credit to the original author ( s ) and the source , provide a link to the creative commons license , and indicate if changes were made .
introduction multiple - choice questions ( mcqs ) are a cornerstone of assessment in medical education .
monitoring item properties ( difficulty and discrimination ) are important means of investigating examination quality .
however , most item property guidelines were developed for use on large cohorts of examinees ; little empirical work has investigated the suitability of applying guidelines to item difficulty and discrimination coefficients estimated for small cohorts , such as those in medical education .
we investigated the extent to which item properties vary across multiple clerkship cohorts to better understand the appropriateness of using such guidelines with small cohorts .
methods exam results for 32 items from an mcq exam were used .
item discrimination and difficulty coefficients were calculated for 22 cohorts ( n = 10 - 15 students ) .
discrimination coefficients were categorized according to ebel and frisbie ( 1991 ) .
difficulty coefficients were categorized according to three guidelines by laveault and gregoire ( 2014 ) .
descriptive analyses examined variance in item properties across cohorts .
results a large amount of variance in item properties was found across cohorts .
discrimination coefficients for items varied greatly across cohorts , with 29 / 32 ( 91 %) of items occurring in both ebel and frisbie 's ' poor ' and ' excellent ' categories and 19 / 32 ( 59 %) of items occurring in all five categories .
for item difficulty coefficients , the application of different guidelines resulted in large variations in examination length ( number of items removed ranged from 0 to 22 ) .
discussion while the psychometric properties of items can provide information on item and exam quality , they vary greatly in small cohorts .
the application of guidelines with small exam cohorts should be approached with caution .
keywords item properties assessment multiple - choice examination http :// dx.doi.org / 10.13039 / 501100000155social sciences and humanities research council of canada435 - 2014 - 2159issue - copyright - statement ( c ) the author ( s ) 2017 what this paper adds item property estimates are often used to judge the quality of multiple - choice questions .
large amounts of variance are observed in estimates of difficulty and discrimination for items on multiple choice examinations used in small cohorts .
large amounts of variance in item property estimates suggest that conclusions based on these estimates should be drawn with caution and that decisions regarding item quality should be informed by many factors , including item property estimates , content representativeness , purpose of assessment , length of assessment , in consideration of cohort size to ensure decisions affecting final scores are done in the most defensible way possible .
introduction assessment serves several key functions in medical education : it is a gatekeeper , a feedback mechanism , a means to support learning [ 1 - 4 ] , and a stepping stone in assuring competent practice ( e. g. , [ 5 - 7 ]) .
while programmes of assessment often include multiple assessment methods [ 8 , 9 ] , one of the most commonly used methods is the written exam based on multiple choice questions ( mcqs ) .
mcqs are known for their objectivity , ease of scoring , and wide sampling of broad content areas [ 10 , 11 ] .
mcqs are a pervasive item format in medical education , often appearing in national - level examinations ( e. g. united states medical licensing exam ( usmle ) and medical council of canada qualifying exam ( mccqe )) .
while ubiquitous , multiple - choice based examinations require careful monitoring to ensure not only continued item and examination quality , but also a credible final score on which to base education judgments - including decisions regarding gate - keeping and remediation .
one means of monitoring is to rely on item statistics or item properties , such as difficulty and discrimination coefficients .
these item properties can be derived after exam administration , and thus are available to help administrators judge the quality of individual items and make decisions regarding the composition of the final examination score [ 12 ] .
pragmatically , if an item 's properties do not meet predetermined standards , the item may be excluded from the final score to derive a more appropriate final score , and then re - evaluated for later use ( either maintained or removed from an ' item bank ') .
while alternate means of assessing item quality are available , one of the more commonly applied approaches in health professions education is that of classical test theory ( ctt ) .
within this framework , difficulty and discrimination coefficients are amongst the most commonly used metrics that examination administrators rely on to assist in quality assurance decisions .
standards for judging the appropriateness or quality of an mcq can be found in the educational psychology literature [ 13 - 17 ] and have been adopted within health professions education to assist quality decisions .
however , most item analysis guidelines were developed from , and intended for use in , examinations administered to large cohorts ( such as national - level examinations ) where item properties such as item difficulty and discrimination coefficients are taken to be stable ( i. e. they do not vary between cohorts due to large examination cohort sizes ) [ 12 , 18 - 21 ] .
several guidelines exist for interpreting item difficulty and discrimination estimates [ 13 - 17 ] ; however the cohort size needed to most appropriately apply item analysis guidelines varies from 30 to 200 examines [ 12 , 18 - 21 ] with a general rule of thumb to have 5 to 10 times as many examinees as items [ 21 ] .
our collective experience suggests that item guidelines are frequently used in medical education cohorts that are smaller , particularly at the clerkship level .
course - level leaders rarely have the opportunity to pursue formalized training in measurement , but are often responsible for overseeing the quality of the examinations used within their educational portfolios .
in order to facilitate these roles , course leaders are often encouraged to attend local or national courses where psychometrics and approaches to examination quality are taught in brief sessions ( e. g. , [ 22 ]) , and where they may be provided with easy - to - digest articles and guidelines that cite original measurement works ( e. g. , [ 23 ]) .
the educational opportunities intended for course leaders introduce concepts relating to the utility of item properties for quality monitoring but given the breadth of topics covered and short timelines , there is rarely an opportunity to discuss the stability or instability of item properties across different cohort sizes - discussions often present in other educational and measurement fields [ 12 , 19 , 20 , 24 ] .
further , while universities often provide reports including information on item properties ( for cohorts of all sizes ) , they are rarely accompanied by information on how to interpret these properties or how best to apply quality monitoring guidelines to the data presented .
the application of item analysis guidelines assumes that item properties are relatively stable , meaning that the estimates of difficulty and discrimination would change little between exam administrations .
for example , according to this assumption a ' difficult ' item remains difficult across different groups of similarly skilled examinees , which resonates well with our intuition and our intent when creating questions [ 12 , 21 ] .
however , if large amounts of item property variance are present in small cohorts , this would undermine the decisions reached while relying on these guidelines , such as choosing to retain an item or remove it from a total exam score or an item bank .
further , if item property of variance is observed , and this variance leads to different items being removed from exams , this may result in changes to the content and composition of an exam across cohorts of students .
large amounts of variance in item properties across cohorts of examinees would , therefore , challenge the appropriateness of applying these guidelines , calling into question the practice of applying item guidelines to small cohorts within a medical education context .
as a first step in understanding whether we can use item analysis guidelines to inform our decision processes in small assessment cohorts , this study investigated the amount of variance observed in item properties when an mcq examination was administered to several sequential clerkship cohorts .
method a descriptive exploratory study was conducted .
this study was approved by the mcgill university research ethics board ( a10 - e82 - 13a ) .
data collection exam results for a locally developed knowledge - based mcq exam at the fourth year clerkship level were used .
the exam was administered within the assessment programme of a senior clerkship rotation .
this topic - based knowledge exam represented 40 % of the final score for that clerkship rotation ( the other 60 % of total score derived from clinical assessment ) .
the purpose of the exam was to assess students' application of knowledge within the focal clerkship .
the exam pass score was set at 60 % , as is the institutional standard for the site of this study .
all students participated in the same academic half - day teaching content , and all had the same set of recommended readings .
the questions within the exam were created , vetted , and adapted by the undergraduate education committee for the relevant clerkship , and were mapped to overall curricular and specific course objectives .
members of the undergraduate education committee for this clerkship were aware of item - writing guidelines ; however , no formal quality assessment process for items ( beyond peer - review , editing , and vetting ) was done. and all represented single best answer questions ( 5 response options ) .
the exam items were chosen from the item bank by the course director to represent a range of topic areas .
item statistics ( i. e. difficulty and discrimination ) were not deliberately used in the selection of examination items .
the exam , comprising 50 items from a pool of 70 items , was administered to 22 cohorts of clerkship students ( n = 10 - 15 students ) between 2010 and 2013 .
two cohorts between 2010 and 2013 were not included in the analysis as they contained only one individual , so calculation of discrimination coefficients was not possible .
dataset of the 70 banked mcq items , 32 items occurred on all examinations .
for each cohort of examinees , the difficulty coefficients ( i. e. the proportion of candidates who answered the item correctly ) and discrimination coefficients ( the ' corrected ' point - biserial correlation , i. e. , the correlation between the item score and overall score ( minus the item score )) were calculated for the 32 repeated examination items .
therefore , each of the 32 mcq items included in this analysis had a total of 22 difficulty coefficients and 22 discrimination coefficients calculated from cohorts ranging in size from 10 - 15 students .
procedure classification of discrimination coefficients each discrimination coefficient ( 32 mcq items with 22 coefficients each ) was classified according to ebel and frisbie 's [ 13 ] guidelines for item quality , that is , coefficients < 0.10 = poor discrimination ; 0.10 - 0.19 = low discrimination ; 0.20 - 0.29 = acceptable discrimination ; 0.30 - 0.39 = good discrimination ; and > 0.40 = excellent discrimination .
classification of difficulty coefficients difficulty coefficients were examined according to three guidelines proposed by laveault and gregoire [ 15 ] for item quality .
for each item , the outcome of categorization was whether or not it should be excluded from the final examination score .
according to guideline 1 , an item should be excluded from the examination if the item difficulty was +/ - two standard deviations from the average difficulty of the examination .
according to guideline 2 , an item should be excluded if it fell +/ - two standard deviations from the passing score ( here , set at 60 %) .
according to guideline 3 , an item should be excluded if difficulty was less than 0.2 or more than 0.8 ( a common ' rule of thumb ' for interpreting difficulty estimates ) .
for each examination cohort , the number of items recommended to be excluded from the final score according to each guideline was recorded .
analysis identification of variance in item characteristics the quantification of variance for difficulty and discrimination coefficients for the 32 mcq items used in 22 repeated small cohorts was primarily descriptive .
using graphical representation , the discrimination and difficulty coefficients were plotted using a box and whisker plot .
the discrimination and difficulty coefficients for a sample item were plotted in order to demonstrate the variance in item characteristics across repeated uses within a single item .
outcome variance of item characteristics on exam composition the exploration of the impact of variance in item properties on exam composition was primarily descriptive .
for item discrimination , frequency analyses were conducted to capture the variance in coefficients ( i. e. , the number of times an item was in each of ebel and frisbie 's five categories ) [ 13 ] .
for difficulty coefficients , descriptive frequency analyses were conducted to capture the impact of applying difficulty guidelines as interpreted from laveault and gregoire [ 25 ] on exam length ( number of items to be excluded from the total score ) .
results identification of variance in item characteristics the range of discrimination and difficulty coefficients for each item are illustrated in fig. 1 ( panel a and b respectively ) .
the majority of examination items display a large amount of variance in item property coefficients. fig. 1 variance in difficulty coefficients ( panel a ) and discrimination coefficients ( panel b ) across repeat use of 32 mcq items across 22 small student clerkship cohorts ( n = 10 - 15 students ) .
error bars represent range while fig. 1 shows the total amount of variance within and across items included in this study , it is difficult to visualize how this variation in item properties is reflected in individual examination cohorts .
in order to assist with visualizing the variance of difficulty and discrimination indices for one sample item across cohort ( time ) , a single mcq item was graphed across cohorts in fig. 2. fig. 2 variability in difficulty and discrimination indices for a single mcq item ( item 6 ) graphed across cohort outcome variance of item characteristics on exam composition item discrimination discrimination coefficients for each individual item varied greatly across cohorts .
discrimination for 29 of 32 exam items ( 91 % of mcq items ) was classified as both ' poor ' and ' excellent ' at least once .
nineteen of 32 ( 59 %) items had discrimination coefficients in all five of ebel and frisbie 's [ 13 ] categories of discrimination ( from poor to excellent ) .
details of the distribution of ebel and frisbie 's categorizations for each item can be found in table 1 .
table 1 frequency of items being categorized in each of ebel and frisbie 's [ 13 ] categories , displayed for each item discrimination categorization item poor low acceptable good excellent 1 12 0 2 2 6 2 19 1 0 0 2 3 20 1 0 1 0 4 12 4 3 2 1 5 14 0 0 5 3 6 10 1 4 2 5 7 11 2 1 2 6 8 14 2 2 2 2 9 12 1 3 3 3 10 16 3 1 1 1 11 14 1 3 1 3 12 7 1 4 2 8 13 5 4 2 5 6 14 12 2 2 1 5 15 8 4 2 1 7 16 19 0 1 0 2 17 18 1 1 2 0 18 15 0 2 0 5 19 13 3 0 0 6 20 11 3 1 1 6 21 19 0 0 1 2 22 5 1 3 3 10 23 16 0 1 2 3 24 19 0 1 1 1 25 20 0 1 1 0 26 9 2 0 5 6 27 12 1 1 1 7 28 8 3 5 2 4 29 9 6 2 2 3 30 9 3 1 1 8 31 9 1 2 2 8 32 13 2 2 1 4 item difficulty mean exam difficulty ( and associated standard deviation of item difficulty ) was calculated independently for each cohort in order to apply guidelines 1 and 2 .
mean difficulty ranged across cohorts from 0.80 to 0.89 , and standard deviations ranged from 0.15 to 0.21 .
decisions regarding whether to include or exclude an item from the final score varied widely by examination cohort and by guideline applied .
guideline 1 ( exclude item if +/ - 2 standard deviations from the average difficulty ) resulted in a range of 0 to 17 items removed across cohorts .
guideline 2 (+/ - 2 standard deviations from the passing score ) resulted in a range of 1 to 18 items removed across cohorts .
guideline 3 ( remove any item below 0.2 or over 0.8 ) resulted in a range of 1 to 22 items removed across cohorts .
table 2 shows the frequency in which each item would be recommended to be removed ( across cohorts ) under each guideline proposed by laveault and gregoire [ 25 ] .
table 2 frequency of items either removed or kept for the total exam score , presented by item , and for each quality monitoring guidelinea item number of times item is removed from the total score applying guideline 1 number of times item is removed from the total score applying guideline 2 number of times item is removed from the total score applying guideline 3 1 0 7 18 2 0 18 21 3 0 17 22 4 9 1 1 5 0 11 21 6 17 3 2 7 0 1 14 8 0 12 22 9 0 10 21 10 0 14 22 11 0 3 11 12 2 0 7 13 0 2 10 14 0 6 20 15 0 1 11 16 0 15 21 17 0 15 22 18 0 14 22 19 1 6 18 20 2 1 6 21 0 15 22 22 2 1 6 23 0 14 22 24 0 17 21 25 0 15 22 26 3 1 3 27 0 8 19 28 1 1 4 29 0 4 20 30 6 1 2 31 0 1 11 32 0 12 21 aguideline 1 : an item should be excluded from the examination if the item difficulty was +/ - two standard deviations from the average difficulty of the examination .
guideline 2 : an item should be excluded if it fell +/ - two standard deviations from the passing score .
guideline 3 : an item should be excluded if difficulty was less than 0.2 or more than 0.8 [ 15 ] .
discussion this study documented large amounts of variance in item difficulty and discrimination coefficients in multiple choice items repeatedly used in small cohorts of learners .
almost every item included in this study was categorized as having ' excellent ' and ' poor ' item discrimination , and over half of the items occurred in each of ebel and frisbie 's five categories of discrimination quality [ 13 ] .
large amounts of variance were also found for item difficulty , with substantial differences across cohorts in the likelihood of an item being recommended to be removed from or included in a final score .
for large cohorts of examination takers , item discrimination and item difficulty are assumed to be stable coefficients , properties that are inherent to the item [ 12 , 19 , 20 , 24 ] .
consequently , removing poor quality items is thought to implicitly improve the quality of the examination .
in the absence of evidence to the contrary , and with a paucity of reasonable alternatives to assure examination quality , our collective experience suggests that exam administrators are apt to use item analysis guidelines to inform decisions regarding whether or not to include an item in a total score , even for examinations administered to small cohorts of test - takers .
however , in this study , a large amount of variance was observed for difficulty and discrimination coefficients across cohorts .
while this study examined the instability of these estimates across multiple cohorts , administrators are often faced with the task of assuring examination quality with little or no historic data regarding item performance ; there is often a pragmatic need to decide whether or not an item should be removed from a total score despite having data from only one cohort , or one time - point .
this study demonstrates the instability of difficulty and discrimination coefficients , which may call into question the application of item analysis guidelines for assessment data generated by small cohorts .
this large amount of variance in item properties may not come as a surprise to those familiar with the development of item quality guidelines ; however , given the use of these guidelines with the small cohorts common in medical education , we believe that it is imperative to illustrate the variance in item properties across small cohorts within this context .
for individuals overseeing examinations administered to small cohorts , such as clerkship examinations , the application of item analysis guidelines in the context of small cohorts may actually undermine the intended goals of quality assessment across small cohorts .
for example , in an attempt to ensure equal difficulty of exams across cohorts , a single exam might be used across time and training location ( supposing it to be equivalent across contexts and academic years ) .
similarly , a ' core ' set of items may be used across exam administrations , sampling similar content across students in an attempt to ensure equivalence and comparability of performance , or assessment of critical knowledge .
however , if the same item , used across time , generates vastly different item properties across cohorts , the likelihood of keeping or removing a given item from an exam score will also vary .
the variation in whether or not an item is included in a final score could consequently impact content representativeness and overall exam difficulty .
these components of ' good assessment ' are of utmost importance to curriculum developers , course directors , and educational leaders , due to the accreditation requirement for equivalent and high quality assessments over time .
this study has some limitations .
it relied on archived examination reports , and so only discrimination and difficulty coefficients were available , not raw examination performance for individuals .
while this approach ensures the anonymity of examinees , it makes it impossible to examine the consequences of the application of item analysis guidelines in small cohorts on examination reliability and overall difficulty .
further , the purpose of this study was to explicitly document the presence of variance in item properties in small cohorts of examinees in health professions education , and as such we are currently unable to provide guidance regarding the boundaries of when , how , and under what circumstances item analysis guidelines can be applied without negative consequences to examination reliability , length , or difficulty , or how various approaches to equivalence may remedy this .
this study also applied item analysis guidelines in their ' purest ' form ; discrimination and difficulty were considered individually , and without information regarding exam blueprint or content coverage .
this may represent an under - nuanced application of item analysis guidelines , but we have few details regarding how item analysis guidelines are currently used or contextualized within health professions education .
to our knowledge no formal evaluation of quality for individual items was done beyond peer - review , revision and vetting , representing a potential limitation to our study .
we have no current access to item writers , nor to formal evaluations of item quality or data supporting construct alignment .
items were generated to align with overall curricular and course - specific learning objectives , and clinical clerks participated in the same academic half - days .
it is possible that other factors such as varying clinical teacher quality , specific metrics of item quality , or undetected issues of construct alignment may contribute to item property variance .
while we are currently unable to parse out the relative contribution of these factors to the item property instability we observed within this study , the instability of estimates of parameters in small cohorts is well supported in other domains ( e. g. law of large numbers in statistics ) .
the purpose of this study was to document and explore the extent of variability in item characteristics in mcq exams given to small cohorts ; we can provide few recommendations beyond cautioning the use of item analysis guidelines with small cohorts .
within the limits of the current work , we would suggest that recommendations based on guidelines ( to remove an item from a total score or not ) should be considered in parallel with other factors such as content representativeness , and that it is good practice to consult individuals with measurement expertise when making quality monitoring decisions for assessment , particularly in the context of assessment data derived from small cohorts .
future work will hopefully be able to provide stronger guidance and recommendations on the appropriate , or at least harm - minimizing , contexts for appropriate use of item analysis guidelines .
item properties are one means of examining item and examination quality , and often underlie important assessment decisions such as whether to exclude items from the final score .
variations due to small cohorts of exam takers raise concerns for assessment decisions based on these metrics , and the application of item analysis guidelines should be approached with caution within small assessment cohorts .
editor 's note : commentary by s. wright , doi 10.1007 / s40037 - 016 - 0323 - z the work reported in this article should be attributed to : centre for medical education , mcgill university and chaire de recherche en pegagogie medicale paul grand'maison de la societe des medecins de l'universite de sherbrooke .
acknowledgements the authors would like to thank ms. katharine dayem , mr. jing yang xaio , and ms. linda bergeron for their assistance with this project .
funding this study was conducted using funds granted by the social science and humanities research council ( sshrc : fund number 435 - 2014 - 2159 to csto and my ) and by the w. dale dauphinee fellowship by the medical council of canada ( mcc : fund number wdd - 1 - 2015 to bac ) .
references 1 .
roediger hl karpicke jd test - enhanced learning : taking memory tests improves long - term retention psychol sci 2006 17 249 255 10.1111 / j.1467 - 9280.2006.01693.x 16507066 2 .
roediger hl karpicke jd the power of testing memory : basic research and implications for educational practice perspect psychol sci 2006 1 181 210 10.1111 / j.1745 - 6916.2006.00012.x 26151629 3 .
larsen dp butler ac roediger hl iii test - enhanced learning in medical education med educ 2008 42 959 966 10.1111 / j.1365 - 2923.2008.03124.x 18823514 4 .
larsen dp butler ac roediger hl iii repeated testing improves long - term retention relative to repeated study : a randomised controlled trial med educ 2009 43 1174 1181 10.1111 / j.1365 - 2923.2009.03518.x 19930508 5 .
tamblyn r abrahamowicz m brailovsky c grand'maison p lescop j norcini j association between licensing examination scores and resource use and quality of care in primary care practice j am med assoc 1998 280 989 996 10.1001 / jama.280.11.989 6 .
tamblyn r abrahamowicz m dauphinee wd association between lincensure examination scores and practice in primary care j am med assoc 2002 288 3019 3026 10.1001 / jama.288.23.3019 7 .
tamblyn r abrahamowicz m dauphinee d physician scores on a national clinical skills examination as predictors of complaints to medical regulatory authorities jama 2007 298 993 1001 10.1001 / jama.298.9.993 17785644 8 .
wallach pm crespo lm holtzman kz galbraith rm swanson db use of a committee review process to improve the quality of course examinations adv health sci educ theory pract 2006 11 1 61 68 10.1007 / s10459 - 004 - 7515 - 8 16583285 9 .
haladyna tm downing sm rodriguez mc a review of multiple - choice item - writing guidelines for classroom assessment appl meas educ 2002 15 309 334 10.1207 / s15324818ame1503 _ 5 10 .
epstein rm assessment in medical education n engl j med 2007 356 387 396 10.1056 / nejme078002 17251535 11 .
wass v jones r van der vleuten c standardized or real patients to test clinical competence ?
the long case revisited med educ 2001 35 321 325 10.1046 / j.1365 - 2923.2001.00928.x 11318993 12 .
crocker l algina j introduction to classical and modern test theory 1986 new york holt , rinehart & winston 13 .
ebel rl frisbie da essentials of educational measurement 1991 englewood cliffs prentice - hall 14 .
nunnally j bernstein i psychometric theory 1994 3new york mcgraw - hill 15 .
laveault d gregoire j introduction aux theories des tests en psychologie et en sciences de l'education 2014 bruxelles de boeck 16 .
hogan tp stephenson r parent n introduction a la psychometrie 2012 montreal cheneliere - education 17 .
schmeiser cb welch cj test development educ meas 2006 4 307 353 18 .
nevo b item analysis with small samples appl psychol meas 1980 4 323 329 10.1177 / 014662168000400304 19 .
kromrey jd bacon tp item analysis of achievement tests based on small numbers of examinees 1992 20 .
millman j green j linn rl the specification and development of tests of achievement and ability educational measurement 1989 3new york ace / macmillan 335 366 21 .
nunnally jc bernstein ih berge j psychometric theory 1967 new york mcgraw - hill 22 .
health professional assessment consultancyfoundations of assessment - programme 2016 23 .
barbara b davis g quizzes , tests , and exams 1993 24 .
jones p smith rw talley d downing sm haladyna tm developing test forms for small - scale achievement testing systems handbook of test development 2006 new york routledge 487 525 25 .
laveault d gregoire j introduction aux theories des tests en sciences humaines 1997 bruxelles de boeck universite