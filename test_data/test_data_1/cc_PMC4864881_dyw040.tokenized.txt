int j epidemiol int j epidemiol ije intjepid international journal of epidemiology 0300 - 5771 1464 - 3685 oxford university press 27097747 4864881 10.1093 / ije / dyw040 dyw040 education corner outcome modelling strategies in epidemiology : traditional methods and basic alternatives greenland sander 1 daniel rhian 2 pearce neil 23 * 1 department of epidemiology and department of statistics , university of california , los angeles , ca , usa 2 department of medical statistics , london school of hygiene and tropical medicine , london , uk 3 centre for public health research , massey university , wellington , new zealand * corresponding author .
department of medical statistics , faculty of epidemiology and population health , london school of hygiene and tropical medicine , keppel street , london wc1e 7ht , uk. e - mail : neil.pearce @ lshtm.ac.uk 4 2016 20 4 2016 20 4 2016 45 2 565 575 05 2 2016 ( c ) the author 2016 .
published by oxford university press on behalf of the international epidemiological association .
2016 this is an open access article distributed under the terms of the creative commons attribution license ( http :// creativecommons.org / licenses / by / 4.0 / ) , which permits unrestricted reuse , distribution , and reproduction in any medium , provided the original work is properly cited .
abstract controlling for too many potential confounders can lead to or aggravate problems of data sparsity or multicollinearity , particularly when the number of covariates is large in relation to the study size .
as a result , methods to reduce the number of modelled covariates are often deployed .
we review several traditional modelling strategies , including stepwise regression and the ' change - in - estimate ' ( cie ) approach to deciding which potential confounders to include in an outcome - regression model for estimating effects of a targeted exposure .
we discuss their shortcomings , and then provide some basic alternatives and refinements that do not require special macros or programming .
throughout , we assume the main goal is to derive the most accurate effect estimates obtainable from the data and commercial software .
allowing that most users must stay within standard software packages , this goal can be roughly approximated using basic methods to assess , and thereby minimize , mean squared error ( mse ) .
wellcome trust10.13039 / 100004440107617 / z / 15 / z key messages the main goal of a statistical analysis of effects should be the production of the most accurate ( valid and precise ) effect estimates obtainable from the data and available software .
this goal is quite different from that of variable selection , which is to obtain a model that predicts observed outcomes well with the minimal number of variables ; this prediction goal is only indirectly related to the goal of change - in - estimate approaches , which is to obtain a model that controls most or all confounding with a minimal number of variables .
we illustrate some basic alternative modelling strategies that focus more closely on accurate effect estimation as measured by mean squared error ( mse ) and which can be implemented by practitioners with limited programming and consulting resources .
introduction we have recently reviewed traditional approaches to confounder selection for outcome ( risk ) and treatment ( propensity ) models , including significance - testing and ' change - in - estimate ' ( cie ) approaches .
1 we argued that the main goal of a statistical analysis of effects should be the production of the most accurate ( valid and precise ) effect estimates obtainable from the data and available software .
allowing that most users must stay within standard software packages , this goal can be roughly approximated using basic methods to minimize estimated mean squared error ( mse ) .
we here provide an illustrated overview of this approach .
scope , aims and assumptions as with our initial review , 1 our coverage is not intended for highly skilled practitioners ; rather , we target teachers , students and working epidemiologists who would like to do better with data analysis , but who lack resources such as r programming skills or a bona fide modelling expert committed to their project .
throughout , we assume that we are applying a conventional risk or rate regression model ( e.g. logistic , cox or poisson regression ) to estimate the effects of an exposure variable x on the distribution of a disease variable y while controlling for other variables , and that the outcome is uncommon enough so that distinctions among risk , rate and odds ratios can be ignored .
the other variables include forced variables , such as age and sex , which we may always want to control , and may also include unforced variables about which we are unsure whether to control .
we also assume that data checking , description and summarization have been done carefully .
2 finally , we assume that all quantitative variables have been : re - centreed to ensure that zero is a meaningful reference value present in the data ; and rescaled so that their units are meaningful differences within the range of the data ; 3 and that univariate distributions and background ( contextual ) information have been used to select categories or an appropriately flexible form ( e.g. splines ) for detailed modelling .
3 elsewhere we have discussed the issues involved in simply adjusting for all measured potential confounders .
1 this approach can be valid when the number of covariates is not too large in relation to the study size and the included covariates are not highly predictive of exposure .
nonetheless , controlling too many variables can lead to or aggravate problems arising from data sparsity or from high multiple correlation of exposure with the controlled confounders ( which we term multicollinearity ) , in which case one may seek to reduce the number of modelled covariates .
there are of course variables for which control may be inappropriate based on preliminary causal considerations .
these include intermediates ( variables on the causal pathway between exposure and diseases ) and their descendants 4 and any other variable influenced by the exposure or outcome .
5 - 7 these also include variables that are not part of minimal sufficient adjustment sets , whose control may increase bias .
4 - 11 we assume that these variables have been identified and eliminated e.g. using causal diagrams 4,6,8 to display contextual theory , 12 leaving us with a set of potential adjustment covariates ( often called ' potential confounders ') , including those variables that we are reasonably confident would reduce bias if controlled and our study size were unlimited .
we focus only on basic selection from these variables , leaving aside many difficult issues about model specification and diagnostics , 3,13 - 19 time - varying exposures and confounders , interactions and mediation .
20 - 23 multicollinearity and mean squared error : modified cie approaches one issue that is not explicitly considered or discussed in most epidemiological strategies is that of multicollinearity of covariates with exposure , i.e. when exposure is nearly a linear combination of other variables in the model .
this problem becomes most obvious in propensity - score analyses when the exposure is so well predicted that there is little overlap in the exposed and unexposed scores. with multicollinearity , exposure effect estimates become unstable , as reflected by large standard errors .
to combine bias and variance considerations when dealing with genuine confounders , consider estimation of an exposure effect measure represented by a single coefficient beta , such as a rate difference or log risk ratio .
the bias b in an estimator of beta is the difference between the expected value ( mean ) mu of the estimator and the ' true ' population value beta , so b = mu - beta . the standard error ( se ) of the estimator is just its standard deviation around that mean mu ; se 2 is thus the estimator 's variance .
the mean squared error ( mse ) of the estimator of beta combines these properties via the equation mse = b 2 + se 2 . 24 - 27 reducing multicollinearity by dropping variables can decrease the variance ( se 2 ) component of the mse , but may also increase the bias b in the estimator of beta if the dropped variables are indeed necessary to adjust for , given the retained variables .
thus we seek ways of reducing the se of the estimator ( e.g. by removing a source of multicollinearity ) without seriously increasing its bias b , so that the mse is reduced .
24,25,27 several formal methods seek to minimize mse in effect estimation with uncertain confounders , but require special programming .
19,28,29 we will describe a more crude approach that extends ordinary cie approaches 1 to consider estimated mse minimization using ordinary software outputs .
suppose we selectively delete confounders from a full model and see what happens to the exposure coefficient estimate and its standard error .
assuming the full - model estimate is unbiased , we can then estimate the bias b reduced from the deletion by the difference between the reduced - model estimate beta ^ reduced and full - model estimate beta ^ full . this step leads to the following equations for estimating the change in mse ( deltamse ) from reducing the model by deleting the confounder : where ( deltab ) 2 estimates the squared - bias increase from the deletion and delta ( se 2 ) estimates the variance decrease from the deletion .
a positive difference , i.e. ( deltab ) 2 > delta ( se 2 ) , indicates that the deletion increased the mse ; a negative difference indicates that the deletion reduced the mse .
we say ' indicates' because , of course , we have only rough estimates of b , se and mse , and beta ^ full , which will be approximately unbiased only when the model , the set of measured confounders and the sample size are all sufficient for approximate validity .
this approach is illustrated in box 1 , with an example involving two correlated variables , sodium and potassium intake .
box 1 we consider an example from a study of sodium intake in infancy ( age 4 months ) and blood pressure at 7 years .
30 the analysis involved adjusting for a relatively large number of potential confounders ( see table 1 ) .
a potentially important confounder was potassium intake at the same age , which was strongly correlated with sodium intake ( r = 0.81 ) .
this was reflected in an increase in the standard error for the sodium coefficient when potassium was also included in the model .
30 the authors therefore note that ' due to high sodium - potassium correlations , effect of sodium independent of potassium could not be estimated with reasonable precision ' , and they therefore did not control for potassium in the analyses .
we did rmse analyses ( table 1 ) , which showed that although there was an increase in the se of the sodium coefficient when potassium is included in the model ( compare model 1 with model 2a ) , the reduction in se from deleting potassium from the model is offset by the increase in bias ( sodium rmse = 0.294 with potassium excluded vs 0.290 with potassium included ) .
thus , controlling for potassium appears to be no worse in accuracy , in addition to having smaller approximate bias .
next , consider potassium as the main exposure : we obtain a lower rmse ( 0.095 ) for the potassium coefficient when including sodium compared with excluding sodium ( 0.130 ) ; thus controlling for sodium appears to be preferable .
table 1 .
associations of sodium and potassium intake at age 4 months with blood pressure ( bp ) at age 7 years 29 model exposure variables * coefficient estimate se for coefficient coefficient bias estimate indicates bias indicates large collinear root mse estimate * 1 sodium 0.518 0.290 referent 0.290 potassium 0.099 0.095 referent 0.095 2a sodium 0.708 0.225 0.190 yes yes 0.294 2b potassium 0.206 0.074 0.107 yes yes 0.130 * all analyses are adjusted for energy intake at 4 or 8 months , age at bp measurement , sex , socioeconomic position ( maternal and paternal education ) , family social class , maternal age at childbirth , parity , birthweight , gestational age , breastfeeding , smoking during pregnancy , sodium intake at 7 years .
as with cie , the exposure - coefficient change resulting from covariate deletion can be assessed by examining the estimated change directly , and also with a collapsibility test , i.e. a test of the hypothesis that the deletion does not change the exposure coefficients .
31 - 33 one caution to these approaches is that an accurate assessment of confounding may require examining changes from moving groups of variables .
regardless of the number of covariates being deleted , however , if there is one exposure term x , then a one degree of freedom chi - squared statistic for this hypothesis is chi c2 = ( deltab ) 2 / delta ( se 2 ) .
33 deleting a variable when deltamse > 0 is equivalent to deleting the variable when chi c2 < 1 , which corresponds to p > 0.32 for collapsibility .
appendix 1 ( available as supplementary data at ije online ) gives further details , describes a generalization of this test to exposures represented by multiple terms and suggests avenues for improvement .
to illustrate the general algorithms , denote by w 1 ,... , w j those variables ( such as age and sex ) that we want forced into all our models along with exposure x because they are expected to be important confounders or modifiers of the exposure effect measure , or because they are known strong risk factors that everyone wants to see in adjustment ; this list could include age splines , sex and ethnicity indicators etc .
our chief concern will be with the remaining variables u 1 ,... , u h , whose importance for adjustment is highly uncertain .
some hypothetical modelling results are shown in table 2 . we suppose result 1 is from a full model for the disease rate with exposure , the forced variables and all potential confounders .
results 2a - d then illustrate the four mutually exclusive possible outcomes of comparing a full ( maximal ) model including the potential confounders ( forced and unforced variables ) with a minimal model including only the main exposure and the forced variables .
result 2a suggests little or no confounding or multicollinearity problems , since there is little difference between the basic and full models ; we might therefore prefer the simplicity of reporting estimates from the minimal model .
in contrast , result 2b suggests there is confounding by the unforced variables , as seen by contrasting the exposure rate ratios from model 1 and model 2b , indicating that it is necessary to control at least some of the unforced variables .
table 2 .
hypothetical results from rate regressions in which a covariate is or is not a confounder or a source of multicollinearity model model variables exposure coefficient estimate rate ratio estimate se for coeff .
95 % cl coefficient bias estimate * indicates bias ?
indicates strongly collinear ?
root mse estimate * collapsibility chi 2 and p - value 33 1 x,w 1 ... w j , u 1 ... u h 0.693 2.00 0.24 1.25,3.20 referent 0.24 some mutually exclusive alternative possibilities under model 2 ( minimal model in which all unforced variables u 1 ... u h are dropped ) 2a x,w 1 ... w j 0.693 2.00 0.24 1.25 , 3.20 0 no no 0.24 0 , p = 1 2b x,w 1 ... w j 1.099 3.00 0.20 2.03 , 4.44 0.405 yes no 0.45 9.34 , p = 0.002 2c x,w 1 ... w j 0.693 2.00 0.14 1.52 , 2.63 0 no yes 0.14 0 , p = 1 2d x,w 1 ... w j 1.099 3.00 0.14 2.28 , 3.95 0.405 yes yes 0.43 4.03 , p = 0.04 * taking model 1 as the referent (' gold standard ') .
results 2c and 2d involve large multicollinearity , as indicated by the difference ( 0.14 compared with 0.24 ) in the standard error for the main exposure coefficient .
the more favourable situation is when the factors causing multicollinearity are very weak confounders , so they can be deleted from the model without increasing the mse of the exposure - effect estimate .
this situation is indicated when deleting these factors leaves the exposure - effect estimate virtually unchanged , but greatly reduces its standard error ( as in result 2c ) , suggesting that the minimal model provides more accurate estimates of the exposure effect ( i.e. it has a smaller mse ) .
again , we caution that this smaller standard error does not account for the preliminary testing and is thus too small by an unknown amount .
it is more difficult to proceed when multicollinearity arises from a strong confounder ( result 2d ) , since the increase in precision due to deleting such a confounder may be more than offset by an increase in confounding .
26 we thus must consider the net impact of reducing the se of the exposure - effect estimate while increasing its bias , and we do so by directly comparing square roots of estimated mse ( rmse ) ; we use the square roots to put the results back on the scale of the effects and biases .
in result 2d , the estimated rmse from the minimal model is substantially larger ( 0.43 ) than from the full model ( 0.24 ) , because the minimal model involves a large increase in confounding and a relatively smaller decrease in multicollinearity .
the task is then to identify a compromise model ( including some but not all the variables in question ) in which multicollinearity is reduced , but there is negligible increase in confounding .
this could occur , for example , if the variables most responsible for confounding were distinct from the variables most responsible for multicollinearity .
candidate variables can be assessed by dropping each variable in turn from the full model. of course , this process may fail to identify any acceptable model reduction , in which case the options are to stay with the full model or else turn to more sophisticated methods such as penalized estimation or hierarchical ( multilevel or mixed ) models to improve accuracy .
13,34 - 37 table 1 gives effect estimates without and with adjustment for the u h , which provides a basis for discussing the plausibility of residual confounding .
for example , if adjustment using imperfectly measured u h removes more than one - half of the excess rate associated with a particular main exposure , then it is reasonable to speculate that adjustment with better u h information would have removed most of the excess rate .
thus it can be worthwhile to present estimates from different degrees of adjustment .
based on the above considerations , box 2 outlines one backward - deletion strategy for screening out potential confounders .
this strategy is intended as a set of options , rather than a prescription ; it would be applicable in settings in which a full model can be fit without problems , there is not an inordinate number of potential confounders to consider and there is no clear and strong heterogeneity .
one implementation is as follows : b1 ) fit the full model , with no exposure - covariate products .
this model provides an average regression across the included covariates , even if heterogeneity is present .
38 - 40 b2 ) enter the following reduction loop , starting with the full model as the ' current model ' : for each candidate variable that remains in the current model , re - run the model without its terms ( the u h that represent it ) and compute the resulting deltamse relative to the current model from dropping those terms ; again , deltamse = ( beta ^ reduced - beta ^ current ) 2 - ( securrent2 - sereduced2 ) if any candidate in the model has deltamse < 0 ( indicating its deletion reduces mse ) , drop the one with the smallest ( most negative ) deltamse and go to step ( a ) if there is any candidate left in the model .
otherwise ( if there is no candidate u h left in the model , or none left have deltamse < 0 ) , stop and use the current model .
box 2 variable selection based on backward deletion using estimated mse reduction baseline specification 1.1 select the variables that are appropriate to include , using a causal directed acyclic graph ( dag ) to exhibit theorized causal relations among variables identified a priori as potentially important for estimating the effects of interest .
1.2 divide the variables into three classes : ( i ) the main exposure x ; ( ii ) forced - in variables ( e.g. age , sex ) which are always included in the model ( w 1 ... w j ) ; and ( iii ) the non - forced variables which will be candidates for deletion ( u 1 ... u h ) .
1.3 run a ' full ' model including all main exposure terms , forced - in variables and non - forced variables from 1.3 , with no exposure - covariate products .
[ if full model does not converge or the results indicate sparse - data bias , change to a forward - selection strategy , or use hierarchical ( multilevel or mixed ) or penalized modelling methods. ] variable selection enter the following reduction loop , starting with the full model as the ' current model ' : 2.1 for each candidate variable that remains in the current model , re - run the model without its terms ( the u h that represent it ) and compute the resulting deltamse relative to the current model from dropping those terms : ( beta ^ reduced - beta ^ current ) 2 - ( securrent2 - sereduced2 ) 2.2 if any candidate has deltamse < 0 , drop the one with the smallest ( most negative ) deltamse and go to step 4.2 if there are any candidates left in the model .
otherwise ( if there is no candidate u h left in the model , or none left have deltamse < 0 ) , stop and use the current model .
assessment of heterogeneity ( effect - measure modification ) 3.1 assess heterogeneity in a series of supplementary analyses , focusing on covariates of a priori interest we can also derive a parallel forward - selection strategy starting with the basic model when there are more potential confounders to consider than can reasonably fit at once ( e.g. when using too many of them results in sparse - data bias , thus spuriously inflating ( deltab ) 2 ) : f1 ) fit the basic model , with no exposure - covariate products .
f2 ) enter the following expansion loop , starting with the basic model as the ' current model ' : for each candidate variable that is not in the current model , re - run the model expanded with its terms u h and compute the deltamse from adding those terms .
if any candidate u h not in the model has deltamse > 0 ( indicating its addition reduces mse ) , enter the one with the largest deltamse and go to step ( a ) if any candidate remains left out .
otherwise ( if there are no more unselected candidates , or if none left out have deltamse > 0 ) , stop and use the current model .
both the above approaches can be viewed as a modification of conventional testing strategies in one major way : the test of the confounder coefficient is replaced by a test of collapsibility of the exposure coefficient over the confounder .
this test is easily constructed from ordinary outputs ( see appendix 1 , available as supplementary data at ije online ) and is appropriately sensitive to the confounder relation to exposure as well as to its relation to disease .
it can also be viewed as a modification of cie strategy that allows for random error in the observed change and for the possible variance reduction from deletion .
in box 3 , these approaches are applied to a study of atopy in poland , and their results are compared with other common approaches .
box 3 we consider an example from a study of the prevalence of atopy in a small town and neighbouring villages in poland in 2003 .
41 in the current analysis , we estimate the association between ' no current unpasteurized milk consumption ' and current atopy status .
it was plausible that lack of unpasteurized milk consumption could increase the risk of atopy .
because drinking unpasteurized milk happens mostly in rural settings , however , there are a number of other exposures which may be related to both unpasteurized milk consumption and the prevalence of atopy .
main exposure : never drinking unpasteurized milk ( 1 : never vs 0 : regularly / sometimes ) .
forced variables : age - group ( seven categories ) , sex .
potential confounders : live in town ( yes / no ) or village live on a farm ( yes / no ) contact ( regular / occasional ) with cows , pigs , poultry , sheep or goats , horses work ( regular / occasional ) milking cows , cleaning barns , collecting eggs firstborn ( yes / no ) number of siblings ( 1 , 2 , 3 +) current smoker ( yes / no ) lived in town ( yes / no ) or village as a child lived on a farm ( yes / no ) as a child parents were farmers ( yes / no ) family kept cows , pigs , poultry , sheep or goats , horses .
basic model model 1 in table 3 shows the results of the basic analysis for milk , adjusted for the forced variables ( age - group and sex ) .
full model model 2a in table 3 shows the results of the full maximum likelihood ( ml ) model , adjusting for all potential confounders ; there is a substantial change in the odds ratio for milk ( from 2.46 to 1.50 ) , but there is also an increase in the se for the coefficient estimate ( from 0.225 to 0.257 ) .
model 2b is the full model fit using the firth adjustment for coefficient - estimate bias .
42,43 this is used as the ' standard ' to estimate the bias of the other models , and is combined with the bootstrap ses to estimate the rmse .
overall , the milk coefficients from the full models have a much lower rmse ( 0.262 , 0.251 ) than in the basic model ( 0.567 ) because the increase in se from including all potential confounders is small in comparison with the change in the coefficient estimate .
traditional stepwise regression model 3a in table 1 shows the results of a forwards stepwise logistic regression ( using p < 0.20 as the criterion for inclusion ) with milk , age group and sex as forced variables ; town , firstborn , current smoker , town as a child , parents farmers , parents kept poultry and parents kept horses were also selected .
model 3b is again a forwards stepwise logistic regression but uses p < 0.05 as the criterion for inclusion .
model 3c and d are the backwards stepwise procedures with p < 0.20 and p < 0.05 , respectively .
aic model 4a in table 1 shows the results of using the akaike information criterion ( aic ) 14 where variables were forward selected to achieve the largest increase in aic at each step .
model 4b is from using aic for backwards deletion .
bic model 5a and b was selected in parallel to 4a and b but using the bayesian information criterion .
14 relative change - in - estimate approach only town residence ( in addition to the forced variables of age group and sex ) produced a substantial change in the estimate for milk ; once this was in the model , no other variable changed the milk odds ratio estimate by more than 10 % , leading to model 6a .
model 6b is from the analogous backwards procedure and resulted in the same model .
rmse model 7a in table 1 shows the results of using rmse reduction for forward selection in two different ways .
model 7a1 used ( at each step ) the larger of the two models being compared as the reference for estimating rmse reduction , and is thus analogous to the other procedures , whereas model 7a2 used the full model as the reference for each step .
model 7b is the backwards version of the same procedure .
model 7b1 used ( at each step ) the larger of the two models being compared as the reference ( for estimating the rmse ) , whereas model 7b2 used the full model as the reference for each step .
penali z ation following previous recommendations , 37,44 we included two analyses with weakly informative shrinkage priors for each coefficient .
the first analysis used a log - f ( 1,1 ) ( haldane ) prior distribution for each coefficient , which is equivalent to using an f ( 1,1 ) prior distribution for the odds ratio ( antilog ) from each coefficient , and assigns 95 % probability to the odds ratio falling between 1 / 648 and 648 .
the second analysis used a log - f ( 2,2 ) ( standard logistic ) distribution for each coefficient , which is equivalent to using an f ( 2,2 ) prior distribution for the odds ratio from each coefficient , and assigns 95 % probability to the odds ratio falling between 1 / 39 and 39 .
the priors were imposed by adding two pseudo - observations for each coefficient to the actual data file , with weights of ( 1 / 2 ) for the f ( 1,1 ) prior and weights of 1 for the f ( 2,2 ) prior , then fitting the full model to the augmented data set by maximum likelihood , with the constant term replaced by an indicator for ' actual - data record ' and weights of 1 for all actual - data records .
36,45,46 discussion in this example , all of the modelling approaches yielded reasonably similar findings - - the full model ( firth bias - adjusted ) yielded an or of 1.47 , and all of the other approaches produced ors in the range of 1.42 to 1.51 .
the rmses were also similar , smaller than that of the full model and substantially smaller than that for the basic model .
the fact that there exist models with lower estimated rmse than the models selected by the rmse procedures 7ab ( using the larger of the two models as the reference ) illustrates how a procedure that selects or rejects variables one at a time ( forwards or backwards ) does not always find the model with the overall optimal value of the criterion being used .
in this example , town is the only variable whose inclusion / exclusion in the model has much impact on the exposure effect estimate .
town is also highly predictive of the outcome .
thus , all methods select it , and whatever else they happen to select makes very little difference for any of the measures considered .
for the same reasons , the bootstrap 95 % cis ( which take variable selection into account ) were in general only slightly larger than the ' standard ' 95 % cis .
we therefore see little apparent advantage of one method over another in this example .
nonetheless , in a setting with strong confounding by intercorrelated groups of multiple confounders , we might find more stark differences among the results from different methods .
table 3 .
model - adjusted associations of current unpasteurized milk consumption with current atopy status 40 model model variables * exposure coefficient estimate se for coefficient or 95 % cl for or estimated bias and rmse bootstrap se + for coefficient bootstrap 95 % cl ++ for or 1 ( basic ) milk 0.899 0.225 2.46 1.58 , 3.82 0.516 0.567 0.236 1.59 , 3.97 2a ( ml full ) milk all other variables # 0.406 0.257 1.50 0.91 , 2.48 0.023 0.262 0.261 0.89 , 2.46 2b ( firth ) 42,43 milk all other variables # 0.383 0.252 1.47 0.91 , 2.40 0.000 0.251 0.251 0.89 , 2.37 3a ( forwards stepwise , p < 0.20 ) milk town firstborn current smoker town as a child parents farmers parents kept poultry parents kept horses 0.390 0.244 1.48 0.91 , 2.38 0.007 , 0.261 0.261 0.87 , 2.43 3b ( forwards stepwise , p < 0.05 ) milk town current smoker town as a child parents kept poultry 0.383 0.243 1.47 0.91 , 2.36 < 0.001 0.261 0.261 0.88 , 2.44 3c ( backward stepwise , p < 0.20 ) milk town firstborn current smoker parents farmers parents kept poultry parents kept horses 0.398 0.244 1.49 0.92 , 2.40 0.015 0.261 0.261 0.88 , 2.47 3d ( backward stepwise , p < 0.05 ) milk town current smoker parents farmers parents kept poultry parents kept horses 0.414 0.244 1.51 0.94 , 2.44 0.031 0.265 0.263 0.93 , 2.61 4a ( forwards aic ) milk town horses firstborn current smoker parents kept poultry 0.381 0.243 1.46 0.91 , 2.36 - 0.002 0.260 0.260 0.86 , 2.39 4b ( backward aic ) milk town horses firstborn current smoker parents kept poultry parents kept horses parents farmers 0.398 0.244 1.49 0.92 , 2.40 0.015 0.262 0.262 0.88 , 2.48 5a ( forwards bic ) milk town current smoker parents kept poultry 0.393 0.243 1.48 0.92 , 2.39 0.010 0.264 0.264 0.88 , 2.45 5b ( backward bic ) milk town current smoker parents kept poultry 0.393 0.243 1.48 0.92 , 2.39 0.010 0.264 0.264 0.87 , 2.45 6a ( forwards cie ) milk town 0.400 0.242 1.49 0.93 , 2.39 0.017 0.255 0.254 0.93 , 2.56 6b ( backward cie ) milk town 0.400 0.242 1.49 0.93 , 2.39 0.017 0.255 0.254 0.92 , 2.52 7a ( forwards rmse , larger model as referent ) milk town poultry collecting eggs number of siblings parents kept cows parents kept poultry 0.363 0.245 1.44 0.89 , 2.32 - 0.020 0.258 0.257 0.86 , 2.35 7b ( backward rmse , larger model as referent ) milk town poultry collecting eggs firstborn 0.350 0.243 1.42 0.88 , 2.29 0.017 0.257 0.256 0.84 , 2.28 8a ( forwards rmse , full model as referent ) milk town 0.400 0.242 1.49 0.93 , 2.39 - 0.033 0.263 0.261 0.88 , 2.45 8b ( backward rmse , full model as referent ) milk town parents kept cows parents kept poultry 0.407 0.242 1.50 0.94 , 2.42 0.024 0.264 0.263 0.89 , 2.51 9a penalization by log - f ( 1,1 ) priors s45 milk all other variables # 0.396 0.253 1.49 0.90 , 2.44 0.013 0.253 0.253 0.90 , 2.42 9b penalization by log - f ( 2,2 ) priors $ 45 milk all other variables # 0.389 0.250 1.47 0.90 , 2.41 0.006 0.246 0.246 0.90 , 2.36 * all analyses are adjusted for age group and sex .
+ based on 4000 bootstrap samples .
++ bias - corrected and accelerated ( bca ) with 4000 resamples .
56 # town , farm , cows , pigs , poultry , sheep / goats , horses , milking cows , cleaning barns , collecting eggs , firstborn , number of siblings , current smoker , lived in town or village as a child , parents were farmers , family kept cows , family kept pigs , family kept poultry , family kept sheep or goats , family kept horses .
s equivalent to f ( 1,1 ) prior for odds ratio ; 95 % prior limits are 1 / 648 , 648 .
$ equivalent to f ( 2,2 ) prior for odds ratio ; 95 % prior limits are 1 / 39 , 39 .
some limitations as with most variable - selection procedures including stepwise and cie , confidence intervals obtained by combining the final point estimate and se from the above strategy are not theoretically valid .
simulation studies 24,25 so far suggest that this invalidity is negligible in typical settings , due to the high significance level and therefore liberal inclusion implicit in using deltamse = 0 as the decision point .
nonetheless , the strategy could be improved by using bootstrapping or cross - validation to estimate deltamse and set confidence intervals .
a further problem with using cie strategies for logistic regression is that it is possible the change in estimate is largely due to more sparse - data bias ( i.e. too few subjects at crucial combinations of variables ) in the full - model estimate beta ^ full rather than increased confounding in the reduced - model estimate beta ^ reduced . for a binary exposure x and disease y , this problem becomes noticeable when there are much fewer than about 4 subjects per confounder coefficient at each exposure - disease combination ; for example , with 7 confounder terms we would want at least 4 ( 7 ) = 28 subjects in each cell of the two - way xy table for some assurance that sparse - data bias in beta ^ full is small .
one way to avoid this problem is to switch to penalized estimation ; it is also possible to apply the above reduction algorithms after minimal penalization to reduce sparse - data bias .
44 - 48 another problem however is that logistic coefficients are in general not collapsible , in that there will be differences between the actual ( underlying ) coefficients with and without a given covariate if the covariate predicts the outcome , even if that covariate is not a confounder by virtue of being independent of exposure .
6 this difference will be negligible unless the outcome is common , in which case it will be advisable to switch to estimation of collapsible effect measures ( such as risk ratios and differences ) , e.g. by regression standardization .
13 discussion like more sophisticated but computationally intensive methods , 19 the strategies we describe differ from stepwise regression and other purely predictive approaches , in that their goal is to improve accuracy of exposure effect estimates rather than to simply predict outcomes .
at the same time , recognizing that the gap between state - of - the - art methods and what is done in most publications has only grown over time , they are intended to fall within the scope of the limits on software and effort that constrain typical researchers .
thus , parsimony is replaced by the goal of minimizing error in effect estimation .
a related point is that , as with parsimony , pursuit of goodness - of - fit may lead to inappropriate decisions about confounder control ; in particular , some variables may not be included in the model because they do not significantly improve the fit , even though they are important confounders .
' global ' tests of fit are especially inadequate for confounder selection 13 since there can be many ' good - fitting ' models that correspond to very different confounder effects and exposure effect estimates .
26 parsimony and goodness - of - fit are helpful only to the extent they reduce variance and bias of the targeted effect estimate .
the general inappropriateness of parsimony as a goal in causal analysis is supported by simulation studies in which full - model analysis has often outperformed conventional selection strategies .
24,25,27 this result raises the question : if we can control for all potential confounders , then why would n't we ?
if indeed we have numbers so large that there is no problem from controlling too many variables , we would generally expect covariate elimination to provide little benefit for the accuracy of effect estimates .
but the harsh reality is that even databases of studies with hundreds of thousands of patients often face severe limits in crucial categories , such as the number of exposed cases .
coupled with the availability of what may be hundreds or even thousands of variables , some kind of algorithmic approach to potential confounders becomes essential .
49,50 the strategies we describe are designed for common borderline situations in which control of all the variables may be possible , but some accuracy improvement may be expected from eliminating some or all variables whose inclusion is of uncertain benefit .
a number of criticisms can be made of the mse - based strategy in box 2 .
first , it can be argued that any data - based model reduction will produce biased estimates because it depends on the assumption that it is not necessary to control the omitted variables ( conditional on control of the included variables ) .
51 we regard this criticism as somewhat misguided insofar as every epidemiological estimate suffers from some degree of bias from uncontrolled confounders , differential subject selection and measurement error ( in both exposures and confounders ) ; the key question is then whether the bias from omitting a variable is of contextual importance .
second , as we have emphasized , simple selection methods ( such as stepwise , cie and apparent mse change ) do not take account of random variability introduced by data - based model selection .
thus , without cross - validation or some other adjustment , the standard error of the resulting effect estimate is not correctly estimated by taking the standard error computed from the final model .
15 with methods that focus on the effect estimate , however , the eliminated variables are generally those that have only weak relations to exposure or disease , the resulting problem is limited .
25 where such problems are of concern , they can be mitigated by the use of shrinkage , penalization and related hierarchical methods , 13,14,34 - 36,45,46,52,53 model averaging , 54,55 cross - validation 19 or bootstrapping .
56 third , the mse approaches we describe may encounter technical difficulties in precisely the situation of most concern here , namely when there is multicollinearity .
as we mentioned , sparse - data bias is a chief concern along with related artefacts due to sample - size limitations , which again suggests using in the mse algorithms the bias - reduced estimates available in commercial software .
45,46 the strategies we have presented in this paper are in no sense optimal ; rather they are rough but transparent heuristics which attempt to mitigate some of the difficulties of common approaches without introducing too much new machinery or subtle statistical concepts .
regardless of the strategy adopted , however , it is important that authors document how they chose their models , so that readers can interpret their results in light of the strengths and weaknesses attendant on the strategy that they used .
supplementary data supplementary data are available at ije online .
funding r.d. acknowledges support from a sir henry dale fellowship jointly funded by the wellcome trust and the royal society ( grant number 107617 / z / 15 / z ) .
the centre for public health research is supported by a programme grant from the health research council of new zealand .
the research leading to these results has received funding from the european research council under the european union 's seventh framework programme ( fp7 / 2007 - 2013 ) / erc grant agreement no 668954 .
acknowledgements we thank deborah lawlor for suggesting the example used in box 1 , and for the analysis summarized in table 3 . we thank barbara sozanska for the use of the data reported in box 3 .
conflict of interest : none declared .
references 1 greenland s pearce n. statistical foundations for model - based adjustments . ann rev public health 2015 ; 36 : 89 - 108 . 25785886 2 greenland s rothman kj. fundamentals of epidemiologic data analysis . chapter 13 .
in : rothman kj greenland s lash tl ( eds ) .
modern epidemiology . 3rd edn.philadelphia , pa : lippincott williams & wilkins , 2008 . 3 greenland s. introduction to regression models . chapter 20 .
in : rothman kj greenland s lash tl ( eds ) .
modern epidemiology . 3rd edn.philadelphia : lippincott williams & wilkins , 2008 . 4 pearl j. causality : models , reasoning , and inference . 2nd edn.new york , ny : cambridge university press , 2009 . 5 cole sr platt rw schisterman ef . illustrating bias due to conditioning on a collider . int j epidemiol 2010 ; 39 : 417 - 20 . 19926667 6 greenland s pearl j. adjustments and their consequences - collapsibility analysis using graphical models . int stat rev 2011 ; 79 : 401 - 26 . 7 rothman kj greenland s lash tl. validity in epidemiologic studies . chapter 9 .
in : rothman kj greenland s lash tl ( eds ) .
modern epidemiology . 3rd edn.philadelphia , pa : lippincott williams & wilkins , 2008 . 8 glymour mm greenland s. causal diagrams . chapter 12 .
in : rothman kj greenland s lash tl ( eds ) .
modern epidemiology . philadelphia , pa : lippincott williams & wilkins , 2008 . 9 myers ja rassen ja gagne jj . effects of adjusting for instrumental variables on bias and precision of effect estimates . am j epidemiol 2011 ; 174 : 1223 - 27 . 22034488 10 pearl j. on a class of bias - amplifying covariates that endanger effect estimates .
in : grunwald p , spirtes p ( eds ) .
proceedings of the twenty - sixth conference on uncertainty in artificial intelligence , catalina island , ca , 8 - 11 july 2010 . corvallis , or : auai , 2010 : 417 - 24 .
11 vanderweele tj shpitser i. on the definition of a confounder . ann stat 2013 ; 41 : 196 - 220 . 25544784 12 krieger n. epidemiology and the people 's health : theory and context . new york , ny : oxford university press , 2011 . 13 greenland s. introduction to regression modelling . chapter 21 .
in : rothman kj greenland s lash tl ( eds ) .
modern epidemiology . 3rd edn.philadelphia , pa : lippincott williams & wilkins , 2008 . 14 harrell f. regression modelling strategies . new york , ny : springer , 2001 . 15 hastie t tibshirani r friendman j. the elements of statistical learning : data mining , inference and prediction . 2nd edn.new york , ny : springer , 2009 . 16 leamer e. specification searches . new york , ny : wiley , 1978 . 17 royston p sauerbrei w. multivariable model - building : a pragmatic approach to regression analysis based on fractional polynomials for modelling continuous variables . chichester , uk : john wiley & sons , 2008 . 18 steyerberg ew. clinical prediction models : a practical approach to development , validation , and updating . new york , ny : springer , 2008 . 19 van der laan m rose r. targeted learning : causal inference for observational and experimental data . new york , ny : springer , 2011 . 20 robins jm. marginal structural models versus structural nested models as tools for causal inference . in : halloran me berry d ( eds ) .
institute for mathematics and its applications 116 . new york , ny : springer , 1999 . 21 robins jm hernan ma. estimation of the causal effects of time - varying exposures . in : fitzmaurice g davidian m verbeke g molenberghs g ( eds ) .
longitudinal data analysis . new york , ny : chapman and hall / crc press , 2009 . 22 vanderweele t. explanation in causal inference . new york : oxford university press , 2015 . 23 valeri l vanderweele tj. mediation analysis allowing for exposure - mediator interactions and causal interpretation : theoretical assumptions and implementation with sas and spss macros . psychol methods 2013 ; 18 : 137 - 50 . 23379553 24 maldonado g greenland s. simulation study of confounder - selection strategies . am j epidemiol 1993 ; 138 : 923 - 36 . 8256780 25 mickey rm greenland s. the impact of confounder selection criteria on effect estimation . am j epidemiol 1989 ; 129 : 125 - 37 . 2910056 26 robins jm greenland s. the role of model selection in causal inference from nonexperimental data . am j epidemiol 1986 ; 123 : 392 - 402 . 3946386 27 weng hy hsueh yh messam llm hertz - picciotto i. methods of covariate selection : directed acyclic graphs and the change - in - estimate procedure . am j epidemiol 2009 ; 169 : 1182 - 90 . 19363102 28 greenland s. reducing mean squared error in the analysis of stratified epidemiologic studies . biometrics 1991 ; 47 : 773 - 75 . 1912271 29 vansteelandt s bekaert m claeskens g. on model selection and model misspecification in causal inference . stat methods med res 2012 ; 21 : 7 - 30 . 21075803 30 brion mj ness ar davey smith g . sodium intake in infancy and blood pressure at 7 years : findings from the avon longitudinal study of parents and children . eur j clin nutr 2008 ; 62 : 1162 - 69 . 17622260 31 clogg cc petkova e haritou a. statistical methods for comparing regression coefficients between models . am j sociol 1995 ; 100 : 1261 - 93 . 32 greenland s maldonado g. inference on collapsibility in generalized linear models . biometr j 1994 ; 36 : 771 - 82 . 33 greenland s mickey rm. closed form and dually consistent methods for inference on strict collapsibility in 2x2xk and 2xjxk tables . appl stat 1988 ; 37 : 335 - 43 . 34 greenland s. when should epidemiologic regressions use random coefficients ?
biometrics 2000 ; 56 : 915 - 21 . 10985237 35 greenland s. principles of multilevel modelling . int j epidemiol 2000 ; 29 : 158 - 67 . 10750618 36 greenland s. bayesian perspectives for epidemiological research. ii. regression analysis . int j epidemiol 2007 ; 36 : 195 - 202 . 17329317 37 greenland s. invited commentary : variable selection versus shrinkage in the control of multiple confounders . am j epidemiol 2008 ; 167 : 523 - 29 . 18227100 38 greenland s maldonado g. the interpretation of multiplicative model parameters as standardized parameters . stat med 1994 ; 13 : 989 - 99 . 8073203 39 maldonado g greenland s. interpreting model coefficients when the true model form is unknown . epidemiology 1993 ; 4 : 310 - 18 . 8347741 40 white h. estimation , inference , and specification analysis . new york , ny : cambridge university press , 1994 . 41 sozanska b macneill sj kajderowicz - kowalik m . atopy and asthma in rural poland : a paradigm for the emergence of childhood respiratory allergies in europe . allergy 2007 ; 62 : 394 - 400 . 17362250 42 firth d. bias reduction of maximum likelihood estimates . biometrika 1993 ; 80 : 27 - 38 . 43 firth d. bias reduction of maximum likelihood estimates . biometrika 1995 ; 82 : 667 - 67 . 44 greenland s mansournia ma. penalization , bias reduction , and default priors in logistic and related categorical and survival regressions . stat med 2015 ; 34 : 3133 - 43 . 26011599 45 discacciatti a orsini n greenland s. bayesian logistic regression via penalized likelihood estimation . stata j 2015 ; 15 : 3 . 46 sullivan s greenland s. bayesian regression in sas software . int j epidemiol 2013 ; 42 : 308 - 17 . 23230299 47 greenland s . smoothing observational data : a philosophy and implementation for the health sciences . int statist rev 2006 ; 74 : 31 - 46 . 48 greenland s schwartzbaum ja finkle wd. problems due to small samples and sparse data in conditional logistic regression analysis . am j epidemiol 2000 ; 151 : 531 - 39 . 10707923 49 joffe mm. exhaustion , automation , theory , and confounding . epidemiology 2009 ; 20 : 523 - 24 . 19525688 50 schneeweiss s rassen ja glynn rj avorn j mogun h brookhart ma. high - dimensional propensity score adjustment in studies of treatment effects using health care claims data . epidemiology 2009 ; 20 : 512 - 22 . 19487948 51 greenland s. modelling and variable selection in epidemiologic analysis . am j public health 1989 ; 79 : 340 - 49 . 2916724 52 greenland s. methods for epidemiologic analyses of multiple exposures : a review and a comparative study of maximum - likelihood , preliminary testing , and empirical - bayes regression . stat med 1993 ; 12 : 717 - 36 . 8516590 53 witte js greenland s kim ll arab l. multilevel modelling in epidemiology with glimmix . epidemiology 2000 ; 11 : 684 - 88 . 11055630 54 greenland s. multilevel modelling and model averaging . scand j work environ health 1999 ; 25 : 43 - 48 . 10628440 55 wang c parmigiani g dominici f. bayesian effect estimation accounting for adjustment uncertainty ( with discussion ) . biometrics 2012 ; 68 : 661 - 86 . 22364439 56 davison ac hinkley dv. bootstrap methods and their application . new york , ny : cambridge university press , 1997 .