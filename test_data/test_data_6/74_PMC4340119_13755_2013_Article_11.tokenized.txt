health inf sci syst health inf sci syst health information science and systems 2047 - 2501 biomed central london 25825662 4340119 11 10.1186 / 2047 - 2501 - 1 - 11 research analysis of contingency tables based on generalised median polish with power transformations and non - additive models klawonn frank frank.klawonn @ helmholtz - hzi.de jayaram balasubramaniam jbala @ iith.ac.in crull katja katja.crull @ gmx.de kukita akiko kukita @ cc.saga - u.ac.jp pessler frank frank.pessler @ helmholtz - hzi.de bioinformatics and statistics , helmholtz centre for infection research , inhoffenstr .
7 , braunschweig , d - 38124 germany ostfalia university of applied sciences , salzdahlumer str .
46 / 48 , wolfenbuettel , d - 38302 germany department of mathematics , indian institute of technology hyderabad , yeddumailaram , 502 205 india department of molecular immunology , helmholtz centre for infection research , inhoffenstr .
7 , braunschweig , d - 38124 germany department of microbiology , saga medical school , saga , japan department of epidemiology , helmholtz centre for infection research , inhoffenstr .
7 , braunschweig , d - 38124 germany 30 5 2013 30 5 2013 2013 1 11 27 3 2013 14 5 2013 ( c ) klawonn et al. ; licensee biomed central ltd .
2013 this article is published under license to biomed central ltd .
this is an open access article distributed under the terms of the creative commons attribution license ( http :// creativecommons.org / licenses / by / 2.0 ) , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .
contingency tables are a very common basis for the investigation of effects of different treatments or influences on a disease or the health state of patients .
many journals put a strong emphasis on p - values to support the validity of results .
therefore , even small contingency tables are analysed by techniques like t - test or anova .
both these concepts are based on normality assumptions for the underlying data .
for larger data sets , this assumption is not so critical , since the underlying statistics are based on sums of ( independent ) random variables which can be assumed to follow approximately a normal distribution , at least for a larger number of summands .
but for smaller data sets , the normality assumption can often not be justified .
robust methods like the wilcoxon - mann - whitney - u test or the kruskal - wallis test do not lead to statistically significant p - values for small samples .
median polish is a robust alternative to analyse contingency tables providing much more insight than just a p - value .
median polish is a technique that provides more information than just a p - value .
it explains the contingency table in terms of an overall effect , row and columns effects and residuals .
the underlying model for median polish is an additive model which is sometimes too restrictive .
in this paper , we propose two related approach to generalise median polish .
a power transformation can be applied to the values in the table , so that better results for median polish can be achieved .
we propose a graphical method how to find a suitable power transformation .
if the original data should be preserved , one can apply other transformations - based on so - called additive generators - that have an inverse transformation .
in this way , median polish can be applied to the original data , but based on a non - additive model .
the non - linearity of such a model can also be visualised to better understand the joint effects of rows and columns in a contingency table .
issue - copyright - statement ( c ) biomed central ltd 2013 introduction contingency tables often arise from collecting patient data and from lab experiments .
the rows and columns of a contingency table correspond to two different categorical attributes .
one of these categorical attributes could account for different drugs with which patients are treated and the other attribute could stand for different forms of the same disease .
each cell of the table contains a numerical entry which reflects a measurement under the combination of the categorical attributes corresponding to the cell .
in the example above , these entries could be the number of patients that have been cured from the disease by the drug corresponding to the cell. or it could be the time or average time it took patients to recover from the disease while being treated with the drug .
table 1 shows an example of a contingency table .
the rows correspond to six different groups .
the columns in this case reflect replicates .
the columns correspond to 3 replicates of a gene expression experiment where cultured cells were transfected with increasing amounts of an effector plasmid ( a plasmid expressing a protein that increases the expression of a gene contained on a second plasmid , referred to as a reporter plasmid ) in the presence or absence of the reporter plasmid .
rows 1 - 3 constitute the negative control experiment , in which increasing amounts of the effector plasmid were transfected , but no reporter plasmid .
the experiments in rows 4 - 6 are identical to those in 1 - 3 , except that increasing amounts of the reporter plasmid were co - transfected .
the data correspond to the intensity of the signal derived from the protein which is expressed by the reporter plasmid .
table 1 a contingency table group replicate g1 6.39 8.10 6.08 g2 8.95 7.48 6.57 g3 5.61 8.58 5.72 g4 813.70 686.50 691.20 g5 4411.50 3778.90 4565.30 g6 32848.40 28866.00 46984.40 a typical question to be answered based on data from a contingency table is whether the rows or the columns show a significant difference .
in the case of the treatment of patients with different drugs for different diseases , one could ask whether one of the drugs is more efficient than the other ones or whether one disease is more severe than the other ones .
for the example of the contingency table 1 , one would be interested in significant differences among the groups , i.e. the rows .
but it might also be of interest whether there might be significant differences in the replicates , i.e. the columns .
if the latter question had a positive answer , this could be a hint to a batch effect , which turn out to be a serious problem in many experiments [ 1 ] .
hypothesis tests are a very common way to carry out such analysis .
one could perform a pairwise comparison of the rows or the columns by the t - test .
however , the underlying assumption for the t - test is that the data in the corresponding rows or columns originate from normal distributions .
for very large contingency tables , this assumption is not very critical , since the underlying statistics will be approximately normal , even if the data do not follow a normal distribution .
non - parametric tests like the wilcoxon - mann - whitney - u test are a possible alternative .
however , for very small contingency tables they cannot provide significant p - values .
in any case , a correction for multiple testing - like bonferroni ( see for instance [ 2 ]) , bonferroni - holm [ 3 ] or false discovery rate ( fdr ) correction [ 4 ] - needs to be carried in the case of pairwise comparisons .
instead of pairwise comparisons with correction for multiple testing , analysis of variance ( anova ) is often applied instead of the t - test .
concerning the underlying model assumptions , anova is even more restrictive than the t - test , since it does even assume that the underlying normal distributions have identical variance .
anova is also - like the t - test - very sensitive to outliers .
the kruskal - wallis test is the corresponding counterpart of the wilcoxon - mann - whitney - u test , carrying out a simultaneous comparison of the medians .
but it suffers from the same problems as the wilcoxon - mann - whitney - u test and is not able to provide significant p - values for small samples [ 5 ] .
a general question is whether a p - value is required at all .
a p - value can only be as good as the underlying statistical model and a lot of information is lost when the interestingness of a whole contingency table is just reflected by a single p - value .
in the worst case , a t - test or anova can yield a significant p - value just because of a single outlier .
median polish [ 6 ] - a technique from robust statistics and exploratory data analysis - is another way to analyse contingency tables based on a simple additive model .
we briefly review the idea of median polish in terms of a simple additive model .
although the simplicity of median polish as an additive model is appealing , it is sometimes too simple to analyse contingency table .
very often , especially in the context of gene , protein or metabolite expression profile experiments , the measurements are not taken directly , but are transformed before further analysis .
in the case of expression profiles , it is common to apply a logarithmic transformation .
the logarithmic transformation is a member of a more general family , the so - called power transformations which we use to introduce a method to find a suitable power transformation that yields the best results for median polish for a given contingency table .
the leads to median polish based on an additive model , but with transformed attribiutes .
we further extend the presented ideas , by transforming the median polish back to the original domain of the attributes .
this back - transformation requires special transformations related to additive generators. with such back - transformation the median polish result can be interpreted on the original data values as non - additive model .
finally , we illustrate how to visualise the non - linearity exploited by the non - additive median polish model .
this paper combines the ideas that were presented in [ 7 , 8 ] .
median polish median polish has been applied to medical and biomedical contingency tables in various settings [ 9 - 11 ] .
the underlying additive model of median polish is that each entry xij in the contingency table can be written in the form g represents the overall or grand effect in the table .
this can be interpreted as general value around which the data in the table are distributed .
ri is the row effect reflecting the influence of the corresponding row i on the values .
cj is the column effect reflecting the influence of the corresponding column j on the values .
epsilonij is the residual or error in cell ( i,j ) that remains when the overall , the corresponding row and column effect are taken into account .
the overall , row and column effects and the residuals are computed by the following algorithm .
for each row compute the median , store it as the row median and subtract it from the values in the corresponding row .
the median of the row medians is then added to the overall effect and subtracted from the row medians .
for each column compute the median , store it as the column median and subtract it from the values in the corresponding column .
the median of the column medians is then added to the overall effect and subtracted from the column medians .
repeat steps 1 - 4 until no changes ( or very small changes ) occur in the row and column medians .
table 2 shows the result of median polish applied to table 1 .
table 2 median polish for the data in table 1 overall : 350.075 r1 r2 r3 row effect g1 0.000 4.795 - 0.310 - 343.685 g2 0.000 1.615 - 2.380 - 341.125 g3 - 0.110 5.945 0.000 - 344.355 g4 122.500 - 1.615 0.000 341.125 g5 0.000 - 629.515 153.800 4061.425 g6 0.000 - 3979.315 14136.000 32498.325 column effect 0.000 - 3.085 0.000 the result of median polish can help to better understand the contingency table .
in the ideal case , the residuals are zero or at least close to zero .
close to zero means in comparison to the row or column effects .
if most of the residuals are close to zero , but only a few have a large absolute value , this is an indicator for outliers that might be of interest .
most of the residuals in table 1 are small , except the ones in the lower right part of the table .
the row effect shows how much influence each row , i.e. in the example , each group has .
one can see that group g1 , g2 and g3 have roughly the same effect .
group g5 and g6 have extremely high influence and show very significant effects .
the column effects are interpreted in the same way .
since the columns represent replicates , they shall have no effect at all in the ideal case .
otherwise , some batch effect might be the cause .
the column effects in table 1 are - as expected - all zero or at least close to zero .
power transformations transformation of data is a very common step of data preprocessing ( see for instance [ 12 ]) .
there can be various reasons for applying transformations before other analysis steps , like normalisation , making different attribute ranges comparable , achieving certain distribution properties of the data ( symmetric , normal etc. ) or gaining advantage for later steps of the analysis .
power transformations ( see for instance [ 6 ]) are a special class of parametric transformations defined by it is assumed that the data values x to be transformed are positive .
if this is not the case , a corresponding constant ensuring this property should be added to the data .
we restrict our considerations on power transformations that preserve the ordering of the values and therefore exclude negative values for lambda .
in the following section , we use power transformations to improve the results of median polish .
finding a suitable power transformations for median polish an ideal result for median polish would be when all residuals are zero or at least small .
the residuals get smaller automatically when the values in the contingency table are smaller .
this would mean that we tend to put a high preference on the logarithmic transformation ( lambda = 0 ) , at least when the values in the contingency table are greater than 1 .
small for residuals does not refer to the absolute values of the residuals being small .
it means that the residuals should be small compared to the row or column effects .
therefore , we should compare the absolute values of the residuals to the absolute values of the row or column effects .
one way to do this would be to compare the mean values of the absolute values of the residuals to the mean value of the absolute values of the row or column effects .
this would , however , be not consistent in the line of robust statistics .
single outliers could dominate this comparison .
this would also lead to the reverse effect as considering the residuals alone .
power transformations with large values for lambda would be preferred , since they make larger values even larger. and since the row or column effects tend to be larger than the residuals in general , one would simply need to choose a large value for lambda to emphasize this effect .
neither single outliers of the residuals nor of the row or column effects should have an influence on the choice of the transformation .
what we are interested in is being able to distinguish between significant row or column effects and residuals .
therefore , the spread of the row or column effects should be large whereas at least most of the absolute values of the residuals should be small .
to measure the spread of the row or column effects , we use the interquartile range which is a robust measure of spread and not sensitive to outliers like the variance .
the interquartile range is the difference between the 75 % - and the 25 % - quantile , i.e. the range that contains 50 % percent of the data in the middle .
we use the 80 % quantile of the absolute values of all residuals to judge whether most of the residuals are small .
it should be noted that we do not expect all residuals to be small .
we might have single outliers that are of high interest .
finally , we compute the quotient of the interquartile range of the row or column effects and divide it by the 80 % quantile of the absolute values of all residuals .
we call this quotient the iqroq value ( interquartile range over the 80 % quantile of the absolute residuals ) .
the higher the iqroq value , the better is the result of median polish .
for each value of lambda , we apply the corresponding power transformation to the contingency table and calculate the iqroq value .
in this way , we obtain an iqroq plot , plotting the iqroq value depending on lambda. of course , the choice of the interquartile range - we could also use the range that contains 60 % percent of the data in the middle - and the 80 % - quantile for the residuals are rules of thumb that yield good results in our applications .
if more is known about the data , for instance that outliers should be extremely rare , one could also choose a higher quantile for the residuals .
before we come to examples with real data in the next section , we illustrate our method based on artificially generated contingency tables .
the first table is a 10 x 10 , generated by the following additive model .
the overall effect is 0 , the row effects are 10,20,30 ,... , 100 , the column effects are 1,2,3 ,... , 10 .
we then added to each entry noise from a uniform distribution over the interval [ - 0.5,0.5 ] to each entry .
figure 1 shows the iqroq plots for the row and column effects for this artificial data set .
in both cases , we have a clear maximum at lambda = 1 .
so the iqroq plots propose to apply the power transformation with lambda = 1 which is the identity transformation and leaves the contingency table as it is. the character of the iqroq plots for the row and column effects is similar , but the values differ by a factor 10 .
this is in complete accordance with the way the artificial data set had been generated .
the row effects were chosen 10 times as large as the column effects .
figure 1 iqroq plot for the row ( left ) and column effects ( right ) for the artificial example data set .
as a second artificial example we consider the same contingency table , but apply the exponential function to each of its entries .
the iqroq plots shown in figure 2 have their maximum at lambda = 0 and therefore suggest to use the logarithmic transformation before applying median polish .
so this power transformation reverses the exponential function and we retrieve the original data which were generated by the additive model .
figure 2 iqroq plot for the row ( left ) and column effects ( right ) for the exponential artificial example data set .
the last artificial example is a negative example in the sense that there is no additive model underlying the data generating process .
the entries in the corresponding 10 x 10 contingency table were produced by a normal distribution with expected value 5 and variance 1 .
the iqroq plots are shown in figure 3 .
the iqroq plot for the row effect has no clear maximum at all and shows a tendency to increase with increasing lambda .
the iqroq plot for the column effect has a maximum at 0 and then seems to oscillate with definitely more than one local maximum .
there is no clear winner among the power transformations. and this due to the fact that there is no underlying additive model for the data and no power transformation will make the data fit to an additive model .
figure 3 iqroq plot for the row ( left ) and column effects ( right ) for a random data set where all entries in the contingency table were generated by a normal distribution with expected value 5 and variance 1 .
examples we now apply the iqroq plots to real data sets .
as a first example , we consider the data set in table 1 .
the iqroq plots are shown in figure 4 .
the iqroq plot for the row effects has its global maximum at lambda = 0 and a local maximum at lambda = 0.5 .
the iqroq plot for the column effects has its global maximum at lambda = 0.5 .
however , we know that in this data set the columns correspond to replicates and it does not make sense to maximise the effects of the replicates over the residuals .
the iqroq values for the column effects are also much smaller than the iqroq values for the row effects .
therefore , we chose the power transformation suggested by the iqroq plot for the row effects , i.e. the logarithmic transformation induced by lambda = 0 .
the second choice would be the power transformation with lambda = 0.5 which would lead to similar effects as the logarithmic transformation , although not so strong .
figure 4 iqroq plot for the row ( left ) and column effects ( right ) for the data set in table 1 .
table 3 shows the result of median polish after the logarithmic transformation has been applied to the data in table 1 .
we compare this table with table 2 which originated from median polish applied to the original data .
in table 3 based on the optimal transformation derived from the iqroq plots , the absolute values of all residuals are smaller than any of the ( absolute ) row effects .
there is no indication of extreme outliers anymore , whereas the median polish in table 2 applied to the original data suggests that there are some extreme outliers .
the entries for g6 for replicate r2 and r3 and even the entry for g5 for replicate r2 show a larger absolute value of the majority of the row effects in table 2. from table 2 , it is also not very clear whether group g4 is similar to groups g1 , g2 , g3 or groups g5 , g6 , whereas after the transformation in table 1 the original groupings g1 , g2 , g3 ( no reporter plasmid ) versus of g4 , g5 , g6 ( with increasing amount of reporter plasmid ) can be easily identified based on the row effects .
table 3 median polish for the data in table 1 after power transformation with lambda = 0 overall : 4.2770 r1 r2 r3 row effect g1 0.0000 0.2422 - 0.0497 - 2.4223 g2 0.1760 0.0017 - 0.1331 - 2.2614 g3 - 0.0194 0.4106 0.0000 - 2.5331 g4 0.1632 - 0.0017 0.0000 2.2614 g5 0.0000 - 0.1497 0.0343 4.1149 g6 0.0000 - 0.1241 0.3579 6.1226 column effect 0.0000 - 0.0051 0.0000 we finally consider two larger contingency tables with 14 rows and 97 columns that are far too large to be included in this paper .
the tables consist of a data set displaying the metabolic profile of a bacterial strain after isolation from different tissues of a mouse .
the columns reflect the various substrates whereas the rows consist of repetitions for the isolates from tumor and spleen tissue .
the aim of the analysis is to identify those substrates that can be utilized by active enzymes and to find differences in the metabolic profile after growth in different organs .
the corresponding iqroq plots are shown in figures 5 and 6 .
the iqroq plots indicate that we choose a value of around lambda = 0.5 , although the iqroq plots do not agree on exactly the same value .
figure 5 iqroq plot for the row ( left ) and column effects ( right ) for a larger contingency table for spleen .
figure 6 iqroq plot for the row ( left ) and column effects ( right ) for a larger contingency table for tumour .
the non - additive model in the previous setting , we have looked at the median polish results for the transformed data .
sometimes , transformations of the attributes might not be desired , since the transformed attribute might not be interpretable for the domain expert anymore .
therefore , we introduce transformations that can be reversed leading to median polish on the original attributes based on non - additive models .
in order to motivate and explain this idea , we take a closer look at the power transformation with lambda = 0 , i.e. we when choose the logarithm for the power transformation .
we then obtain the following model .
1 transforming back to the original data yields the model so it is in principle a multiplicative model ( instead of an additive model as in standard median polish ) as follows : where , , , . the part of the model which is not so nice is that the residuals also enter the equation by multiplication .
normally , residuals are always additive , no matter what the underlying model for the approximation of the data is. towards overcoming this drawback , we propose the following approach .
we apply the median polish algorithm to the log - transformed data in order to compute g ( or ) , ri ( or ) and cj ( or ) .
the residuals are then defined at the very end as 2 let us now rewrite eq .
( 1 ) in the following form : assuming that the residuals are small , we have transforming this back to the original data , we obtain a natural question that arises now is the following : what happens with other power transformations , i.e. , forlambda > 0 ?
in principle the same , as we obtain 3 let us denote by (+) lambda the corresponding , possibly associative , operator obtained as follows : 4 now , we can interpret eq .
( 3 ) as 5 thus the problem of determining a suitable transformation of the data before applying the median polish algorithm essentialy boils down to finding that operator (+) lambda which minimises the residuals in ( 2 ) , viz. , transformations and additive generators of fuzzy logic connectives it is very interesting to note the similarity between the operator (+) lambda and t - norms / t - conorms [ 13 ] , operators for modelling the and , respectively the or operator in fuzzy logic .
on the one hand , the above family of power transformations closely resembles the schweizer - sklar family of additive generatorsa of t - norms .
in fact , the power transformations are nothing but the negative of the additive generator of the schweizer - sklar t - norms .
note that additive generators of t - norms are non - increasing , and in the case of continuous t - norms they are strictly decreasing , which explains the need for a negative sign to make the function decreasing .
on the other hand , given continuous and strict additive generators , one constructs t - norms / t - conorms precisely by using eq .
( 4 ) .
however , it should be emphasised that additive generators of t - norms or t - conorms cannot be directly used here .
the additive generator of a t - norm is non - increasing while one requires a transformation to maintain the monotonicity in the arguments .
in the case of the additive generator of a t - conorm , though monotonicity can be ensured , their domain is restricted to just [ 0,1 ] .
this can be partially overcome by normalising the data to fall in this range .
however , this type of normalisation may not be reasonable always .
further , the median polish algorithm applied to the transformed data do not always remain positive and hence determining the inverse with the original generator is not possible .
the above discussion leads us to consider a suitable modification of the additive generators of t - norms / t - conorms that can accommodate a far larger range of values both in their domain and co - domain .
representable uninorms are another class of fuzzy logic connectives that are obtained by the additive generators of both a t - norm and a t - conorm .
in this work , we construct newer transformations by suitably modifying the underlying generators of these representable uninorms [ 13 ] .
modified additive generators of uninorms : an example let us assume that the data x are coming from the interval ( - m,m ) .
consider the following modified generator of the uninorm obtained from the additive generators of the schweizer - sklar family of t - norms and t - conorms .
let ein ( - m,m ) be any arbitrary value .
then the following is a valid transformation with , for all lambda != 0 .
note that hlambda is monotonic for all lambda != 0 and increases with decreasing lambda. that this modified generator is a reasonable transformation can be seen by applying it to the random data set that was already used to generate the iqroq plots in figure 1. from the iqroq plots for this data given in figure 7 , it can be seen that the global maxima occur at lambda = 1 .
so the iqroq plots propose to apply the above transformation with lambda = 1 which is a linear transformation of the data .
figure 7 iqroq plots for the column and row effects of the artificial data with modified schweizer - sklar generator .
( a ) artificial data , e = 5,l = 110 , iqroq column plot .
( b ) artificial data , e = 5,l = 110 , iqroq row plot .
a novel way of finding a suitable transformation in this section we present the algorithm to find a suitable transformation of the given data such that the mp algorithm performs well to elucidate the underlying structures in the data .
we only consider a one parameter family of operators with the parameter denoted by lambda .
the proposed algorithm is as follows .
let (+) lambda denote the one parameter family of operators whose domain and range allow it to be operated on the data given in the contingency table .
then for each lambda the following steps are performed : apply the transformation (+) lambda to the contingency table .
apply median polish to the transformed data to find the overall , row and column effects , viz. , for each i,j. find the residuals for each i,j. determine the iqroq values of the above residuals .
finally , we plot lambda versus the above iqroq values to get the iqroq plots for the column and row effects .
clearly , the operator corresponding to the lambda at which the above iqroq plots peak is a plausible transformation for the given contingency table .
some illustrative examples as an example with real world data , let us consider the data given in the contingency table 4 .
applying the above algorithm with the transformation hlambda we obtain the following iqroq plots as detailed above .
the corresponding iqroq plots are shown in figures 8 ( a ) and ( b ) .
the iqroq plots suggest a value of around lambda = - 0.5 .
the ' median polished ' contingency table for lambda = - 0.5 is given in table 5 .
table 4 infant mortality vs educational qualification of the parents in deaths per 1000 live births in the years 1964 - 1966 ( source : u.s. dept. of health , education and welfare ) <= 8 9 - 11 12 13 - 15 >= 16 north - west 25.3 25.3 18.2 18.3 16.3 north - central 32.1 29.0 18.8 24.3 19.0 south 38.8 31.0 19.3 15.7 16.8 west 25.4 21.1 20.3 24.0 17.5 figure 8 iqroq plots for the column and row effects of the infant mortality data .
( a ) e = 2,m = 40 , iqroq column plot .
( b ) e = 2,m = 40 , iqroq row plot .
table 5 median polish on the h lambda - transformed infant mortality data with lambda = - 0 . 5 overall : 0.2919985 <= 8 9 - 11 12 13 - 15 >= 16 re nw 0.00025312 0.0027983 - 0.00025004 - 0.010879 0.0000000 - 0.010113225 nc - 0.00025312 - 0.0027983 - 0.01200293 0.010879 0.0078014 0.006694490 s 0.01098492 0.0091121 0.00025004 - 0.044525 - 0.0035433 - 0.001558958 w - 0.01102793 - 0.0305895 0.00456985 0.014641 0.0000000 0.001558958 ce 0.0318984143 0.0293532152 - 0.0112376220 0.0002531186 - 0.0294192135 we can also visualise the non - linear aggregation operator (+) lambda ( here : lambda = - 0.5 ) that is used for the non - additive median polish model .
the non - linearity is clearly illustrated in figure 9 which suggests that strong row and column effects seem to even amplify each other .
figure 9 the operator for the non - additive median polish model for the infant mortality data .
we also apply the non - additive median polish model to the data set that was already used for figure 5 .
the corresponding iqroq plots are shown in figures 10 ( a ) and ( b ) .
the iqroq plots indicate that we choose a value of around lambda = 0.4 .
figure 10 iqroq plots for the column and row effects of the spleen data .
( a ) e = 10,m = 20000 , iqroq column plot .
( b ) e = 10,m = 20000 , iqroq row plot .
an example based on clinical data we consider a data set from [ 14 ] containing a sample of male residents of framingham in massachusetts shown in table 6 .
table 6 median polish for the data in table 7 overall : 3.625 c1 c2 c3 c4 c5 c6 c7 row effect p1 absent 33.000 5.000 - 1.500 - 0.125 - 2.625 3.625 - 5.625 13.125 p2 absent 40.500 1.500 0.000 33.375 - 0.125 - 7.875 - 6.125 20.625 p3 absent 22.750 1.750 - 13.750 44.625 0.125 - 9.625 - 8.875 29.375 p4 absent 36.500 - 6.500 0.000 45.375 - 6.125 5.125 - 7.125 22.625 p5 absent 17.000 0.000 - 10.500 5.875 0.375 - 3.375 - 1.625 13.125 p6 absent 0.000 0.000 0.5000 - 0.125 0.375 - 0.375 - 2.625 7.125 p7 absent 8.750 - 5.250 0.250 10.625 - 7.875 - 1.625 0.125 7.375 p8 absent - 2.750 - 3.750 - 2.250 0.125 0.625 - 0.125 0.625 1.875 p1 present 0.500 0.500 0.000 - 3.625 - 0.125 0.125 - 0.125 - 3.375 p2 present - 2.500 1.500 0.000 3.375 - 1.125 3.125 - 0.125 - 2.375 p3 present 0.000 0.000 1.500 - 2.125 - 0.625 0.625 3.375 - 2.875 p4 present - 1.750 - 0.750 - 1.250 1.125 1.625 - 0.125 2.625 - 2.125 p5 present 0.000 0.000 - 0.500 - 1.125 1.375 2.625 0.375 - 2.875 p6 present - 0.500 0.500 0.000 - 2.625 - 0.125 1.125 3.875 - 3.375 p7 present 0.000 - 1.000 - 1.500 - 3.125 0.375 3.625 1.375 - 1.875 p8 present - 1.000 0.000 2.500 - 3.125 0.375 - 0.375 0.375 - 2.875 column effect 1.250 - 0.750 - 0.250 3.375 - 0.125 0.625 - 0.125 the age of the persons ranges between 40 and 59 year .
several attributes were taken into accout , among them blood pressure and cholesterol level .
the persons were classified whether they developed a coronary heart disease within a period of six years .
the blood pressure was divided into eight levels , p1 referring to the lowest level (< 117 ) , p2 to a blood pressure between 117 and 126 etc. and p8 corresponding to blood pressures above 186 .
similar to the blood pressure , the cholesterol level was divided into seven groups ( c1 : < 200 , c2 : 200 - 209 , c3 : 210 - 219 , c4 : 220 - 244 , c5 : 245 - 259 , c6 : 260 - 284 , c7 : > 284 ) .
as one would expect from such a study , the number of observed cases with coronary disease within this six year period is relatively small compared to number of persons not being classified as having a coronary disease .
this makes it quite difficult to see what would be expected , namely that a high level of cholesterol and high blood pressure increase the risk of coronary disease .
table 7 shows the result of applying median polish without any transformation to table 6 .
this table contains large residuals , the largest absolute residual of 45.375 at ( p4 absent,c3 ) exceeds by far the row and column effects .
the absolute values of the residuals also exhibit a large variation .
the relative variance of the absolute residuals is 18.192 .
the principal expected effects can be guessed from the median polish result , but could be doubted due to the large residuals compared to the row and column effects .
it is obvious to expect a positive row effect for the first eight rows , i.e. for the persons who did not show any signs of heart disease , simply because this group of persons forms the large majority in the table .
we would also expect that this positive effect is smaller for larger levels of the blood pressure .
this can be observed , but these effects do not look significant compared to the large residuals .
the column effects , i.e. the cholesterol levels , seem to have a small influence .
none of the column effects is larger than the mean ( 4.504 ) of the absolute residuals , all column effects are even smaller than the median ( 1.438 ) of the absolute residuals .
table 7 coronary disease data from [[ 14 ]] cholesterol level heart disease pressure c1 c2 c3 c4 c5 c6 c7 absent p1 51 21 15 20 14 21 11 absent p2 66 25 24 61 24 17 18 absent p3 57 34 19 81 33 24 24 absent p4 64 19 26 75 20 32 19 absent p5 35 16 6 26 17 14 15 absent p6 12 10 11 14 11 11 8 absent p7 21 5 11 25 3 10 11 absent p8 4 1 3 9 6 6 6 present p1 2 0 0 0 0 1 0 present p2 0 2 1 8 0 5 1 present p3 2 0 2 2 0 2 4 present p4 1 0 0 6 3 2 4 present p5 2 0 0 3 2 4 1 present p6 1 0 0 1 0 2 4 present p7 3 0 0 2 2 6 3 present p8 1 0 3 1 1 1 1 since we have zero values in the table , we cannot apply the logarithmic power transformation to the data .
in order to avoid this problem , we apply laplace correction , i.e. we add a positive constant , say 1 , to all entries in the table .
the iqroq plots for the laplace corrected data set , shown in figure 11 , indicate that a value for lambda around 0.4 yields the most suitable power transformation .
figure 11 iqroq plot for the row ( left ) and column effects ( right ) for the data in table 7 .
table 8 shows the result of median polish applied to the transformed data .
the residuals are now smaller compared to the row and column effects .
the largest absolute residual is 3.756 at ( p1 absent,c1 ) .
even this largest residual is smaller than three of the row effects which can then be considered significant .
also the relative variance of the absolute values of the residuals is much smaller now .
it is only 0.897 .
now there is also one column effect which is larger than the mean ( 0.787 ) of the absolute residuals and two column effects are larger than the median ( 0.611 ) of the absolute residuals .
table 8 median polish for the data in table 7 after power transformation with lambda = 0 . 4 overall : 1.756 c1 c2 c3 c4 c5 c6 c7 row effect p1 absent 3.756 1.130 0.000 - 0.520 - 0.243 0.301 - 0.849 3.423 p2 absent 3.570 0.244 0.000 2.578 - 0.050 - 1.844 - 0.967 4.904 p3 absent 1.731 0.320 - 1.859 3.034 0.050 - 1.815 - 1.111 5.990 p4 absent 3.125 - 0.956 0.000 3.401 - 0.943 0.052 - 1.082 5.187 p5 absent 1.829 0.021 - 2.400 0.108 0.050 - 1.188 - 0.291 3.689 p6 absent 0.000 0.459 0.589 - 0.170 0.539 - 0.139 - 0.170 2.010 p7 absent 1.094 - 1.485 0.050 1.107 - 2.402 - 0.910 0.025 2.549 p8 absent - 0.818 - 1.368 - 0.415 0.121 0.627 - 0.052 0.652 0.612 p1 present 0.570 0.101 0.000 - 1.390 - 0.050 0.070 - 0.025 - 1.656 p2 present - 1.608 0.682 0.000 1.331 - 0.849 1.092 - 0.025 - 0.857 p3 present 0.000 - 0.470 0.809 - 0.581 - 0.620 0.080 1.664 - 1.085 p4 present - 0.713 - 0.602 - 0.703 0.851 1.100 - 0.052 1.531 - 0.953 p5 present 0.000 - 0.470 - 0.570 - 0.108 0.759 0.960 0.203 - 1.085 p6 present - 0.010 0.101 0.000 - 0.592 - 0.050 0.651 2.234 - 1.656 p7 present 0.000 - 0.943 - 1.044 - 1.054 0.286 1.172 0.784 - 0.612 p8 present - 0.132 - 0.021 1.731 - 0.714 0.627 - 0.052 0.652 - 1.534 column effect 0.709 - 0.201 - 0.100 1.291 - 0.050 0.629 - 0.075 it is also interesting to take a look at the transformed data set that was found based on the iqroq plots .
figure 12 visualises the original ( left ) and the transformed ( right ) contingency table .
both table show a tendency of higher values in the upper half ( persons with absent heart disease ) .
but the difference between the upper and the lower half is much clearer for the the transformed contingency table than for the original one .
this means that even without applying median polish , it might be useful to look at the transformed contingency table generated by the transformation derived from the iqroq plots .
figure 12 heatmap visualisation of the data from table 7 ( left ) and the data after transformation ( right ) .
conclusions we have proposed two methods to improve the results of median .
either we apply a suitable power transformation to the data before applying median polish .
based on the iqroq plots , the most suitable power transformation can be chosen. or , as an alternative , one can apply reversible transformations based on additive generators , leading to non - additive median polish .
again , the most suitable reversible transformation is chosen based on iqroq plots .
the joint non - linear connection of column and row effects can be visualsied by a function in two variables in order to better understand the nature of the interaction of column and row effects .
the example on heart disease has demonstrated that it can be useful to apply a transformation derived from iqroq plots , even if it is not necessarily intended to use median polish afterwards .
the transformed contingency table might already exhibit a clearer structure than the original table .
ethical approval all data sets referred to in this manuscript have been published before and were in compliance with the helsinki declaration .
no specific or additional experiments were carried out for this manuscript .
software the iqroq plots in this paper were generated by an implementation of the described method in r , a free software environment for statistical computing and graphics [ 15 ] ( see http :// www.r - project.org /) .
the simple r implementation for generating iqroq plots can be downloaded at http :// public.ostfalia.de /~ klawonn / hiss _ mp.r. endnote a an additive generator of a function f ( x,y ) in two real variables is a function h in one real variable such that f ( x,y ) = h - 1 ( h ( x ) + h ( y )) holds .
competing interests the authors declare that they have no competing interests .
authors' contributions fk and bj developed the theoretical background , implemented the methods and processed the data .
kc , ak and fp provided data sets and helped in the interpretation of the analysis results .
all authors read and approved the final manuscript .
acknowledgements this study was co - financed by the european union ( european regional development fund ) under the regional competitiveness and employment objective and within the framework of the bi2 son project einsatz von informations - und kommunikationstechnologien zur optimierung der biomedizinischen forschung in sudost - niedersachsen .
this work was partly carried out during the visit of the second author to the department of computer science , ostfalia university of applied sciences under the fellowship provided by the alexander von humboldt foundation .
references 1 .
leek j scharpf r corrado bravo h simcha d langmead b johnson w geman d baggerly k irizarry r tackling the widespread and critical impact of batch effects in high - throughput data nature rev | genet 2010 11 733 739 20838408 2 .
shaffer jp multiple hypothesis testing ann rev psych 1995 46 561 584 10.1146 / annurev.ps.46.020195.003021 3 .
holm s a simple sequentially rejective multiple test procedure scand j stat 1979 6 65 70 4 .
benjamini y hochberg y controlling the false discovery rate : a practical and powerful approach to multiple testing j r stat soc ser b ( methodological ) 1995 57 289 300 5 .
mehta c the exact analysis of contingency tables in medical research stat methods med res 1995 3 153 156 6 .
hoaglin d mosteller f tukey j understanding robust and exploratory data analysis 2000 new york wiley 7 .
klawonn f crull k kukita a pessler f he j liu x krupinski e xu g median polish with power transformations as an alternative for the analysis of contingency tables with patient data health information science : first international conference 2012 berlin springer 25 35 8 .
jayaram b klawonn f kruse r berthold m moewes c gil m grzegorzewski p hryniewicz o generalised median polish based on additive generators synergies of soft computing and statistics for intelligent data analysis 2012 berlin springer 439 448 9 .
enke h elementary analysis of multidimensional contingency tables .
adaptation to a medical example biomed j 1986 28 305 322 10 .
shahpar c li g homicide mortality in the united states , 1935 - 1994 : age , period , and cohort effects am j epidemiol 1999 150 1213 1222 10.1093 / oxfordjournals.aje.a009948 10588082 11 .
selvin s statistical analysis of , epidemiologic data 2004 new york oxford university press 12 .
berthold m borgelt c hoppner f klawonn f guide to , intelligent data analysis : how to intelligently make sense of real data 2010 london springer 13 .
klement e mesiar r pap a triangular norms 2000 dordrecht kluwer 14 .
agresti a categorical data analysis 1990 new york wiley 15 .
r development core team r : a language and , environment for statistical computing 2009 vienna r foundation for statistical computing