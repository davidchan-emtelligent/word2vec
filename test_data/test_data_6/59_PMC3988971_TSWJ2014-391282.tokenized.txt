scientificworldjournal scientificworldjournal tswj the scientific world journal 2356 - 6140 1537 - 744x hindawi publishing corporation 25143974 3988971 10.1155 / 2014 / 391282 research article improved stability criteria of static recurrent neural networks with a time - varying delay http :// orcid.org / 0000 - 0001 - 8352 - 7139 ding lei 1 zeng hong - bing 2 * wang wei 3 yu fei 4 1school of information science and engineering , jishou university , jishou 416000 , china 2school of electrical and information engineering , hunan university of technology , zhuzhou 412007 , china 3hunan railway professional technology college , zhuzhou 412001 , china 4jiangsu provincial key laboratory for computer information processing technology , soochow university , soochow 215006 , china * hong - bing zeng : 9804zhb @ 163.com academic editors : j. shu and z. chen 2014 24 2 2014 2014 391282 17 8 2013 8 1 2014 copyright ( c ) 2014 lei ding et al .
2014 this is an open access article distributed under the creative commons attribution license , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .
this paper investigates the stability of static recurrent neural networks ( srnns ) with a time - varying delay .
based on the complete delay - decomposing approach and quadratic separation framework , a novel lyapunov - krasovskii functional is constructed. by employing a reciprocally convex technique to consider the relationship between the time - varying delay and its varying interval , some improved delay - dependent stability conditions are presented in terms of linear matrix inequalities ( lmis ) .
finally , a numerical example is provided to show the merits and the effectiveness of the proposed methods .
http :// dx.doi.org / 10.13039 / 501100001809 national natural science foundation of china61304064http :// dx.doi.org / 10.13039 / 501100001809 national natural science foundation of china61363073http :// dx.doi.org / 10.13039 / 501100001809 national natural science foundation of china61273157http :// dx.doi.org / 10.13039 / 501100001809 national natural science foundation of china61262032hunan provincialnatural science foundation of china13jj6058research foundation of education bureau of hunan province13a075natural science foundation of hunan university of technology2012hzx06 1 .
introduction during the past decades , recurrent neural network ( rnn ) has been successfully applied in many fields , such as signal processing , pattern classification , associative memory design , and optimization .
therefore , the study of rnn has attracted considerable attention and various issues of neural networks have been investigated ( see , e.g. , [ 1 - 4 ] and the references therein ) .
as the integration and communication delay is unavoidably encountered in implementation of rnn and is often the main source of instability and oscillations , much efforts have been expended on the problem of stability of rnns with time delays ( see , e.g. , [ 5 - 14 ]) .
rnns can be classified as local field networks and static neural networks based on the difference of basic variables ( local field states or neuron states ) [ 15 ] .
recently , the stability of static recurrent neural networks ( srnns ) with time - varying delay was investigated in [ 16 ] , where sufficient conditions were obtained guaranteeing the global asymptotic stability of the neural network .
nevertheless , some negative semi - definite terms were ignored in [ 16 ] , which lead to the conservatism of the derived result. by retaining these terms and considering the low bound of the delay , some improved stability conditions were derived for srnns with interval time - varying delay in [ 17 ] .
in [ 18 ] , an input - output framework was proposed to investigate the stability of srnns with linear fractional uncertainties and delays .
based on the augmented lyapunov - krasovskii functional approach , some new conditions were derived to assure the stability of srnns in [ 19 - 22 ] , but the results can be further improved .
in this paper , the problem of stability of srnns with time - varying delay is investigated based on the complete delay - decomposing approach [ 12 ]. by employing a reciprocally convex technique , some sufficient conditions are derived in the forms of linear matrix inequalities ( lmis ) .
the effectiveness and the merit are illustrated by a numerical example. notations. through this paper , n t and n - 1 stand for the transpose and the inverse of the matrix n , respectively ; p > 0 ( p >= 0 ) means that the matrix p is symmetric and positive definite ( semipositive definite ) ; rn denotes the n - dimensional euclidean space ; diag {...} denotes a block - diagonal matrix ; || z || is the euclidean norm of z ; the symbol * within a matrix represents the symmetric terms of the matrix ; for example , [ xy * z ]=[ xyytz ] .
matrices , if not explicitly stated , are assumed to have compatible dimensions .
2. system description consider the following delayed neural network : ( 1 ) x. ( t )= - ax ( t )+ f ( wx ( t - tau ( t ))+ j ) , x ( t )= phi ( t ) , - tau <= t <= 0 , where x ( t ) = [ x 1 ( t ) , x 2 ( t ),... , x n ( t )] t in rn and j = [ j 1 , j 2 ,... , j n ] t in rn denote the neuron state vector and the input vector , respectively ; f (.)
= [ f 1 (.) , f 2 (.),... , f n (.)] t in rn is the neuron activation function ; phi ( t ) is the initial condition ; a = diag ( a 1 , a 2 ,... , a n ) > 0 and w are known interconnection weight matrices ; and tau ( t ) is the time - varying delay and satisfies ( 2 ) 0 <= tau ( t )<= tau , ( 3 ) tau. ( t )<= mu. furthermore , the neuron activation functions satisfy the following assumption .
assumption 1 the neuron activation functions are bounded and satisfy ( 4 ) 0 <= fi ( alpha1 ) - fi ( alpha2 ) alpha1 - alpha2 <= li , for - allalpha1,alpha2inr , where l i >= 0 for i in 1,2 ,... , n. for simplicity , denote l = diag ( l 1 , l 2 ,... , l n ) .
under assumption 1 , there exists an equilibrium x * of ( 1 ) .
hence , by the transformation z * = x (.)
- x * , ( 1 ) can be transformed into ( 5 ) z. ( t )= - az ( t )+ g ( wz ( t - tau ( t ))) , z ( t )= psi ( t ) , - tau <= t <= 0 , where z ( t ) = [ z 1 ( t ) , z 2 ( t ),... , z n ( t )] t is the state vector ; psi ( t ) = phi ( t ) - x * is the initial condition ; and the transformed neuron activation functions g ( wz (.))
= f ( wz (.) + wx * + j ) - f ( wx * + j ) satisfies ( 6 ) 0 <= gi ( alpha ) alpha <= li , for - allalpha != 0 ; gi ( 0 )= 0 , iin1,2 ,... , n. notice that there exists an equilibrium point z ( t ) = 0 in neural network ( 5 ) , corresponding to the initial condition psi ( t ) = 0 .
based on the analysis above , the problem of analyzing the stability of system ( 1 ) at equilibrium is changed into a problem of analyzing the zero stability of system ( 5 ) .
before presenting our main results , we first introduce two lemmas , which are useful in the stability analysis of the considered neural network .
lemma 2 ( see [ 23 ]) let m = m t > 0 be a constant real n x n matrix , and suppose x. :[ - h,0 ]| - > rn with h > 0 such that the subsequent integration is well defined .
then , one has ( 7 ) - hintegralt - htx.t ( s ) mx. ( s ) ds <= zetat ( t )[ - mm * - m ] zeta ( t ) , where zeta ( t ) = col { x ( t ) , x ( t - h )} .
lemma 3 ( see [ 24 ]) let h 1 , h 2 ,... , h n : rn | - > r be given finite functions , and they have positive values for arbitrary value of independent variable in an open subset m of rn. the reciprocally convex combination of h i ( i = 1,2 ,... , n ) in m satisfies ( 8 ) min sumi = 1n1lambdaihi ( t )= sumi = 1nhi ( t )+ max sumi = 1nsumj = 1,j != ingi,j ( t ) subject to ( 9 ) { lambdai > 0 , sumi = 1nlambdai = 1 , gi,j ( t ) : rn | - > r , gj,i ( t )= gi,j ( t ) , [ hi ( t ) gi,j ( t ) gi,j ( t ) hj ( t )]>= 0 } .
3. main results in the sequel , following the method proposed in [ 13 ] , we decompose the delay interval [ 0,tau ] into m equidistant subintervals , where m is a given integer ; that is , [ 0,tau ]= unionj = 1m [( j - 1 ) delta,jdelta ] with delta = tau / m. thus , for any t >= 0 , there should exist an integer k in { 1,2 ,... , m } , such that tau ( t ) in [( k - 1 ) delta , kdelta ] .
then the lyapunov - krasovskii functional candidate is chosen as ( 10 ) v ( zt )| k := v ( zt )| tau ( t ) in [( k - 1 ) delta,kdelta ] , v ( zt )| k = v1 ( zt )+ v2 ( zt )+ v3 ( zt )+ v4 ( zt )+ v5 ( zt ) with ( 11 ) v1 ( zt )= zt ( t ) pz ( t )+ 2sumi = 1ndiintegral0wiz ( t ) gi ( alpha ) dalpha,v2 ( zt )= integralt - deltatzeta1t ( s ) razeta1 ( s ) ds,v3 ( zt )= sumj = 1mdeltaintegral - jdelta - ( j - 1 ) deltaintegralt + thetatz.t ( s ) zjz. ( s ) ds dtheta,v4 ( zt )= sumj = 1k - 1integralt - jdeltat - ( j - 1 ) deltazeta2t ( s ) qjzeta2 ( s ) ds + integralt - tau ( t ) t - ( k - 1 ) deltazeta2t ( s ) qkzeta2 ( s ) ds,v5 ( zt )= sumj = 1mintegralt - jdeltat - ( j - 1 ) deltagt ( wz ( s )) mjg ( wz ( s )) ds , where ( 12 ) p > 0 , d = diag { d1,d2 ,... , dn }>= 0,ra =[ r11r12 ... r1m * r22 ... r2m **......*** rmm ]> 0 , qj =[ qjxj * yj ]>= 0,zj > 0 , mj > 0 , j = 1,2 ,... , m , are to be determined , zeta1 ( s )=[ zt ( s ) zt ( s - delta )... zt ( s - ( m - 1 ) delta )] t , zeta2 ( s )=[ zt ( s ) gt ( wz ( s ))] t , and w i denotes the ith row of matrix w. remark 4 notice that a novel term v 4 ( x t ) being continuous at tau ( t ) = tau k is included in the lyapunov - krasovskii functional ( 10 ) , which plays an important role in reducing conservativeness of the derived result .
next , we develop some new delay - dependent stability criteria for the delayed neural networks described by ( 5 ) and ( 6 ) with tau ( t ) satisfying ( 2 ) and ( 3 ). by employing the lyapunov - krasovskii functional ( 10 ) , the following theorem is obtained .
theorem 5 for a given positive integer m , scalars tau > 0 and mu , the origin of system ( 5 ) with the activation function satisfying ( 6 ) and a time - varying delay satisfying conditions ( 3 ) is globally asymptotically stable if there exist ( 13 ) p > 0 , ra =[ r11r12 ... r1m * r22 ... r2m **......*** rmm ]> 0,qj =[ qjxj * yj ]>= 0 , zj > 0 , mj > 0 , j = 1,2 ,... , m , and d = diag { d 1 , d 2 ,... , d n } >= 0 , t 1 = diag { t 11 , t 12 ,... , t 1n } >= 0 , t 2 = diag { t 21 , t 22 ,... , t 2n } >= 0 , and g j , j = 1,2 ,... , m , with appropriate dimensions , such that , for k = 1,2 ,... , m , ( 14 ) [ omega11 ( k ) omega12 ( k ) delta1gamma1tz * omega22 ( k ) delta1gamma2tz ** - z ]< 0 , ( 15 ) [ zkgk * zk ]> 0 , where ( 16 ) omega11 ( k )= phi11 + phi 11 ( k )+ lambda1 ( k ) , omega12 ( k )= phi12 + lambda2 ( k ) , omega22 ( k )= phi22 + lambda3 ( k ) , phi11 =[ 0000 ... 00 * phi11phi12phi13 ... phi1m0 ** phi22phi23 ... phi2m - r1m *** phi33 .........****... phi ( m - 1 ) m - r ( m - 2 ) m ****... phimmzm - r ( m - 1 ) m ****...* - zm - rmm ] , phi12 =[ wtlt200 ... 0pw00 ... 0000 ... 0 ............... 000 ... 0 ] , phi22 =[ beta1wtd ... 00 * beta2 ... 00 ...............**... betam + 10 **...* betam + 2 ] , phi 11 ( k )=( psi ij ( k ))( m + 2 ) x ( m + 2 )+( psi ij ( k ))( m + 2 ) x ( m + 2 ) t,lambda1 ( k )= diag { lambda11 ( k ) , lambda22 ( k ),... , lambda2 ( m + 2 )( k )} , lambda2 ( k )= diag { lambda21 ( k ) , lambda12 ( k ),... , lambda1 ( m + 2 )( k )} , lambda3 ( k )= diag { lambda31 ( k ) , lambda32 ( k ),... , lambda3 ( m + 2 )( k )} , gamma1 =[ 0 - a0 ... 0 ] , gamma2 =[ i0 ... 0 ] , z = sumj = 1mzj with ( 17 ) phiij ={ pa + atp + r11 - z1,i = j = 1,r12 + z1,i = 1 , j = 2,rij,i = 1 , 3 <= j <= m,rij - r ( i - 1 )( j - 1 ) - zi - zi - 1,i = j = 2,3 ,... , m,rij - r ( i - 1 )( j - 1 )+ zi,2 <= i <= m - 1 , j = i + 1,rij - r ( i - 1 )( j - 1 ) , otherwise,betaj ={ - 2t2,j = 1 , - 2t1 + m1,j = 2,mj - 1 - mj - 2,3 <= j <= m + 1 , - mm,j = m + 2,psi ij ( k )={ - zk + gk,i = j = 1,zk - gkt,i = 1,j = k + 1,zk - gk,i = 1,j = k + 2 , - zk + gk,i = k + 1,j = k + 2,0,otherwise,lambda1j ( k )={ - ( 1 - mu ) qk,j = 1,q1,j = 2,qj - 1 - qj - 2,3 <= j <= k + 1,0,otherwise,lambda2j ( k )={ - ( 1 - mu ) xk,j = 1,x1 + wtlt1 - atwtd,j = 2,xj - 1 - xj - 2,3 <= j <= k + 1,0,otherwise,lambda3j ( k )={ - ( 1 - mu ) yk,j = 1,y1,j = 2,yj - 1 - yj - 2,3 <= j <= k + 1,0,otherwise .
proof from assumption 1 , it can be deduced that , for any diagonal matrices t i >= 0 , i = 1,2 , ( 18 ) 0 <= 2gt ( wz ( t )) t1 [ lwz ( t ) - g ( wz ( t ))] , 0 <= 2gt ( wz ( t - tau ( t ))) xt2 [ lwz ( t - tau ( t )) - g ( wz ( t - tau ( t )))] .
now , calculating the derivative of v ( z t )| k along the solutions of neural network ( 5 ) yields ( 19 ) v. ( zt )| k = v.1 ( zt )+ v.2 ( zt )+ v.3 ( zt )+ v.4 ( zt ) , where ( 20 ) v.1 ( zt )= 2zt ( t ) pz. ( t )+ 2gt ( wz ( t )) dwz. ( t ) , v.2 ( zt )= zeta1t ( t ) razeta1 ( t ) - zeta1t ( t - delta ) razeta1 ( t - delta ) , v.3 ( zt )= sumj = 1m { delta2z.t ( t ) zjz. ( t ) - deltaintegralt - jdeltat - ( j - 1 ) deltaz.t ( s ) zjz. ( s ) ds } , v.4 ( zt )= sumj = 1k - 1xit ( t - jdelta )( qj + 1 - qj ) xi ( t - jdelta )+ xit ( t ) q1xi ( t ) - ( 1 - tau. ( t )) xit ( t - tau ( t )) qkxi ( t - tau ( t ))<= sumj = 1k - 1xit ( t - jdelta )( qj + 1 - qj ) xi ( t - jdelta ) xit ( t ) q1xi ( t ) - ( 1 - mu ) xit ( t - tau ( t )) qkxi ( t - tau ( t )) , v.5 ( zt )= sumj = 1m - 1gt ( wz ( t - jdelta ))( mj + 1 - mj ) g ( wz ( t - jdelta ))+ gt ( wz ( t )) m1g ( wz ( t )) - gt ( wz ( t - tau )) mmg ( wz ( t - tau )). by lemmas 2 and 3 , it can be deduced that ( 21 ) - deltaintegralt - kdeltat - ( k - 1 ) deltax.t ( s ) zkx. ( s ) ds = - deltaintegralt - tau ( t ) t - ( k - 1 ) deltax.t ( s ) zkx. ( s ) ds - deltaintegralt - kdeltat - tau ( t ) x.t ( s ) zkx. ( s ) ds <= - deltatau ( t ) - ( k - 1 ) delta [ x ( t - ( k - 1 ) delta ) x ( t - tau ( t ))] t [ zk - zk * zk ] x [ x ( t - ( k - 1 ) delta ) x ( t - tau ( t ))] - deltakdelta - tau ( t )[ x ( t - tau ( t )) x ( t - kdelta )] t [ zk - zk * zk ][ x ( t - tau ( t )) x ( t - kdelta )] <= - [ x ( t - ( k - 1 ) delta ) - x ( t - tau ( t )) x ( t - tau ( t )) - x ( t - kdelta )] t [ zkgk * zk ] x [ x ( t - ( k - 1 ) delta ) - x ( t - tau ( t )) x ( t - tau ( t )) - x ( t - kdelta )] = xit ( t )[ - 2zk + gk + gktzk - gktzk - gk * - zkgk ** - zk ] xi ( t ) , where xi t = [ x ( t - tau ( t )) t x ( t - ( k - 1 ) delta ) t x ( t - kdelta ) t ] .
next , we introduce a new vector as ( 22 ) zeta ( t )=[ zeta1t ( t ) zeta2t ( t )] t , where ( 23 ) zeta1 ( t )=[ z ( t - tau ( t )) z ( t ) z ( t - delta ) z ( t - 2delta )... z ( t - mdelta )] , zeta2 ( t )=[ g ( wz ( t - tau ( t ))) g ( wz ( t )) g ( wz ( t - delta )) g ( wz ( t - 2delta ))... g ( wz ( t - mdelta ))] .
then , rewrite system ( 5 ) as ( 24 ) z. ( t )=[ gamma1gamma2 ] zeta ( t ) .
adding the right sides of ( 18 ) to ( 19 ) and applying ( 21 ) yield ( 25 ) v. ( zt )| k = zeta ( t ) t ( t )[ omega ( k )+ delta2gammatsumj = 1mzjgamma ] zeta ( t ) , where ( 26 ) omega ( k )=[ omega11 ( k ) omega12 ( k )* omega22 ( k )] , gamma =[ gamma1gamma2 ] .
for all k = 1 ,... , m , if omega ( k ) + delta 2gammatsumj = 1 m z jgamma < 0 , which is equivalent to lmis ( 14 ) in the sense of schur complement [ 25 ] , then v. ( zt )| k < 0 for any zeta ( t ) != 0 .
note that v ( z t ) is continuous at tau ( t ) = tau k , so the system ( 5 ) is globally asymptotically stable .
this completes the proof .
remark 6 in the proof of theorem 5 , tau ( t ) - ( k - 1 ) delta and kdelta - tau ( t ) are not simply enlarged to delta as [ 16 ] does. by employing reciprocally convex approach to consider this information , theorem 5 may be less conservative , which will be verified by the simulation results in the next section .
remark in previous works such as [ 16 , 19 ] , considerable attention has been paid to the case that the derivative of the time - varying delay tau. ( t ) satisfies ( 3 ) .
however , in the case of tau. ( t ) satifying ( 27 ) tau. ( t )<= muk , tau ( t ) in [ tauk - 1,tauk ] , k = 1,2 ,... , m , the treatment in [ 16 , 19 ] means that tau. ( t ) in ( 27 ) is enlarged to tau. ( t )<= mu = max { mu1,mu2 ,... , mum } , which may lead to conservativeness inevitably. by contrast , the case above can be taken fully into account by replacing mu with mu k in theorem 5 .
for the case that the time - varying delay tau ( t ) is nondifferentiable or tau. ( t ) is unknown , setting qj = 0 , j = 1,2 ,... , m , in theorem 5 , a delay - dependent and rate - independent criterion is easily derived as follows .
corollary 8 for a given positive integer m , scalars tau > 0 , the origin of system ( 5 ) with the activation function satisfying ( 6 ) and a time - varying delay satisfying condition ( 2 ) is globally asymptotically stable if there exist ( 28 ) p > 0 , ra =[ r11r12 ... r1m * r22 ... r2m **......*** rmm ]> 0,zj > 0 , mj > 0 , j = 1,2 ,... , m,d = diag { d1,d2 ,... , dn }>= 0,t1 = diag { t11,t12 ,... , t1n }>= 0,t2 = diag { t21,t22 ,... , t2n }>= 0,gj , j = 1,2 ,... , m , with appropriate dimensions , such that , for k = 1,2 ,... , m , lmis in ( 15 ) and ( 29 ) hold ( 29 ) [ phi11 + phi 11 ( k ) phi21delta1gamma1tz * phi22delta1gamma2tz ** - z ]< 0 , where phi11 , phi12 , phi22 , phi 11 ( k ) , gamma1 , and gamma2 are defined in theorem 5 .
4. numerical examples in this section , we will provide a numerical example to show the effectiveness of the presented criteria .
example 1 consider neural network ( 1 ) with the following parameters : ( 30 ) a =[ 7.34580006.99870005.5949 ] , w =[ 13.6014 - 2.9616 - 0.69367.473621.68103.21000.7920 - 2.6334 - 20.1300 ] .
the activation functions satisfy ( 6 ) with ( 31 ) l = diag ( 0.3680,0.1795,0.2876 ) .
this example has been discussed in [ 16 - 22 ]. by using theorem 5 and corollary 8 with m = 2 , for various mu , the upper bounds tau that guarantee the global asymptotic stability of neural network ( 1 ) are computed and listed in table 1 .
it can be concluded that the upper bounds obtained by our method are much better than those in [ 16 - 22 ] .
obviously , the conditions proposed in this paper are an improvement over the existing ones .
5. conclusions this paper has studied the stability of srnns by constructing a complete delay - decomposing lyapunov - krasovskii functional .
some improved delay - dependent stability conditions have been derived by utilizing a reciprocally convex technique to consider the relationship between the time - varying delay and its varying interval , which are formulated in linear matrix inequalities ( lmis ) .
finally , a numerical example has been provided to show the effectiveness of the proposed methods .
acknowledgments this work was supported by the national natural science foundation of china ( nos .
61304064 , 61363073 , 61273157 , and 61262032 ) , the hunan provincial natural science foundation of china ( no .
13jj6058 ) , the research foundation of education bureau of hunan province ( no .
13a075 ) , and the natural science foundation of hunan university of technology ( no. 2012hzx06 ) .
conflict of interests the authors declare that there is no conflict of interests regarding the publication of this paper .
1 michel an liu d qualitative analysis and synthesis of recurrent neural networks 2002 new york , ny , usa marcel dekker 2 zhang qj lu xq a recurrent neural network for nonlinear fractional programming mathematical problems in engineering 2012 2012 18 pages 807656 3 goto k shibata k emergence of prediction by reinforcement learning using a recurrent neural network journal of robotics 2010 2010 9 pages 437654 4 ku c - c lee ky diagonal recurrent neural networks for dynamic systems control ieee transactions on neural networks 1995 6 1 144 156 2 - s2.0 - 0029207879 18263294 5 liu y wang z liu x global exponential stability of generalized recurrent neural networks with discrete and distributed delays neural networks 2006 19 5 667 675 2 - s2.0 - 33646511197 16046098 6 li t zheng wx lin c delay - slope - dependent stability results of recurrent neural networks ieee transactions on neural networks 2011 22 12 2138 2143 2 - s2.0 - 83855160856 21984499 7 li t luo q sun c zhang b exponential stability of recurrent neural networks with time - varying discrete and distributed delays nonlinear analysis : real world applications 2009 10 4 2581 2589 2 - s2.0 - 61749087097 8 cao j wang j global asymptotic and robust stability of recurrent neural networks with time delays ieee transactions on circuits and systems i 2005 52 2 417 426 2 - s2.0 - 14644434391 9 li c liao x robust stability and robust periodicity of delayed recurrent neural networks with noise disturbance ieee transactions on circuits and systems i 2006 53 10 2265 2273 2 - s2.0 - 33750480558 10 liu y wang z serrano a liu x discrete - time recurrent neural networks with time - varying delays : exponential stability analysis physics letters a 2007 362 5 - 6 480 488 2 - s2.0 - 33846401362 11 he y liu gp rees d wu m stability analysis for neural networks with time - varying interval delay ieee transactions on neural networks 2007 18 6 1850 1854 2 - s2.0 - 38149066040 12 zeng h - b he y wu m zhang c - f complete delay - decomposing approach to asymptotic stability for neural networks with time - varying delays ieee transactions on neural networks 2011 22 5 806 812 2 - s2.0 - 79955824878 21421436 13 zhang x - m han q - l new lyapunov - krasovskii functionals for global asymptotic stability of delayed neural networks ieee transactions on neural networks 2009 20 3 533 539 2 - s2.0 - 63049089465 19224733 14 xiao s - p zhang x - m new globally asymptotic stability criteria for delayed cellular neural networks ieee transactions on circuits and systems ii 2009 56 8 659 663 2 - s2.0 - 69649093490 15 xu z - b qiao h peng j zhang b a comparative study of two modeling approaches in neural networks neural networks 2004 17 1 73 85 2 - s2.0 - 0347511636 14690709 16 shao h delay - dependent stability for recurrent neural networks with time - varying delays ieee transactions on neural networks 2008 19 9 1647 1651 2 - s2.0 - 52149090973 18779095 17 zuo z yang c wang y a new method for stability analysis of recurrent neural networks with interval time - varying delay ieee transactions on neural networks 2010 21 2 339 344 2 - s2.0 - 76749160268 20028620 18 li x gao h yu x a unified approach to the stability of generalized static neural networks with linear fractional uncertainties and delays ieee transactions on systems , man , and cybernetics , part b : cybernetics 2011 41 5 1275 1286 2 - s2.0 - 80052889894 19 wu y - y wu y - q stability analysis for recurrent neural networks with time - varying delay international journal of automation and computing 2009 6 3 223 227 2 - s2.0 - 70349427470 20 zeng h - b xiao s - p liu b new stability criteria for recurrent neural networks with a time - varying delay international journal of automation and computing 2011 8 1 128 133 2 - s2.0 - 79952320284 21 bai yq chen j new stability criteria for recurrent neural networks with interval time - varying delay neurocomputing 2013 121 179 184 22 ji md he y zhang ck wu m novel stability criteria for recurrent neural network with time - varying delay neurocomputing 2014 23 han q - l a new delay - dependent stability criterion for linear neutral systems with norm - bounded uncertainties in all system matrices international journal of systems science 2005 36 8 469 475 2 - s2.0 - 27944440489 24 park p ko jw jeong c reciprocally convex approach to stability of systems with time - varying delays automatica 2011 47 1 235 238 2 - s2.0 - 78650807125 25 boyd s ghaoui le feron e balakrishnan v linear matrix inequalities in system and control theory 1994 philadelphia , pa , usa siam table 1 allowable upper bounds of tau - for different mu. mu 0 0.1 0.5 0.9 any mu [ 16 ] 1.3323 0.8245 0.3733 0.2343 0.2313 [ 19 ] 1.3325 0.8404 0.4265 0.3217 0.3211 [ 20 ] 1.3324 0.8402 0.4266 0.3225 0.3218 [ 17 ] 1.3323 0.8402 0.4264 0.3214 0.3209 [ 18 ] ( n = 1 ) 1.5157 0.9279 0.4267 - - 0.3212 [ 18 ] ( n = 2 ) 1.5330 0.9331 0.4268 - - 0.3215 [ 21 ] - - 0.8411 0.4267 0.3227 0.3215 [ 22 ] 1.5575 0.9430 0.4417 0.3632 0.3632 the proposed ( m = 2 ) 1.7685 1.0431 0.4382 0.3668 0.3644